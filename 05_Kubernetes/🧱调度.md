来，咱们把这些干货重新码一码，保持原滋原味，但让逻辑更顺、层次更清。这份笔记我分 **「理论架构层」「白话解读层」「实战操作层」** 三层展开，层层递进，但内容一字不减。

---

# 📋 Kubernetes 调度机制知识清单（生产级 · 体系化）

> ✅ 目标：覆盖所有影响 Pod 被调度到哪个节点的核心机制，按逻辑分层，便于查漏补缺与架构设计。

---

## 一、【基础前提】调度发生的条件

- Pod 必须处于 **未绑定状态**（`spec.nodeName` 为空）
- 节点必须处于 **Ready 状态**（`node.status.conditions` 中 `Ready=True`）
- 节点未被 **手动封锁**（`node.Spec.Unschedulable=true`，即 `kubectl cordon`）

---

## 二、【核心流程】调度器工作阶段

### 1. **过滤阶段（Filtering / Predicate）**

> 排除不符合硬性条件的节点

涉及机制：

- 资源充足性（CPU/Memory/Ephemeral Storage/GPU/扩展资源）
- 节点状态（NotReady、NetworkUnavailable、DiskPressure 等 taint 自动添加）
- **Taint/Toleration 匹配**
- **Node Affinity（硬）**
- **Pod Affinity/Anti-Affinity（硬）**
- **Topology Spread Constraints（当 `whenUnsatisfiable: DoNotSchedule`）**
- Volume 拓扑约束（如 CSI 驱动要求的 `volumeBindingMode: WaitForFirstConsumer`）
- Node 上已有 Pod 数量限制（`pods per node`）

### 2. **打分阶段（Scoring / Priority）**

> 对候选节点评分，选最高分者

涉及机制：

- 资源均衡（LeastRequested、MostRequested）
- **Node Affinity（软）**
- **Pod Affinity/Anti-Affinity（软）**
- **Topology Spread（当 `whenUnsatisfiable: ScheduleAnyway`）**
- 镜像本地性（ImageLocality）
- 节点亲和缓存、自定义优先级插件（通过 scheduler profile）

---

## 三、【四大核心调度控制机制】

### 1. **节点排斥机制：Taint & Toleration**

- Taint（Node 上）：`key=value:Effect`（NoSchedule / PreferNoSchedule / NoExecute）
- Toleration（Pod 上）：匹配 Taint 才能调度或不被驱逐
- 典型场景：专用节点、维护模式、自动扩缩容隔离池

### 2. **节点选择机制：Node Affinity**

- **硬亲和**：`requiredDuringSchedulingIgnoredDuringExecution`
- **软亲和**：`preferredDuringSchedulingIgnoredDuringExecution`（带 weight）
- 替代并增强 `nodeSelector`
- 支持复杂表达式（In, NotIn, Exists, Gt, Lt）

### 3. **Pod 间关系机制：Pod Affinity / Anti-Affinity**

- 控制 Pod 与其他 Pod 的相对位置
- 同样分 **硬/软**
- 常用于：
  - **亲和**：App 与 Cache 同机架（降低延迟）
  - **反亲和**：避免多个副本在同一主机（防单点故障）

### 4. **拓扑分布机制：Topology Spread Constraints**

- 控制一组 Pod 在拓扑域（zone/rack/host）中的分布均匀性
- 关键参数：`topologyKey`, `maxSkew`, `whenUnsatisfiable`, `labelSelector`
- 是现代高可用架构的标配（替代部分 PodAntiAffinity 场景）

---

## 四、【资源与容量约束】

- **Requests/Limits**：调度基于 `requests`，驱逐基于 `limits`
- **扩展资源（Extended Resources）**：如 `nvidia.com/gpu`、自定义设备插件资源
- **Ephemeral Storage**：临时存储请求也参与调度
- **Pod 密度限制**：每个节点默认最多 110 个 Pod（可配置）
- **节点 Allocatable**：`capacity - kube-reserved - system-reserved`

---

## 五、【存储感知调度】

- **Volume Topology Requirements**：
  - 某些存储后端（如 AWS EBS、GCE PD）要求 Pod 调度到与 PV 同区域的节点
  - 由 **Volume Binding Mode = WaitForFirstConsumer** 触发
- **Local Persistent Volume**：
  - 必须配合 **Node Affinity** 使用（PV 本身带有节点亲和性）
  - 调度器会自动将 Pod 调度到 PV 所在节点

---

## 六、【高级调度能力】

### 1. **多调度器（Multiple Schedulers）**

- 可指定 `spec.schedulerName` 使用自定义调度器
- 适用于特殊业务（如 AI 作业调度、批处理）

### 2. **调度器配置（Scheduler Profiles）**

- 通过 `KubeSchedulerConfiguration` 自定义过滤/打分插件
- 支持启用/禁用特定策略（如关闭某些亲和性）

### 3. **抢占与驱逐（Preemption）**

- 高优先级 Pod 可抢占低优先级 Pod 的资源（需配置 `priorityClassName`）
- 抢占后，被抢占 Pod 进入 Terminating，新 Pod 被调度

### 4. **Descheduler（非原生，但生产常用）**

- 事后重调度工具，用于：
  - 节点负载不均
  - 违反反亲和规则
  - 节点维护前疏散
- 常与 Cluster Autoscaler 配合使用

---

## 七、【节点自身属性与标签体系】

- **标准标签（强烈建议使用）**：
  - `kubernetes.io/hostname`
  - `topology.kubernetes.io/region`
  - `topology.kubernetes.io/zone`
  - `kubernetes.io/os`, `kubernetes.io/arch`
- **自定义标签**：
  - 用于业务分组（如 `env=prod`, `team=ai`, `disk=ssd`）
- **污点自动注入**：
  - 节点异常时（如 MemoryPressure），kubelet 自动加 taint

---

## 八、【生产最佳实践组合模式】

| 场景                       | 推荐组合                                                 |
| -------------------------- | -------------------------------------------------------- |
| **专用节点（GPU/DB）**     | Taint + Toleration + Node Affinity（硬）                 |
| **高可用微服务**           | Topology Spread（DoNotSchedule） + PodAntiAffinity（软） |
| **成本优化（Spot 实例）**  | Node Affinity（软，weight 高） + Toleration（容忍中断）  |
| **有状态服务（Local PV）** | PV 自带 Node Affinity + Pod 不设额外亲和                 |
| **混合云/边缘**            | 自定义 topologyKey（如 `edge-zone`）+ Topology Spread    |

---

## 九、【调试与可观测性】

- `kubectl describe pod <name>` → 查看 Events 中调度失败原因
- `kubectl get nodes -o wide --show-labels` → 查看节点标签与状态
- `kubectl describe node <name>` → 查看 Taints、Allocatable、Conditions
- Scheduler 日志（需开启 debug）→ 查看过滤/打分详情
- Prometheus 指标：`scheduler_pending_pods`, `scheduler_scheduling_duration_seconds`

---

## 十、【常见陷阱与注意事项】

- ❌ 只设 Toleration 不设 Node Affinity → Pod 可能跑在普通节点
- ❌ Topology Spread 的 `labelSelector` 忘记匹配自身 → 分布无效
- ❌ `NoExecute` Taint 未配 `tolerationSeconds` → Pod 立即被驱逐
- ❌ 多个硬约束冲突 → Pod 永久 Pending
- ❌ 忽略 Volume 拓扑 → PVC 无法绑定，Pod 卡住

---

> ✅ **总结一句话**：  
> Kubernetes 调度 = **资源匹配 + 拓扑约束 + 亲和排斥 + 高可用分布 + 存储协同**，  
> 生产环境必须 **显式声明意图**，不能依赖默认行为。

---

## 🧱 二、老师傅白话解读（掰开揉碎版）

好嘞！既然你叫我一声“老师傅”，那我就放下扳手、泡壶茶，给你掰开揉碎地讲讲 Kubernetes 里这套**调度机制**——节点污点（Taint）、Pod 容忍度（Toleration）、节点调度（Node Scheduling）和拓扑感知调度（Topology-aware Scheduling）。这些玩意儿看着高大上，其实说白了，就是 **“谁能在哪儿跑”** 的规矩。

---

### 🧭 先上张调度流程图（简化但完整）

```
                +---------------------+
                |   Pod 被创建        |
                +----------+----------+
                           |
                           v
                +----------+----------+
                |  Scheduler 启动调度  |
                +----------+----------+
                           |
           +---------------+------------------+
           |                                  |
   [过滤阶段 Filtering]                [打分阶段 Scoring]
           |                                  |
   - 检查节点资源是否足够                  - 计算各节点得分：
   - 检查 Taint/Toleration 是否匹配         • 资源均衡
   - 检查 Node Affinity 是否满足            • 亲和性偏好
   - 检查 Pod Topology Spread 约束         • 拓扑分散度
           |                                  |
   剔除不满足条件的节点                    选出最高分节点
           |                                  |
           +---------------+------------------+
                           |
                           v
                +----------+----------+
                |  将 Pod 绑定到目标 Node |
                +---------------------+
```

> ✅ **关键点**：  
> - **Taint/Toleration 和 Node Affinity 在 Filtering 阶段生效**（硬门槛）  
> - **Topology Spread Constraints 也在 Filtering 阶段检查**（若 `whenUnsatisfiable: DoNotSchedule`）  
> - 所有“软”策略（如 `preferred`）在 Scoring 阶段影响分数

---

### ⚠️ 节点污点（Taint）——“这台机器，一般人别来！”

> **Taint 是加在 Node 上的“排斥标签”**。

想象一台服务器是专门给数据库用的，你肯定不希望随便一个 Web 应用就跑上来占资源。这时候你就给这台 Node 打个“污点”：

```bash
kubectl taint nodes db-node01 dedicated=db:NoSchedule
```

- `dedicated=db`：污点的 key=value
- `NoSchedule`：效果（Effect），意思是“**不允许新 Pod 调度上来**”（但已经在上面的 Pod 不赶走）

#### 污点的三种 Effect：

| Effect             | 含义                                                         |
| ------------------ | ------------------------------------------------------------ |
| `NoSchedule`       | 新 Pod 不能调度上来（除非有容忍）                            |
| `PreferNoSchedule` | 尽量别调度上来（软限制，调度器会尽量避开）                   |
| `NoExecute`        | **不仅不让新 Pod 来，连已经在上面的、没容忍的 Pod 都要驱逐！**（硬核清理） |

> 💡 举个现实例子：GPU 节点通常打上 `gpu=true:NoSchedule`，只有带 GPU 任务的 Pod 才配上去。

---

### 🛡️ Pod 容忍度（Toleration）——“我有通行证，不怕你污点！”

> **Toleration 是写在 Pod spec 里的“豁免权”**。

如果一个 Pod 想跑在有污点的节点上，它必须声明自己“容忍”这个污点：

```yaml
apiVersion: v1
kind: Pod
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "db"
    effect: "NoSchedule"
```

这样，这个 Pod 就能无视 `dedicated=db:NoSchedule` 的污点，被调度到那台“专属”节点上。

#### 注意几个细节：

- 如果只写 `key` 和 `effect`，不写 `value`，可以用 `operator: Exists` 表示“只要这个 key 存在，不管值是多少我都容忍”。
- 容忍 **不会强制** Pod 跑到有污点的节点上！它只是“允许”。你还需要配合 **节点亲和性（Node Affinity）** 来“主动选择”。

> ✅ 小结：**污点是 Node 拒绝别人，容忍是 Pod 说自己不怕拒**。

---

### 📍 节点调度（Node Scheduling）——“我想去哪台机器？”

除了污点/容忍这种“被动准入”，Pod 还可以**主动指定**想跑在哪类节点上，这就是 **节点亲和性（Node Affinity）**。

#### 两种亲和性：

1. **requiredDuringSchedulingIgnoredDuringExecution**  
   → 必须满足，否则调度失败（硬亲和）
2. **preferredDuringSchedulingIgnoredDuringExecution**  
   → 尽量满足，不满足也能跑（软亲和）

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "disk-type"
            operator: In
            values: ["ssd"]
```

> 这个 Pod 只能调度到 label 为 `disk-type=ssd` 的节点上。

💡 **和 nodeSelector 的区别？**  
`nodeSelector` 是老式、简单的 key=value 匹配；`nodeAffinity` 更强大，支持 `In`、`NotIn`、`Exists` 等操作符。

---

### 🌐 拓扑感知调度（Topology-aware Scheduling）——“我要跨机架/可用区高可用！”

这是高级玩法，解决的是 **“Pod 副本别全挤在一个地方”** 的问题。

比如你部署一个 3 副本的 Web 服务，希望它们分别跑在 **不同可用区（zone）**，避免一个区挂了全军覆没。

这就用到 **Pod 拓扑分布约束（Pod Topology Spread Constraints）**：

```yaml
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: my-web
```

- `topologyKey`: 按什么拓扑域分散？常见有：
  - `kubernetes.io/hostname`（不同机器）
  - `topology.kubernetes.io/zone`（不同可用区）
  - `topology.kubernetes.io/region`（不同地域）
- `maxSkew: 1`：任意两个拓扑域中，Pod 数量差不超过 1
- `whenUnsatisfiable`: 不满足时是拒绝调度（DoNotSchedule）还是尽量满足（ScheduleAnyway）

> 🎯 效果：3 个副本 → zone-a:1个, zone-b:1个, zone-c:1个（完美分散）

---

### 🔧 调度机制全景图

| 机制                   | 作用对象 | 目的                   | 类型          |
| ---------------------- | -------- | ---------------------- | ------------- |
| **Taint（污点）**      | Node     | 排斥不合适的 Pod       | 节点主动防御  |
| **Toleration（容忍）** | Pod      | 获取进入“禁区”的权限   | Pod 主动申请  |
| **Node Affinity**      | Pod      | 主动选择目标节点       | 调度偏好      |
| **Topology Spread**    | Pod      | 控制副本在拓扑域的分布 | 高可用 & 容灾 |

> 💡 **最佳实践组合**：  
> - 专用节点：打 `Taint` + 关键 Pod 配 `Toleration` + `Node Affinity` 强制绑定  
> - 多副本服务：加上 `topologySpreadConstraints` 实现跨 AZ 高可用

---

### 🛠️ 运维口诀

> 污点设防，容忍通行；  
> 亲和择主，反亲避同；  
> 拓扑分散，高可用成；  
> 过滤打分，调度分明。

---

## 📄 三、真实 YAML 示例组合（生产可用）

### 场景设定：

- 你有一组 **GPU 计算节点**，打了污点，只允许 AI 任务跑
- AI 应用叫 `ai-inference`，需要 3 副本
- 要求：**必须跑在 GPU 节点**，且 **3 个副本尽量分散在不同可用区（zone）**

---

### 步骤 1️⃣：给 GPU 节点打污点（运维操作）

先给节点打标签：

```bash
# 给名为 node2 的 Kubernetes 节点打上标签 node-role.kubernetes.io/gpu=true
kubectl label node node2 node-role.kubernetes.io/gpu=true
# 查看指定节点的所有标签
kubectl get node node2 --show-labels
# 查看指定节点的指定标签
kubectl get node node2 -L node-role.kubernetes.io/gpu

# 如果标签已存在，默认会报错。
# 若想覆盖已有标签，需加上 --overwrite 参数，比如：
kubectl label node node2 node-role.kubernetes.io/gpu=true --overwrite
# 移除标签示例：（注意末尾的 - 表示删除该 key）
kubectl label node node2 node-role.kubernetes.io/gpu-
```

再执行污点操作：

```bash
# 假设你的 GPU 节点 label 为: node-role.kubernetes.io/gpu=true
kubectl taint nodes -l node-role.kubernetes.io/gpu=true gpu=dedicated:NoSchedule
```

> 这样，普通 Pod 就进不去了。

---

### 步骤 2️⃣：部署 AI 应用（完整 Deployment YAML）

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-inference
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-inference
  template:
    metadata:
      labels:
        app: ai-inference
    spec:
      # 1. 容忍 GPU 节点的污点
      tolerations:
        - key: "gpu"
          operator: "Equal"
          value: "dedicated"
          effect: "NoSchedule"

      # 2. 强制调度到 GPU 节点（通过节点亲和）
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/gpu
                    operator: In
                    values: ["true"]

        # 3. 拓扑分布：跨可用区高可用
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: ai-inference
                topologyKey: kubernetes.io/hostname

      # 4. 拓扑感知分散（更精细控制）
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: ai-inference

      containers:
        - name: model-server
          image: your-registry/ai-model:v1
          resources:
            limits:
              nvidia.com/gpu: 1  # 请求 1 张 GPU
            requests:
              memory: "8Gi"
              cpu: "4"
```

---

### 🔍 关键配置解读

| 配置项                      | 作用                                                         |
| --------------------------- | ------------------------------------------------------------ |
| `tolerations`               | 允许 Pod 忽略 `gpu=dedicated:NoSchedule` 污点                |
| `nodeAffinity`              | **强制**只调度到带 `node-role.kubernetes.io/gpu=true` 的节点 |
| `podAntiAffinity`（软）     | 尽量不让两个副本跑在同一台机器（防单机故障）                 |
| `topologySpreadConstraints` | **硬性要求**：3 个副本在不同 zone，数量差 ≤1                 |

> 💡 **为什么同时用 podAntiAffinity 和 topologySpread？**  
> - `podAntiAffinity` 控制 **同节点** 避免堆积  
> - `topologySpread` 控制 **跨 zone** 均匀分布  
>   二者互补，覆盖不同粒度。

---

### ✅ 验证命令（上线后检查）

```bash
# 查看 Pod 分布在哪几个 zone
kubectl get pods -l app=ai-inference -o wide

# 查看节点标签（确认 zone 信息）
kubectl get nodes --show-labels | grep topology.kubernetes.io/zone
```

理想输出：

```
ai-inference-7d5b8c9f4-x1a2b   Running   gke-gpu-pool-zone-a
ai-inference-7d5b8c9f4-y3c4d   Running   gke-gpu-pool-zone-b
ai-inference-7d5b8c9f4-z5e6f   Running   gke-gpu-pool-zone-c
```

---

## 🧠 四、老师傅最后的叮嘱

1. **污点 + 容忍 + 节点亲和 = 专用节点黄金三角**，缺一不可。
2. **拓扑分布约束** 是现代云原生应用高可用的标配，别省！
3. 生产环境建议：`whenUnsatisfiable: DoNotSchedule`（宁可不调度，也不集中风险）。
4. 调试时先看 `kubectl describe pod` 的 Events，再看 scheduler 日志。
5. 记住口诀：**显式声明意图，不依赖默认行为**——这是生产环境的铁律。

---

这份重组后的笔记，从系统架构 → 白话理解 → 完整实战 → 验证闭环，内容一字未删，但逻辑闭环更清晰。拿去直接就能当生产手册用！