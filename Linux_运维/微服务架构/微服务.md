

## 微服务

### 前言

- 将一个大型复杂应用程序拆分成多个小型、松耦合、可独立部署的服务的架构风格。

每个微服务具备以下典型特征（9大特征，几乎所有定义都绕不开这几点）：

| 特征             | 具体含义                                                     |
| ---------------- | ------------------------------------------------------------ |
| 按业务能力拆分   | 按领域驱动设计（DDD）的Bounded Context划分，而不是按技术层（如MVC） |
| 独立进程         | 每个服务独立运行在一个或多个进程里（通常是容器/Docker）      |
| 独立部署         | 改一个服务只重新部署这一个服务，不影响其他                   |
| 独立数据库       | Database per Service（每个服务拥有自己的数据库，互不共享表） |
| 轻量级通信       | 通常用HTTP/REST、gRPC、消息队列（Kafka/RabbitMQ）            |
| 技术异构         | 一个服务可以用Java，另一个可以用Go、Node.js、Python，随意选  |
| 去中心化治理     | 没有统一的技术标准，也没有中央ESB总线                        |
| 高容错与自动化   | 必须配合服务发现、熔断、限流、链路追踪、CI/CD流水线才能玩得转 |
| 产品而非项目思维 | 每个微服务由一个小型跨职能团队（5-10人）长期负责到底         |

#### 契机

微服务架构产生的真正契机（痛点驱动）。微服务不是理论家坐在办公室里想出来的，而是互联网公司被现实打出来的“血泪进化史”。

| 阶段       | 当时真实痛点（单体架构的灾难）                        | 典型公司与年份               | 结果/转折点                              |
| ---------- | ----------------------------------------------------- | ---------------------------- | ---------------------------------------- |
| 2005年以前 | 单体还行，代码量小，团队小，部署慢点还能忍            | 传统企业                     | 没动力拆                                 |
| 2008-2012  | 流量暴涨、功能暴涨，单体代码几十万→上百万行           | 淘宝、Amazon、Netflix        | 编译40分钟，改一行代码全站重启，崩溃频发 |
| 2011-2012  | 几百人同时改一个代码库，合并冲突、回归测试地狱        | Netflix（2011已拆100+服务）  | 发现“只有拆成小服务才能活”               |
| 2012-2013  | 高峰期只能整体扩容，10%的热点功能拖垮90%的冷功能      | Amazon、Gilt、Uber           | 必须做到“单独扩容某个服务”               |
| 2013-2014  | Docker出现（2013），突然“独立部署”有了技术载体        | Docker发布                   | 微服务终于可以低成本落地                 |
| 2014       | Martin Fowler & James Lewis 发表《Microservices》论文 | 正式命名                     | 微服务从“民间实践”变成“官方架构风格”     |
| 2014-2015  | Kubernetes（2014）、服务网格、CI/CD成熟               | Google开源K8s                | 大规模微服务集群变得可运维               |
| 2015年以后 | 国内互联网公司全面跟进                                | 阿里、腾讯、京东、美团、滴滴 | 微服务成为云计算时代的事实标准           |

代表性里程碑：

- 2011年：Netflix 已经跑了几百个微服务，每天部署几千次
- 2013年：Amazon 公开说“我们早就全部微服务化了”
- 2014年：Martin Fowler 正式给它取名“Microservices”
- 2015年：国内双11第一次全面依赖微服务架构（阿里、京东）

总结： 微服务不是为了好看，而是**在业务复杂度、团队规模、流量规模、迭代速度四重压力下，单体架构彻底失灵后产生的必然进化结果**。容器和DevOps工具的成熟，才让这种进化真正落地。



### 演变

- 从“一坨全放一起” → “分层” → “粗拆服务” → “细拆微服务”

| 阶段        | 架构名称               | 核心特征                                                | 典型年代   | 主要解决什么问题                       | 典型痛点（为什么又进化了）                   |
| ----------- | ---------------------- | ------------------------------------------------------- | ---------- | -------------------------------------- | -------------------------------------------- |
| 1. 巨石时代 | 单体架构（Monolithic） | 所有代码打包成一个大WAR/JAR，全部功能混在一起           | 2000年前   | 开发简单，初期快                       | 代码大了以后编译慢、部署慢、改一行重启全站   |
| 2. 分层时代 | MVC三层架构            | 前端+业务逻辑+数据库分层，但还在同一个项目里            | 2000~2008  | 代码结构更清晰                         | 项目还是一个整体，规模大了依然改不动、扩不动 |
| 3. 粗拆时代 | SOA面向服务架构        | 把系统拆成几个大服务（订单系统、用户系统），用ESB总线连 | 2008~2013  | 支持跨部门复用，解决重复造轮子         | ESB太重、部署仍需协调、版本耦合、扩展性仍差  |
| 4. 细拆时代 | 微服务架构             | 拆成几十上百个小服务，每个服务独立部署、技术随便选      | 2014年至今 | 独立部署、独立扩容、技术自由、团队自治 | 分布式复杂性（调用链、事务、监控）大幅上升   |

业务越复杂、团队越大、流量越高、迭代越快，就越要把系统拆得越小——从“一坨”→“三层”→“几个大服务”→“几百个小服务”。

### 优缺点

**优点（4个字）**：快、活、韧、省

- 快：独立部署、快速迭代
- 活：技术自由、团队自治
- 韧：故障隔离、高可用
- 省：热点服务单独扩容，省机器

**缺点（也是4个字）**：难、乱、多、贵

- 难：分布式事务、调用链调试难
- 乱：服务太多，治理难
- 多：运维、监控、日志成本翻倍
- 贵：前期投入大（人+基础设施）

### 微服务技术栈

- 当前最常见的微服务技术栈（2025年主流两套）

| 层级         | Java系（国内90%大厂）        | Go/云原生系（新兴&中小厂）       |
| ------------ | ---------------------------- | -------------------------------- |
| 微服务框架   | Spring Cloud Alibaba         | Go Micro / Istio + gRPC          |
| 服务注册发现 | Nacos                        | Consul / Kubernetes Service      |
| 配置中心     | Nacos / Apollo               | Nacos / ConfigMap                |
| 网关         | Spring Cloud Gateway / Zuul  | Kong / Traefik / Envoy           |
| 熔断限流     | Sentinel                     | Hystrix → Resilience4j / Istio   |
| 链路追踪     | SkyWalking / Zipkin          | Jaeger / OpenTelemetry           |
| 消息队列     | RocketMQ                     | Kafka / NATS                     |
| 容器编排     | Kubernetes（K8s）            | Kubernetes（K8s）                |
| CI/CD        | Jenkins / GitLab CI          | ArgoCD + Tekton                  |
| 典型代表公司 | 阿里、京东、腾讯、字节、美团 | 抖音部分团队、网易、云原生新公司 |

国内90%互联网公司实际在用：Spring Boot + Spring Cloud Alibaba + Nacos + Sentinel + RocketMQ + K8s，这就是传说中的“SCA全家桶”。

### 微服务架构模式

- 最常见的3种微服务架构模式（2025年）

1. **经典Spring Cloud模式**（国内最常见） Spring Boot + Nacos + Gateway + OpenFeign + Sentinel + SkyWalking + Docker + K8s → 阿里、京东、拼多多、美团点评都在用
2. **Service Mesh模式**（下一代） 业务代码零入侵，所有流量走Sidecar（Istio/Envoy） → 字节跳动、腾讯云、华为云大规模落地
3. **Go + 云原生极简模式** Go Micro / Gin + Kubernetes + gRPC + Jaeger → 新创业公司、游戏公司、部分抖音业务

### 后微服务时代

**2014～2018 年 = 前微服务时代** 主题：怎么把单体拆成微服务？（Spring Cloud、Dubbo 大杀器）

**2018 年以后 = 后微服务时代** 主题：拆完了，几十上百个服务怎么管？怎么跑得又快又稳又省钱？

后微服务时代的三大核心命题 → 也正好对应三大新物种：

| 命题                      | 痛点来源                 | 标志性产物                   | 代表年份 |
| ------------------------- | ------------------------ | ---------------------------- | -------- |
| 1. 几百个服务怎么管通信？ | 调用链乱、熔断限流到处写 | Service Mesh（Istio）        | 2018～   |
| 2. 几百个容器怎么管资源？ | 手动扩缩容、机器利用率低 | Kubernetes Operator + 云原生 | 2019～   |
| 3. 能不能连容器都不管？   | 运维太重、冷启动太慢     | Serverless（FaaS + Knative） | 2020～   |

Service Mesh 与 Serverless 概念比较

| 项目       | 服务网格（Service Mesh）                      | 无服务器（Serverless）                       |
| ---------- | --------------------------------------------- | -------------------------------------------- |
| 核心理念   | “把微服务之间的通信全部接管”                  | “连服务器都不想管，我只写函数”               |
| 你管什么   | 只写业务代码，通信/熔断/监控全交给旁路Sidecar | 只写业务函数，容器/扩缩容/冷启动全交给云厂商 |
| 典型代表   | Istio、Linkerd、Envoy                         | AWS Lambda、阿里云函数计算、腾讯云SCF        |
| 部署方式   | 每个微服务旁边自动注入一个代理（Sidecar）     | 完全不部署服务器，上传函数代码就行           |
| 适用场景   | 已有大量微服务，想一键解决通信治理            | 事件驱动、短时任务、流量波动极大（如秒杀）   |
| 国内谁在用 | 字节跳动、腾讯、华为云、饿了么                | 支付宝小程序、抖音部分活动、微信小程序云开发 |
| 一句话评价 | 微服务的“超级外挂”，专治调用链各种乱          | 微服务的“终极懒人版”，连容器都不想维护       |

Kubernetes 成为容器战争胜利者标志着后微服务时代的开端，但 Kubernetes 仍然没有能够完美解决全部的分布式问题！

| 分布式顽疾                 | K8s 原生有没有完美解决？ | 现状（2025年）实际靠谁补洞                      |
| -------------------------- | ------------------------ | ----------------------------------------------- |
| 分布式事务（最终一致性）   | 完全没解决               | 靠业务代码+Saga、TCC、RocketMQ 事务消息手动实现 |
| 服务调用链追踪             | 只给 Pod 日志，没调用链  | SkyWalking、Jaeger、OpenTelemetry               |
| 熔断、限流、降级           | 没有                     | Sentinel、Istio、Resilience4j                   |
| 服务发现与负载均衡         | 有，但很原始             | Nacos、Consul + L4/L7 智能路由                  |
| 配置统一管理与动态刷新     | ConfigMap 太弱           | Nacos、Apollo、Spring Cloud Config              |
| 跨服务链路超时预算控制     | 没有                     | Istio 超时策略 + OpenTelemetry Trace            |
| 灰度发布、金丝雀、流量染色 | Deployment 能凑合        | Argo Rollouts + Istio/Flagger 真正好用          |
| 多集群、多地域服务治理     | 完全没解决               | 自研多集群联邦 + Istio Multi-Cluster            |
| 冷启动与极致弹性（秒级）   | Pod 启动慢               | Knative + Kourier 或直接上 Serverless           |
| 安全（mTLS、服务间加密）   | 要自己装证书             | Istio 自动 mTLS 才是真香                        |

**真实大厂现在的做法（2023-2025）**

| 公司     | 公开表态 / 真实做法                                          |
| -------- | ------------------------------------------------------------ |
| Amazon   | CEO 亲自发邮件：新项目一律单体优先，能不拆就不拆             |
| Shopify  | 公开喊了5年“我们后悔早拆微服务”，2024年把核心交易系统重新合回单体 |
| 字节跳动 | 新业务99%先用单体 + Gin/Fiber 起，火了再拆                   |
| 阿里     | 淘系核心交易链2023年启动“回归单体”计划                       |
| Segment  | 经典案例：拆成500个微服务后崩溃，2020年全部合并回一个Rails单体 |

**最终结论结语（记住这三句话就够了）**

1. **99% 的项目一辈子都用不着微服务**
2. **微服务不是目标，是“税”——你业务做大后不得不交的税**
3. **能用模块化单体解决的问题，绝不用微服务**





### ZooKeeper 2888/3888

#### 工作原理

ZooKeeper 采用的是推拉相结合的方式：
客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据。

ZooKeeper 具有以下两大特性:
客户端如果对ZooKeeper 的一个数据节点注册 Watcher监听，那么当该数据节点的内容或是其子节点列表发生变更时，ZooKeeper 服务器就会向已注册订阅的客户端发送变更通知。
对在ZooKeeper上创建的临时节点，一旦客户端与服务器之间的会话失效，那么该临时节点也就被自动清除。

- **推（Push）**：服务端只推送一个轻量级的“事件通知”，告诉客户端“有变化发生了”。这非常高效，网络开销小，服务端压力小。

- **拉（Pull）**：客户端收到通知后，自己决定何时以及如何获取最新数据。这给了客户端更大的灵活性，并且保证了数据的一致性（客户端总是主动去获取最新数据，避免了服务端推送可能带来的数据不一致或旧数据问题）。

#### 服务流程

1. 生产者启动
2. 生产者注册至zookeeper
3. 消费者启动并订阅频道
4. zookeeper 通知消费者事件
5. 消费者调用生产者
6. 监控中心负责统计和监控服务状态

![image-20251129102828036](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251129102828036.png)

#### 单机安装部署

```powershell
# 准备 JAVA 环境 (Ubuntu2404)
apt update && apt -y install openjdk-21-jdk
Java -version
# 部署 ZooKeeper （这里使用二进制安装）
wget https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.9.4/apache-zookeeper-3.9.4-bin.tar.gz
tar xf /root/apache-zookeeper-3.9.4-bin.tar.gz -C /usr/local/
ln -s /usr/local/apache-zookeeper-3.9.4-bin /usr/local/zookeeper
注意：不支持软链接到/usr/local/bin 目录，此方法会导致配置文件无法找到，无法启动
# 支持修改PATH变量
echo 'PATH=/usr/local/zookeeper/bin:$PATH' > /etc/profile.d/zookeeper.sh
. /etc/profile.d/zookeeper.sh
ll /usr/local/zookeeper/
ls /usr/local/zookeeper/bin/ && ls /usr/local/zookeeper/conf/
# 准备配置文件、数据路径和日志路径
cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
mkdir -p /usr/local/zookeeper/{data,logs}
# 默认配置可不做修改,
grep -v "#" /usr/local/zookeeper/conf/zoo.cfg
tickTime=2000 #服务器与服务器之间的单次心跳检测时间间隔，单位为毫秒
initLimit=10 #集群中leader 服务器与follower服务器初始连接心跳次数，即多少个 2000 毫秒
syncLimit=5 #leader 与follower之间检测发送和应答的心跳次数，如果该follower在时间段5*2000不能与leader进行通信，此follower将不可用
dataDir=/usr/local/zookeeper/data #自定义的zookeeper保存数据的目录
dataLogDir=/usr/local/zookeeper/logs # 指定日志路径，强烈建议事务日志目录和数据目录分开
clientPort=2181 #客户端连接 Zookeeper 服务器的端口，Zookeeper会监听这个端口，接受客户端的访问请求
autopurge.snapRetainCount=3  # 只保留此最新3个快照和相应的事务日志,并分别保留在 dataDir 和 dataLogDir 中
autopurge.purgeInterval=24  # 自动清理间隔是24小时；默认是 0，表示不开启自动清理功能
# 提供prometheus监控功能
metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
metricsProvider.httpHost=0.0.0.0
metricsProvider.httpPort=7000
metricsProvider.exportJvmInfo=true
# 前台启动观察启动过程
zkServer.sh start-foreground  
# 查看启动状态 
zkServer.sh status && ss -tunlp | egrep '7000|2181'
# 查看是否能暴露指标
curl 10.0.0.200:7000/metrics
# 创建 service 文件
cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target
[Service]
Type=forking
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart
[Install]
WantedBy=multi-user.target
EOF
# 启动服务；查看服务状态
systemctl start zookeeper && systemctl status zookeeper
```

#### 

#### 客户端访问

```powershell
# 进入 zookeeper 客户端工具 (已经将bin目录下的文件在变量中定义了，所以这里直接运行！)
zkCli.sh
ls /zookeeper	# 查看文件目录
get /zookeeper	# 查看文件内容；注意：文件夹既可以做目录也可以做文件存放数据；文件=文件夹
create /sre		# 创建文件
set /sre ceshi	# 对sre文件添加内容
# 查看 zookeeper 数据存放路径
tree /usr/local/zookeeper/data
```

图形化客户端

Linux 客户端

```powershell
注意：此软件因年代久远不再更新，只支持 JAVA-8，不支持JAVA-11以上版本；
且不支持Ubuntu20.04,但支持Ubuntu22.04和Rocky8
# 范例： Ubuntu24.04 编译 zooinspector
apt update && apt -y install openjdk-8-jdk maven
# 查看已安装的 Java 版本，并选择 Java 版本
root@ubuntu-200:~# update-alternatives --config java
There are 2 choices for the alternative java (providing /usr/bin/java).

  Selection    Path                                            Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-21-openjdk-amd64/bin/java      2111      auto mode
  1            /usr/lib/jvm/java-21-openjdk-amd64/bin/java      2111      manual mode
  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode

Press <enter> to keep the current choice[*], or type selection number: 2
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode
# 检查 Java 版本
java -version
mvn -v
# 编译加速 
# 参考网址：https://developer.aliyun.com/mirror/maven?spm=a2c6h.13651102.0.0.7be81b11KoLURJ
vim /etc/maven/settings.xml
<mirrors>
    <mirror>
      <!--阿里云镜像-->
      <id>nexus-aliyun</id>
      <mirrorOf>*</mirrorOf>
      <name>Nexus aliyun</name>
      <url>http://maven.aliyun.com/nexus/content/groups/public</url>
    </mirror>
<mirrors>

git clone https://github.com/zzhang5/zooinspector.git
cd zooinspector
mvn clean package -Dmaven.test.skip=true
chmod +x /root/zooinspector/target/zooinspector-pkg/bin/zooinspector.sh
# 进入target目录下运行脚本
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh
# 此时会弹出一个图形界面，ip修改为zookeeper服所在的地址：10.0.0.200:2181  ，然后ok回车
# 可以看到之前在zookeeper客户端中创建的文件以及文件内容；

WindTerm —— 开启 X server —— 
WindTerm —— 标签栏：会话 —— 首选项 —— 会话设置 —— X11 —— 内部 X 显示 —— 保存
```

Windows 客户端

Java下载地址：https://www.oracle.com/java/technologies/downloads/#license-lightbox

```powershell
# 打开 Windows powerShell 终端查看java环境： java -version
# 如果没有java环境，根据上面的下载地址，下载 Windows 系统 Java 8 版本的包；
ls /root/zooinspector/target
# 将该目录下的 zooinspector-1.0-SNAPSHOT-pkg.tar 或者 zooinspector-pkg 文件夹传到 Windows 系统中；
双击文件中bin目录下的bat文件：zooinspector.bat
```



#### 集群部署

###### 前言

- 基于 ZAB 协议，所以 zookeeper 集群节点数必须是不低于3的奇数；

节点角色状态：

- looking：寻找 Leader 状态，处于该状态需要进入选举流程
- leading：领导者状态，处于该状态的节点说明是角色已经是Leader
- following：跟随者状态，表示 Leader已经选举出来，当前节点角色是follower
- observer：观察者状态，表明当前节点角色是 observe

选举 ID：

- ZXID ：每个改变 Zookeeper状态的操作都会自动生成一个对应的zxid。ZXID最大的节点优先选为Leader
  - ZXID 值越大，数据越新；
- myid ：服务器的唯一标识(SID)，通过配置 myid 文件指定，集群中唯一,当ZXID一样时,myid大的节点优先选为Leader
- **epoch选举优先级最高，其次是zxid，myid的优先级最低**

在分布式系统中，有多种协议被设计来解决一致性问题，Paxos、Raft、ZAB 等分布式算法经常会被称作是“强一致性”的分布式共识协议；

- ZAB (原子广播协议) 保证了分布式过程中的消息顺序一致性和崩溃恢复能力。它主要用在主-备模式的系统中；
- Paxos （帕克索斯） 的描述和实现被公认为相对复杂，这也是Raft等其他一致性算法出现的原因。
- Raft通过选举算法确保了分布式系统中的领导者唯一性。所有的写操作都通过领导者完成，这样就可以确保所有复制节点上的数据一致性。

强一致指的是尽管系统内部节点可以存在不一致的状态，但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的；

###### 部署

- 10.0.0.200
- 10.0.0.201
- 10.0.0.202
- 三台 ubuntu2404；都安装JDK8或JDK11,JDK21

```powershell
# 准备 JAVA 环境 (Ubuntu2404)
apt update && apt -y install openjdk-21-jdk
# 查看
Java -version

# 部署 ZooKeeper （这里使用二进制安装）
wget https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.9.4/apache-zookeeper-3.9.4-bin.tar.gz
tar xf /root/apache-zookeeper-3.9.4-bin.tar.gz -C /usr/local/
ln -s /usr/local/apache-zookeeper-3.9.4-bin /usr/local/zookeeper
echo 'PATH=/usr/local/zookeeper/bin:$PATH' > /etc/profile.d/zookeeper.sh
. /etc/profile.d/zookeeper.sh
ll /usr/local/zookeeper/
ls /usr/local/zookeeper/bin/ && ls /usr/local/zookeeper/conf/
cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
mkdir -p /usr/local/zookeeper/{data,logs}
# 默认配置可不做修改,
grep -v "#" /usr/local/zookeeper/conf/zoo.cfg
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/usr/local/zookeeper/data
dataLogDir=/usr/local/zookeeper/logs
clientPort=2181
autopurge.snapRetainCount=3  
autopurge.purgeInterval=24  
server.1=10.0.0.200:2888:3888
server.2=10.0.0.201:2888:3888
server.3=10.0.0.202:2888:3888
# 格式: server.MyID服务器唯一编号=服务器IP:Leader和Follower的数据同步端口(只有leader才会打开):Leader和Follower选举端口(L和F都有)

# 为当前Zookeeper节点分配一个在集群中唯一的数字标识，使节点能够正确识别自己在集群配置中的位置。
echo 1 > /usr/local/zookeeper/data/myid
# 创建 service 文件
cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target
[Service]
Type=forking
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart
[Install]
WantedBy=multi-user.target
EOF

# 三个节点配置文件相同
scp /usr/local/zookeeper/conf/zoo.cfg 10.0.0.200:/usr/local/zookeeper/conf/zoo.cfg
scp /usr/local/zookeeper/conf/zoo.cfg 10.0.0.202:/usr/local/zookeeper/conf/zoo.cfg
# 为当前Zookeeper节点分配一个在集群中唯一的数字标识，使节点能够正确识别自己在集群配置中的位置。
echo 2 > /usr/local/zookeeper/data/myid
# 在 10.0.0.200 与 10.0.0.202 主机中分别配置数字标识
echo 1 > /usr/local/zookeeper/data/myid
echo 3 > /usr/local/zookeeper/data/myid

# 启动服务，查看服务状态
systemctl start zookeeper && systemctl status zookeeper
/usr/local/zookeeper/bin/zkServer.sh status
# 只有leader监听2888/tcp端口;follower会监听3888/tcp端口
ss -tunlp | grep 888

客户端（若没有，按照前面的进行客户端部署！）
update-alternatives --config java
# 进入target目录下运行脚本
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh
# 在 lead 节点中进行增删改，follow 节点不支持增删改，但是 lead 节点的增删改都会同步到 follow 节点中；
```

### Kafka 9092

#### 角色

- Producer：即生产者，消息的产生者，是消息的入口。负责发布消息到Kafka broker。
- Consumer：消费者，用于消费消息，即处理消息
- Broker：Broker是kafka实例，每个服务器上可以有一个或多个kafka的实例。
- Controller：是整个 Kafka 集群的管理者角色
- Topic ：消息的主题，可以理解为消息的分类；
  - 每个broker上都可以创建多个 topic
  - 一个Topic相当于数据库中的一张表,一条消息相当于关系数据库的一条记录
- Consumer group: 每个consumer 属于一个特定的consumer group
  - 同一topic的一条消息只能被同一个consumer group 内的一个consumer 消费，但多个
    consumer group 可同时消费这一消息
- Partition：分区。表现形式就是一个一个的文件夹,该文件夹下存储该partition的数据和索引文件。
  - 同一个topic在不同的分区的数据是不重复的,一般Partition数不要超过节点数；
  - 同一个 partition 数据是有顺序的，但不同的 partition 则是无序的。

![image-20251129185748520](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251129185748520.png)

Kafka的复制配置
Kafka的复制方式可以通过 acks 配置来实现：

1. acks=0：生产者发送消息后不等确认即认为成功。这种方式相当于没有复制，只有单个副本存储在Leader上。
2. acks=1：消息发送到Leader即认为成功，相当于异步复制。在这种情况下，生产者只需要等待Leader的确认，而不需要等待ISR（In-Sync Replicas，同步副本集）中其他副本的确认。
3. acks=all：需要等待ISR列表中所有同步的Slave都确认才认为成功，相当于同步复制。这种方式下，生产者需要等待ISR中所有副本都
   写入成功后才认为消息发送成功。

总的来说，Kafka可以配置为提供强一致性，但这可能会影响其性能和吞吐量。在实际使用中，你需要根据自己的业务需求在一致性和性能之间进行权衡。

#### Kafka 工具

- 图形工具 Offset Explorer (Kafka Tool)
  - 下载链接：https://www.kafkatool.com/download.html
- 基于Web的Kafka集群监控系统 kafka-eagle （ 具体的看课件 ）



#### 单机部署

- 官方下载地址：[Apache Kafka](https://kafka.apache.org/downloads)
- 阿里云下载地址：https://mirrors.aliyun.com/apache/kafka/3.9.1/kafka_2.13-3.9.1.tgz
- Kafka-v4.0 开始即将不再支持 Zookeeper
- Kafka 提供的脚本不支持创建软链接到/usr/local/bin/路径，只支持绝对路径或相对径执行
- Kafka 有内置的 zookeeper 服务，所以不需要再去单独部署 zookeeper 服务；

###### 基于 zookeeper 部署 kafka

```powershell
wget https://mirrors.aliyun.com/apache/kafka/3.9.1/kafka_2.13-3.9.1.tgz
# 注意主机名解析,默认主机名称反向解析IP进行访问
root@ubuntu-200:~# hostname
ubuntu-200
echo "10.0.0.200 ubuntu-200" >> /etc/hosts
apt update && apt install -y openjdk-21-jdk
tar xf kafka_2.13-3.9.1.tgz -C /usr/local/
cd /usr/local/ && ls
ln -s /usr/local/kafka_2.13-3.9.1/ /usr/local/kafka && ls /usr/local/kafka
# 修改配置文件，此目录无需手动创建，启动会自动创建
sed -i "s#^dataDir.*#dataDir=/data/zookeeper#" /usr/local/kafka/config/zookeeper.properties
# 临时启动
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
# 修改配置文件，此目录无需手动创建，启动会自动创建
sed -i "s#^log.dirs.*#log.dirs=/data/kafka-logs#" /usr/local/kafka/config/server.properties
# 临时启动
/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties
# 查看端口以及图形化客户端
ss -tunlp | grep 2181

# 创建service文件
cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh -daemon /usr/local/kafka/config/zookeeper.properties
ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh
ExecReload=/bin/kill -HUP \$MAINPID

[Install]
WantedBy=multi-user.target
EOF
# 创建service文件
cat > /lib/systemd/system/kafka.service <<EOF
[Unit]
Description=kafka.service
After=network.target zookeeper.service

[Service]
Type=forking
ExecStart=/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh
ExecReload=/bin/kill -HUP \$MAINPID

[Install]
WantedBy=multi-user.target
EOF

systemctl  start zookeeper && systemctl  status zookeeper
systemctl  start kafka && systemctl  status kafka
# 补充：临时启动 zookeeper
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
```

###### 基于KRaft 部署 kafka单

- 主机：10.0.0.201
- 注意：Kafka-4.0.0 版本不再支持Zookeeper，使用Kraft

```powershell
wget https://mirrors.aliyun.com/apache/kafka/4.1.1/kafka_2.13-4.1.1.tgz
tar xf kafka_2.13-4.1.1.tgz -C /usr/local/
cd /usr/local/ && ls
ln -s kafka_2.13-4.1.1/ kafka
# 生成集群唯一ID,Generate a Cluster UUID,集群内的多个节点需使用同一集群ID
cd /usr/local/kafka && KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
echo KAFKA_CLUSTER_ID
# 建议修改数据日志目录
sed -i "s#^log.dirs.*#log.dirs=/data/kraft-combined-logs#" /usr/local/kafka/config/server.properties
# 查看（对照）	ls /data/kraft-combined-logs
# 格式化出事数据
bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties
# 查看（不出意外的话，就有内容生成了）	ls /data/kraft-combined-logs
# Kafka 启动！
bin/kafka-server-start.sh config/server.properties
# 查看端口 9092 、 9093
ss -tunlp 
# 基于service文件就不配置了，目前现网上主流还是以zookeeper为主的！
```

#### 集群部署

###### 基于 zookeeper 模式的集群

环境准备

- 注意：每个kafka节点的主机名称解析需要提前准备，否则会导致失败
  - node1：10.0.0.201
  - node2：10.0.0.202
  - node3：10.0.0.200

```powershell
# 修改每个kafka节点的主机名
hostnamectl set-hostname node1
hostnamectl set-hostname node2
hostnamectl set-hostname node3
# 在所有kafka节点上实现主机名称解析
cat >> /etc/hosts <<eof
10.0.0.200 node3
10.0.0.201 node1
10.0.0.202 node2
eof
# 安装 JAVA
apt update && apt -y install openjdk-21-jre
java -version
```

```powershell
tar xf kafka_2.13-3.9.1.tgz -C /usr/local/
ln -s /usr/local/kafka_2.13-3.9.1/ /usr/local/kafka && ls /usr/local/kafka
```

```powershell
mkdir -p /usr/local/kafka/data/
# 集群配置必须配置时间相关 （三个节点就配置相同）
vim /usr/local/kafka/config/zookeeper.properties
#必须添加时间相关配置
tickTime=2000
initLimit=10
syncLimit=5
#保留下面内容
clientPort=2181
maxClientCnxns=0
admin.enableServer=false
#添加下面集群配置
dataDir=/usr/local/kafka/data/
server.1=10.0.0.201:2888:3888
server.2=10.0.0.202:2888:3888
server.3=10.0.0.200:2888:3888

# 主机：10.0.0.200
echo 3 > /usr/local/kafka/data/myid
# 主机：10.0.0.201
echo 1 > /usr/local/kafka/data/myid	
# 主机：10.0.0.202
echo 2 > /usr/local/kafka/data/myid	
```

```powershell
# 各节点部署 Kafka 配置文件 （单机部署不需要改配置，但是集群部署必须要修改一些参数！）
vi /usr/local/kafka/config/server.properties 
# 每个节点id号唯一 （10.0.0.200对应0；1对应1；2对应2）
broker.id=0
# kafka监听端口，默认9092 (每个节点写自身的ip地址)
listeners=PLAINTEXT://10.0.0.200:9092
log.dirs=/usr/local/kafka/data
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.200:2181
```

```powershell
# 创建service文件
cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh -daemon /usr/local/kafka/config/zookeeper.properties
ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh
ExecReload=/bin/kill -HUP \$MAINPID

[Install]
WantedBy=multi-user.target
EOF
# 创建service文件
cat > /lib/systemd/system/kafka.service <<EOF
[Unit]
Description=kafka.service
After=network.target zookeeper.service

[Service]
Type=forking
ExecStart=/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh
ExecReload=/bin/kill -HUP \$MAINPID

[Install]
WantedBy=multi-user.target
EOF

systemctl  start zookeeper && systemctl  status zookeeper
systemctl  start kafka && systemctl  status kafka

# 图形化客户端 （部署在了10.0.0.200主机）
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh
# 进入图形客户端后，在brokers的ids文件夹中可以看到012这三个brokers编号！
```

------

##### Topic 操作

###### 一、分区概念

- **分区数量尽量与节点（Broker）数量相等**
  这样可以让每个 Broker 都承担 Leader 角色，提升整体吞吐性能。
- **Kafka 集群节点数量建议不少于 3 且为奇数**
  便于进行选举，提高集群的高可用性。
- **副本数至少为 2（Leader + Follower）**
  Leader 负责读写，Follower 同步数据，当 Leader 宕机可自动切换。
- **分区 = 性能；副本 = 高可用**
  ✔ 分区数量越多 → 并发能力越高
  ✔ 副本数量越多 → 数据容错能力越强

------

###### 二、创建 Topic

**创建流程（非常关键）**

- 你向 10.0.0.201 发出创建 Topic 请求
- 201 节点把创建请求转发给 **Kafka Controller**
- Controller 创建集群元数据中的 Topic duan
- Controller 向所有 Broker 下发创建指令

```bash
# 创建一个名字叫 duan 的 Topic
# 分成 3 个分区（提高吞吐量）
# 副本数为 2（高可用）
# 新版用法需要 --bootstrap-server
/usr/local/kafka/bin/kafka-topics.sh --create --topic duan --bootstrap-server 10.0.0.201:9092  --partitions 3 --replication-factor 2
```

**✔ 创建后数据落盘（目录变化说明）**

Kafka 各 broker 的数据目录一般为：

```
ls /usr/local/kafka/data
```

执行 ls /usr/local/kafka/data
 你会看到类似：

- broker 0 的目录：duan-0 和 duan-2
- broker 1 的目录：duan-1 和 duan-2
- broker 2 的目录：duan-0 和 duan-1

说明数据分区在不同 broker 上分布，副本也同步成功。

**（可选）图形化查看 Topic**

```bash
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh
```

------

###### 三、查看 Topic 列表

```bash
/usr/local/kafka/bin/kafka-topics.sh  --list  --bootstrap-server 10.0.0.201:9092
```

------

###### 四、查看 Topic 详细信息（重点）

```bash
/usr/local/kafka/bin/kafka-topics.sh   --describe   --bootstrap-server 10.0.0.201:9092   --topic Shirley

Topic: Shirley  TopicId: HMxVk-p5SF2O4bWWNlX0vw PartitionCount: 3       ReplicationFactor: 2    Configs: 
        Topic: Shirley  Partition: 0    Leader: 0       Replicas: 0,2   Isr: 0,2        Elr: N/ALastKnownElr: N/A
        Topic: Shirley  Partition: 1    Leader: 2       Replicas: 2,1   Isr: 2,1        Elr: N/ALastKnownElr: N/A
        Topic: Shirley  Partition: 2    Leader: 1       Replicas: 1,0   Isr: 1,0        Elr: N/ALastKnownElr: N/A
```

可以看到：

- 1 分区号，对应的 Leader 为 2 （这里的2指的是节点号），副本存放位置在 1节点中；（副本在1节点，那么2节点的follow就是1）

------

###### 五、生产消息（Producer）

正确命令：

```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server 10.0.0.201:9092  --topic duan
```

进入交互界面：

```
hello kafka
this is message 1
order_created
```

每敲一行就是一条消息。若此时你在消费信息的交互界面就能实时看到生产者发送的消息；

------

###### 六、消费消息（Consumer）

正确命令：

```bash
/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server 10.0.0.201:9092 --topic duan --from-beginning
```

- --from-beginning：从最早的 offset 开始读取

你会看到：

```
hello kafka
this is message 1
order_created
```

如果你继续在 producer 输入，consumer 会实时接收到新消息。

------

**消费组**

- 一个 Topic 也可以被多个不同组消费 ；
- 同一个组可以消费多个 Topic

```powershell
/usr/local/kafka/bin/kafka-console-consumer.sh --topic duan --bootstrap-server 10.0.0.202:9092 --from-beginning --consumer-property group.id=group1
# 启动一个消费组，这个消费组可以消费 Topic = duan 的消息；
消费组的价值 不在“能否消费消息”，而在“消费的协调与进度管理”
```

1️⃣ **不指定消费组 vs 指定消费组的本质区别**

| 方式             | 消费者行为                                                   | 消息存储/offset                                   | 适用场景                                               |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------ |
| **不指定 group** | 临时消费者，每次启动都是全量消费 --from-beginning 或默认最新消息 | Kafka 不记录消费进度                              | 临时调试、测试、只想看看消息                           |
| **指定 group**   | 属于固定消费组，组内多消费者分摊分区                         | Kafka 会保存 offset，下次启动可以接着上次位置消费 | 真实业务生产环境，负载均衡，消息只被组内一个消费者处理 |

###### 七、删除 Topic

**删除流程（非常关键）**

- 你向 10.0.0.201 发出删除 Topic 请求
- 201 节点把删除请求转发给 **Kafka Controller**
- Controller 删除集群元数据中的 Topic duan
- Controller 向所有 Broker 下发删除指令

```bash
/usr/local/kafka/bin/kafka-topics.sh --delete --bootstrap-server 10.0.0.201:9092  --topic duan
```

验证是否成功删除：

```bash
/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server 10.0.0.201:9092
```

- **不要手动删除 __consumer_offsets-\* 等内部文件**
- 调整 **offsets.retention.minutes** 清理消费者偏移量
- 修改后，Kafka 会自动删除超过保留策略的日志文件。

```powershell
# 按时间清理
log.retention.hours=168       # 默认7天
# 按大小清理
log.retention.bytes=1073741824  # 1GB
```

##### 消息积压

为什么会发生消息积压？

- 消费者消费能力不足；
- 消息产生速度过快；
- 分区负载不均衡；
- 网络故障；硬件资源不足等

如何衡量消息积压？

```powershell
Log End Offset - Current Offset = Lag > 0 ————> 消息积压
```

查看消费组详情及积压

```powershell
/usr/local/kafka/bin/kafka-consumer-groups.sh   --bootstrap-server 10.0.0.201:9092   --describe   --group group1
```

输出示例：

```powershell
GROUP     TOPIC    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG  CONSUMER-ID
group1    duan     0          15              20              5    consumer-1
```

解释：

- CURRENT-OFFSET：消费组已消费到的偏移量
- LOG-END-OFFSET：主题当前的最新偏移量
- LAG：积压数量（未被消费的消息数）

> 积压就是 LOG-END-OFFSET - CURRENT-OFFSET。



### Nacos

主要功能：服务注册与发现、分布式配置管理 （目前主要学这两个）

下载链接：[Nacos Server 下载 | Nacos 官网](https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu)

服务提供者分类：

- 临时实例: 服务提供者定期周期主动向nacos 发送心跳监测报告,如果一段时间后,nacos无法收到心跳,则删除此实例；
- 非临时实例(永久实例): nacos 主动定时监测此类实例,如果提供者实例异常,则并不会删除只是标记此实例异常,等待此实例恢复

集群模式：

- Nacos 默认使用 AP 模式；存在非临时实例时,会采用CP模式；（AP：弱一致性；CP：强一致性）
- Eureka 采用 AP( Availability和Partiton tolerance)模式；
- CP模式基于强一致性协议 Raft
- AP模式基于阿里的Distro(基于Gossip和Eureka协议优化而来)最终一致性的AP 分布式协议

Nacos 支持三种部署模式

- 单机模式 - 用于测试和单机试用
- 集群模式 - 用于生产环境，确保高可用
- 多集群模式 - 用于多数据中心场景

环境准备

- 安装好 JDK，需要 1.8 及其以上版本
- 建议: 2核 CPU / 4G 内存 及其以上
- 建议: 生产环境 3 个节点 及其以上

下载链接：[Nacos Server 下载 | Nacos 官网](https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36nc4nT3)

2.X 版本与3.X 版本有较大的不同：

> 启动命令(standalone代表着单机模式运行，非集群模式):
>
> ```
> sh startup.sh -m standalone
> ```
>
> 如果您使用的是ubuntu系统，或者运行脚本报错提示[[符号找不到，可尝试如下运行：bash startup.sh -m standalone
>
> 随后启动程序会提示您输入3个鉴权相关配置（Nacos从3.0.0版本开始默认启用控制台鉴权功能，因此如下3个鉴权相关配置必须填写）如下所示：
>
> ```powershell
> nacos.core.auth.plugin.nacos.token.secret.key is missing, please set with Base64 string: ${your_input_token_secret_key}
> nacos.core.auth.plugin.nacos.token.secret.key Updated:
> ----------------------------------
> nacos.core.auth.server.identity.key is missing, please set: ${your_input_server_identity_key}
> nacos.core.auth.server.identity.key Updated:
> ----------------------------------
> nacos.core.auth.server.identity.value is missing, please set: ${your_input_server_identity_key}
> nacos.core.auth.server.identity.value Updated:
> ----------------------------------
> ```
>
> 若已经在 conf/application.properties 中设置过这3个配置，则不会提示输入。

> 老版本：10.0.200:8848/nacos
>
> 新版本：10.0.0.200:8080

#### 单体部署

###### 二进制安装Nacos-2.X （Ubuntu2404）

```powershell
apt update && apt -y install openjdk-8-jdk
wget https://download.nacos.io/nacos-server/nacos-server-2.5.2.zip?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu&file=nacos-server-2.5.2.zip
unzip nacos-server-2.5.2.zip -d /usr/local/
# 添加PATH变量中，可选
echo 'PATH=/usr/local/nacos/bin:$PATH' >> /etc/profile
. /etc/profile
# 测试：启动(standalone单机模式) ；关闭服务 
/usr/local/nacos/bin/startup.sh -m standalone	# 若做了上述的变量：bash startup.sh -m standalone
bash shutdown.sh
#  准备Sevice文件
id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos
chown -R nacos: /usr/local/nacos/ && ll /usr/local/nacos/
cat > /lib/systemd/system/nacos.service <<eof
[Unit]
Description=nacos.service
After=network.target
[Service]
Type=forking
ExecStart=/usr/local/nacos/bin/startup.sh -m standalone
ExecStop=/usr/local/nacos/bin/shutdown.sh
User=nacos
Group=nacos
[Install]
WantedBy=multi-user.target
eof

systemctl daemon-reload
systemctl start nacos.service && systemctl status nacos.service
# 查看日志，如果日志没有报错那服务启动就成功了！
tail /usr/local/nacos/logs/startup.log
# 浏览器访问：10.0.0.201:8848/nacos
```



###### 二进制安装Nacos-3.X （Ubuntu2404）

```powershell
apt update && apt -y install openjdk-21-jdk
wget https://hub.docker.com/r/nacos/nacos-server/tags?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu&page=1&name=3.1.1
unzip nacos-server-3.1.1 -d /usr/local/
# 添加PATH变量中 （可选）
echo 'PATH=/usr/local/nacos/bin:$PATH' >> /etc/profile
. /etc/profile
# 测试：启动(standalone单机模式) ；关闭服务 
/usr/local/nacos/bin/startup.sh -m standalone	
bash shutdown.sh
这里就会显现差异了！Nacos 3.X 以后版本默认要求必须开启鉴权才能启动
# 生成一个安全的 Base64 编码密钥（32字符以上）（如果使用默认密钥，这里就不用生成新的密钥了！）
openssl rand -base64 32
# 修改配置文件永久设置 （主要修改三行配置）
vi /usr/local/nacos/conf/application.properties
nacos.core.auth.server.identity.key=duan
nacos.core.auth.server.identity.value=99
........... 
nacos.core.auth.plugin.nacos.token.secret.key=VGhpc0lzTXlDdXN0b21TZWNyZXRLZXkwMTIzNDU2Nzg=
```

```powershell
#  准备Sevice文件
id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos
chown -R nacos: /usr/local/nacos/ && ll /usr/local/nacos/
cat > /lib/systemd/system/nacos.service <<eof
[Unit]
Description=nacos.service
After=network.target
[Service]
Type=forking
ExecStart=/usr/local/nacos/bin/startup.sh -m standalone
ExecStop=/usr/local/nacos/bin/shutdown.sh
User=nacos
Group=nacos
[Install]
WantedBy=multi-user.target
eof

systemctl daemon-reload
systemctl start nacos.service && systemctl status nacos.service
# 查看日志，如果日志没有报错那服务启动就成功了！
tail /usr/local/nacos/logs/startup.log
# Nacos 3.0.0默认控制台使用8080端口,而8848端口用于API访问
ss -tunlp |grep java
# 浏览器访问：10.0.0.200:8080		账户密码都是：nacos
```

模拟：向Nacos发起服务注册申请

服务注册

```python
# 服务注册 （临时实例）
curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.duan.serviceName&ip=1.2.3.4&port=8080'
# 再注册一个临时实例（对照）
curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.duan.serviceName&ip=20.21.22.23&port=8090'
# http://127.0.0.1:8848: Nacos 服务器地址和端口
# serviceName=nacos.duan.serviceName: 要注册的服务名称
# ip=1.2.3.4: 服务实例的 IP 地址
# port=8080: 服务实例的端口号
```

服务发现

```python
# 查询服务实例列表
curl 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=nacos.duan.serviceName'

"ephemeral": true - 注册的是临时实例
需要持续心跳来维持注册状态
心跳超时："instanceHeartBeatTimeOut": 15000 (15秒)
心跳间隔："instanceHeartBeatInterval": 5000 (5秒)

# 添加 ephemeral=false 参数明确删除持久化实例
curl -X DELETE 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.duan.serviceName&ip=1.2.3.4&port=8080&ephemeral=false'
```

发布配置

```powershell
curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
```

获取配置

```powershell
curl -X GET "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test"
```



#### **高可用集群部署**

- 下载链接：https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36nc4nT3
- 部署版本：nacos-server-3.1.1

###### 角色

| 角色                | IP 地址    | 备注                         |
| :------------------ | :--------- | :--------------------------- |
| VIP                 | 10.0.0.100 | 虚拟IP地址                   |
| haproxy1+keepalived | 10.0.0.101 | HAProxy 1 与 Keepalived 实例 |
| haproxy2+keepalived | 10.0.0.102 | HAProxy 2 与 Keepalived 实例 |
| nacos1              | 10.0.0.201 | Nacos 服务节点1              |
| nacos2              | 10.0.0.202 | Nacos 服务节点2              |
| nacos3              | 10.0.0.203 | Nacos 服务节点3              |
| MySQL               | 10.0.0.200 | MySQL 数据库服务             |

###### MySQL 服务器

```powershell
apt update &&  apt install mysql-server
# 修改远程监听端口
sed -i '/127.0.0.1/s/^/#/' /etc/mysql/mysql.conf.d/mysqld.cnf
systemctl restart mysql.service
ss -tunlp |grep 3306
mysql
create database nacos;
create user nacos@'%' identified with mysql_native_password by '123123';
grant all on nacos.* to nacos@'%' ;
```

###### Nacos 服务器

- 在三台 nacos 服务器部署 nacos-server-3.1.1 版本的服务；（步骤参考上述的内容）
- nacos 服务默认数据库存储位置是在服务文件中的 data 目录下；（ 比如：/usr/local/nacos/data ）
- nacos 服务基于 MySQL 数据存储在 /usr/local/nacos/conf/mysql-schema.sql 文件中

```powershell
# nacos 每个节点都需要下载 MySQL 客户端
apt update && apt -y install mysql-client
# Nacos1 服务器部署 MySQL 做为数据源
# 导入数据库
mysql -unacos -p123123 -h10.0.0.200 nacos < /usr/local/nacos/conf/mysql-schema.sql
# 查看导入数据表
mysql -unacos -p123123 -h10.0.0.200 -e 'show tables in nacos'
# 重启服务后，进入浏览器 10.0.0.201:8080 显示初始化密码，这就证明数据库已经发生变化，初始化成功！
# 查看 MySQL 数据库 users 表，会发现已经有一个用户 nacos 注册信息；
mysql -unacos -p123123 -h10.0.0.200 -e "USE nacos; SELECT * FROM users;"
```

若使用MySQL数据源，需要对所有集群节点上修改下面文件 ;（若使用内置数据源无需修改配置）

```powershell
vi /usr/local/nacos/conf/application.properties
nacos.core.auth.caching.enabled=true  
# 在 3.1.1 版本中是默认开启的，在之前的老版本中需要手动开启；
nacos.core.auth.server.identity.key=duan 
nacos.core.auth.server.identity.value=99 
nacos.core.auth.plugin.nacos.token.secret.key=VGhpc0lzTXlDdXN0b21TZWNyZXRLZXkwMTIzNDU2Nzg= 
# 在 3.1.1 版本中是默认开启的，在之前的老版本中需要手动添加这三段配置的字符；
spring.sql.init.platform=mysql 	# 取消注释
db.num=1 					  # 取消注释
db.url.0=jdbc:mysql://10.0.0.200:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC
db.user=nacos
db.password=123123
# 补充：这里使用的字符集是：utf8 ，与 MySQL 是兼容的，MySQL 的字符集是 utf8mb4 ，生产工作中建议：utf8mb4
mysql -unacos -p123123 -h10.0.0.200 -e "show create database nacos;"
```

集群配置文件

```powershell
cat > /usr/local/nacos/conf/cluster.conf <<eof
# ip:port
10.0.0.201:8848
10.0.0.202:8848
10.0.0.203:8848
eof
# 因为是集群模式，所以不能单独打开，需要修改配置文件
sed -i 's|-m standalone||g' /lib/systemd/system/nacos.service
# 所有节点同步配置
chown -R nacos: /usr/local/nacos
for ip in 202 203; do
    scp -o StrictHostKeyChecking=no /usr/local/nacos/conf/application.properties 10.0.0.$ip:/usr/local/nacos/conf/application.properties
done
for ip in 202 203; do
    scp -o StrictHostKeyChecking=no /usr/local/nacos/conf/cluster.conf 10.0.0.$ip:/usr/local/nacos/conf/cluster.conf
done
for ip in 202 203; do
    scp -o StrictHostKeyChecking=no /lib/systemd/system/nacos.service 10.0.0.$ip:/lib/systemd/system/nacos.service
done
# 每个节点都要重启服务和重新加载配置文件
systemctl  daemon-reload
systemctl restart nacos && systemctl  status nacos
浏览器 10.0.0.201:8080 集群管理————节点列表
```

###### 负载均衡服务器

配置 haproxy 和 keepalived 实现负载均衡和高可用

```powershell
# 允许进程绑定到非本地IP地址（如VIP虚拟IP）
echo net.ipv4.ip_nonlocal_bind = 1 >> /etc/sysctl.conf
# 重新加载所有配置
sysctl -p
# 在两台服务器上安装配置haproxy实现负载均衡反向代理和高可用
apt update && apt -y install haproxy
# 编辑HAProxy配置文件
vim /etc/haproxy/haproxy.cfg
# 在配置文件中添加以下内容：
cat >> /etc/haproxy/haproxy.cfg <<eof
listen stats
    mode http
    bind 0.0.0.0:9999
    stats enable
    log global
    stats uri /haproxy-status
    stats auth admin:123123

listen nacos-8848
    bind 10.0.0.100:8848
    server nacos01 10.0.0.201:8848 check
    server nacos02 10.0.0.202:8848 check
    server nacos03 10.0.0.203:8848 check

listen nacos-9848
    mode tcp
    bind 10.0.0.100:9848
    server nacos01 10.0.0.201:9848 check
    server nacos02 10.0.0.202:9848 check
    server nacos03 10.0.0.203:9848 check
eof

# 重新加载HAProxy配置
systemctl reload haproxy 
systemctl start haproxy  && systemctl status haproxy 
```

配置keepalived实现高可用

```powershell
# 在两台服务器上安装配置keepalived实现高可用
apt update && apt -y install keepalived
# 修改配置文件
# 这台服务器优先级设置为100，如果这个服务有异常，就会在原有的服务优先级数值减30；
# 另一台服务器优先级设置为80，那么服务器发生异常，优先级发生变化，主备就会发生切换！
cat > /etc/keepalived/keepalived.conf <<eof
global_defs {
    router_id ka1  # 另一台主机上为 ka2
}

vrrp_script chk_haproxy {
    script "killall -0 haproxy"  # cheaper than pidof
    interval 1
    weight -30
}

vrrp_instance VI_1 {
    interface eth0
    virtual_router_id 66
    state MASTER  # 另一台主机上为 BACKUP
    priority 100  # 另一台主机上为 80
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123123
    }
    virtual_ipaddress {
        10.0.0.100/24 dev eth0 label eth0:1
    }
    track_script {
        chk_haproxy
    }
}
eof
systemctl  restart keepalived.service  && systemctl  status keepalived.service
hostname -I
```

备用节点 （10.0.0.102）

```powershell
cat > /etc/keepalived/keepalived.conf <<eof
global_defs {
    router_id ka2
}

vrrp_instance VI_1 {
    interface eth0
    virtual_router_id 66
    state BACKUP
    priority 80  
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123123
    }
    
    virtual_ipaddress {
        10.0.0.100/24 dev eth0 label eth0:1
    }
}
eof
systemctl  restart keepalived.service  && systemctl  status keepalived.service
hostname -I
# 浏览器访问haproxy的管理页,用户名/密码:admin/123123
http://10.0.0.100:9999/haproxy-status
# 可以看到三个节点都是up状态
```

测试

```powershell
# 创建配置，10.0.0.100的LB的VIP地址 （在 Windows 的终端 CMD 输入指令）
curl -X POST "http://10.0.0.100:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
# 在三台任意一个节点 web 查看配置管理的信息变化 10.0.0.201:8080/
# 可以在配置管理中看到 nacos.cfg.dataid 的ID ,到此 Nacos 的高可用架构就算完成了！
```

###### Prometheus 监控

> 默认没有开启Prometheus监控

- 配置打开 Promethues 监控功能

```powershell
# Nacos 节点修改配置
vi /usr/local/nacos/conf/application.properties
# 取消下面行注释开启prometheus监控，指标路径：http://127.0.0.1:8848/nacos/actuator/prometheus
management.endpoints.web.exposure.include=prometheus,health
# 重启服务
systemctl restart nacos
# 验证访问
curl -s http://127.0.0.1:8848/nacos/actuator/prometheus | grep -v '^#'|grep nacos
```

-  配置 Prometheus 监控 Nacos

```powershell
cat >> /usr/local/prometheus/conf/prometheus.yml <<eof
- job_name: 'nacos'
  metrics_path: '/nacos/actuator/prometheus'
  static_configs:
    - targets: ["nacos1:8848","nacos2:8848","nacos3:8848"]
eof
# 编辑hosts文件
vi /etc/hosts
10.0.0.201 nacos1
10.0.0.202 nacos2
10.0.0.203 nacos3
```

#### Nacos 项目服务注册和发现

##### 角色

- Nacos 服务：10.0.0.100:8848
- 生产者：10.0.0.101:8001
- 消费者：10.0.0.102:8002

##### Nacos 部署

```powershell
apt update && apt install -y docker.io
# 配置 Docker 镜像加速器（如果还没配置）
cat > /etc/docker/daemon.json << 'EOF'
{
  "registry-mirrors": [
    "https://docker.mirrors.ustc.edu.cn",
    "https://hub-mirror.c.163.com",
    "https://mirror.baidubce.com",
    "https://registry.docker-cn.com"
  ]
}
EOF

systemctl restart docker

# 尝试拉取
docker pull nacos/nacos-server:v2.4.3
docker images
docker run --name nacos -e MODE=standalone -p 8848:8848 -p 9848:9848 -d --restart always nacos/nacos-server:v2.4.3
docker ps -a
```

##### 运行服务提供者的JAVA应用 ( 10.0.0.101 )

```powershell
# Nacos服务主机的名称解析
echo "10.0.0.100 nacos.duan.org" >> /etc/hosts

# 安装JDK21,17,8都支持
apt update && apt -y install openjdk-21-jdk
apt update && apt -y install openjdk-17-jdk
apt update && apt -y install openjdk-8-jdk
apt update &&apt -y install maven
mvn -v

# 配置nacos服务相关信息
root@hp-kp-101:~/nacos-provider/src/main/resources# ls
application.properties

cat /root/nacos-provider/src/main/resources/application.properties 
server.port=8001
spring.application.name=nacos-provider-api
spring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848
# 编译打包
mvn clean package -Dmaven.test.skip=true
# 运行JAVA应用
java -jar /root/nacos-provider/target/nacos-provider-1.0-SNAPSHOT.jar
ss -ntlp|grep 8001
# 如果临时修改nacos地址信息 (因为我修改了地址为nacos.duan.org，所以用这个启动，否则启动失败！)
java -Dspring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848 -Dspring.cloud.nacos.config.server-addr=nacos.duan.org:8848 -jar /root/nacos-provider/target/nacos-provider-1.0-SNAPSHOT.jar
# 测试与 Nacos 通信是否正常
curl -v http://nacos.duan.org:8848/nacos/
# 登录 Nacos 网页，可以查看到服务管理——服务列表中有一个服务已经注册！
```

##### 运行服务消费者的JAVA应用 （ 10.0.0.102 ）

```powershell
# Nacos服务主机的名称解析
echo "10.0.0.100 nacos.duan.org" >> /etc/hosts

# 安装JDK21,17,8都支持
apt update && apt -y install openjdk-21-jdk
apt update && apt -y install openjdk-17-jdk
apt update && apt -y install openjdk-8-jdk
apt update &&apt -y install maven
mvn -v

# 准备JAVA应用
mkdir -p /root/nacos-consumer/src/main/resources && mkdir -p /root/nacos-consumer/target
# 配置nacos服务相关信息
cat > /root/nacos-consumer/src/main/resources/application.properties <<eof
server.port=8002
spring.application.name=nacos-consumer-api
spring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848
eof
# 操作是这样！但是我没有拿到源码包，只有编译打包好的jar包，上面那个也是如此！
cd /root/nacos-consumer &&  mvn clean package -Dmaven.test.skip=true
java -jar /root/nacos-consumer/target/nacos-consumer-1.0-SNAPSHOT.jar
ss -ntlp|grep 8002
# 如果临时修改nacos地址信息
java -Dspring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848 -Dspring.cloud.nacos.config.server-addr=nacos.duan.org:8848 -jar /root/nacos-consumer/target/nacos-consumer-1.0-SNAPSHOT.jar
# 测试与 Nacos 通信是否正常
curl -v http://nacos.duan.org:8848/nacos/
# 登录 Nacos 网页，可以查看到服务管理——服务列表中共有两个服务已经注册！
# 也可以用Windows系统自带的CMD终端测试：
curl 10.0.0.101:8001/echo/duanxueli
curl 10.0.0.102:8001/echo/duanxueli
```

### Seata Server 安装

#### 二进制部署

#### 使用 Docker 部署



### Spring Cloud Gateway

```powershell
示例: 路由配置

#微服务项目中的application.yml内容
server:
    port: 10010 #网关端口
spring:
    application:
    name: gateway #服务名称
cloud :
    nacos:
    server-addr: localhost:8848 # nacos地址
gateway:
    routes: #网关路由配置
    - id: user-service #路由id,自定义值,必须唯一
    uri: lb:/userservice #路由的目标地址Lb表示负载均衡,后面跟Nacos中微服务名称
    # uri: http://127.0.0.1:8081 #路由的目标地址,如果使用http表示固定地址
    predicates: #路由断言,判断请求是否符合路由规则的条件
    - Path=/user/*# #这个是按照路径匹配,只要以/user/开头就符合要求 http://127.0.0.1:10010/user/xxx/yyy
    - id: order-service
    uri: lb:/orderservice #Nacos中微服务名称
    predicates:
    - Path=/order/*# #这个是按照路径匹配,只要以/order/开头就符合要求 http://127.0.0.1:10010/order/xxx/yyy
    filters:
    - StripPrefix=1 #从请求路径中删除第一个前缀/order,再转发给后端
    #原始请求: http://127.0.0.1:10010/order/xxx/yyy
    #后端微服务实际得到http://127.0.0.1:10010/xxx/yyy
```












## 常用指令

### Linux

```powershell
cat /etc/os-release
top
uptime
free -h
df -h
du -sh
uname -a
hostname -I
```

```powershell
df -Th
mount / umount
fdisk -l

```

```powershell
ll
ldd /usr/local/kafka/bin/kafka
cp -r /src /dest
mv a b
ls | grep "^test" | xargs -r rm -rf
mkdir -p a/b/c
touch file
ln -s /opt/a b
```

```powershell
ip add
ip route
ip link show
ip link set dev eth0 up
curl -I URL
curl -v URL
ss -tunlp
traceroute
dig 域名
```

```powershell
ps aux grep xxx
kill -9 PID
nohup command &
journalctl -u elasticsearch -f
```

```powershell
unzip a.zip
zip -r a.zip a/
tar -zxvf a.tar.gz
tar -zcvf a.tar.gz a/
```

```powershell
chmod 755 file
chown user:group file
```

```powershell
tail -f xxx.log
tail -n 20 xxx.log
```

```powershell
apt purge <包名> && apt autoremove
userdel -r minio
```



### 三剑客

> - 使用 `grep` 的 `-A` (After) 参数来显示匹配行**之后**的指定行数。

```powershell
egrep -v "^$|^#" /root/test
grep -R keyword /path
grep -A 5 '^administrator' 文件名
find /path -name "*keyword*"
```

```powershell
awk -F ":" '{print $1}' /etc/passwd
awk -F '[ :]' '{print $1, $3}' file.txt
awk '$3 > 1000 {print $1,$3}' /etc/passwd
```

```powershell
sed -i 's/old/new/g' file
sed -i '$d' file
```



### 待收录

```powershell
update-alternatives --config java
timedatectl set-ntp true
ps aux --sort=-%cpu | head
du -sh * | sort -h | tail
!tail
journalctl -u 服务
tail -100f 日志路径
date -R
date -s '2025-12-06 13:10:10'
ll /etc/localtime
ll -svf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
# 取上一个命令的最后一个参数 （有些终端也可以是：Alt .）
Esc .
```

```powershell
docker save -o [文件名].tar [镜像名]:[标签]
docker load -i [文件名].tar
```



### VIM

```powershell
vim /root/test +77
dwi
:set nu
:set cursorcolumn
:set colorcolumn=
:10G
vim +10 test
```

### 奇淫巧技

```powershell
qpdf --password=Magedu@M65! --decrypt 文件输入路径 文件输出路径
```

