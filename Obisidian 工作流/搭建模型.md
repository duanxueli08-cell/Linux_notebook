

# 搭建模型

## Windows 系统

下载网址之一：[Ollama](https://ollama.com/)
先下载引擎，然后下载模型
```
# 查看版本
ollama version
# 下载模型，刚开始推荐阿里的通义千问
ollama run qwen2.5:7b     # 需要 8G 的显存门槛；
ollama run qwen2.5:3b     # 4G 的显存就可以玩；
# 后续与模型交互的指令；（回答完问题就会自动退出）
ollama run qwen2.5:3b "请给出查看 Linux 系统平均负载的三个命令"
```

```powershell
# 编写简单的批处理脚本 (.bat) —— 一劳永逸
# 新建一个文件叫 qw.bat 内容如下：
@echo off
ollama run qwen2.5:3b %*
# 把这个 qw.bat 所在的文件夹路径 C:\Users\Administrator\Documents\qw-bat 加入到 Windows 的 系统环境变量 PATH 中
# 效果：在任何地方输入 qw "问题内容" 即可
```

模型推荐：（同样适用于 Linux ）

| **模型名称**            | **适合场景**           | **运行表现 (在你的虚拟机里)**                          |
| ----------------------- | ---------------------- | ------------------------------------------------------ |
| **Qwen2.5:3b**          | 日常对话、简单代码     | **极快**。像真人打字一样流畅。                         |
| **Qwen2.5:7b**          | 复杂逻辑、运维脚本编写 | **中等**。每秒出 3-5 个字，思考质量更高。              |
| **Llama-3.2:3b**        | 英文处理、通用任务     | **极快**。Meta 出品，兼容性极好。                      |
| **DeepSeek-R1:1.5b/7b** | 深度思考、逻辑推理     | **1.5b 秒回，7b 略慢**。最近非常火，适合解决疑难杂症。 |

示例：

```
PS C:\Users\Administrator> ollama version
Error: unknown command "version" for "ollama"
PS C:\Users\Administrator> ollama --version
ollama version is 0.13.5
PS C:\Users\Administrator> ollama run qwen2.5:7b
pulling manifest
pulling 2bada8a74506:  20% ▕███████████                                               ▏ 951 MB/4.7 GB   10 MB/s   6m10s
PS C:\Users\Administrator> ollama run qwen2.5:3b
pulling manifest
pulling 5ee4f07cdb9b: 100% ▕██████████████████████████████████████████████████████████▏ 1.9 GB
pulling 66b9ea09bd5b: 100% ▕██████████████████████████████████████████████████████████▏   68 B
pulling eb4402837c78: 100% ▕██████████████████████████████████████████████████████████▏ 1.5 KB
pulling b5c0e5cf74cf: 100% ▕██████████████████████████████████████████████████████████▏ 7.4 KB
pulling 161ddde4c9cd: 100% ▕██████████████████████████████████████████████████████████▏  487 B
verifying sha256 digest
writing manifest
success
>>>
>>> 你好
你好！有什么我能帮助你的吗？

>>> 请介绍一下自己
当然可以。我是Qwen，由阿里云开发的超大规模语言模型。我的设计目的是为了提供各种信息和帮助用户解决他们可能遇到的问题或
满足他们的需求。我可以生成文本、回答问题、创作故事、编写代码以及完成其他复杂的任务。

我在经过大量的训练数据学习后，能够理解和处理自然语言，并且具备在特定领域进行有效沟通的能力。此外，我还具有逻辑推理能
力，能够在给定的信息下做出合理的推论和决策。我始终致力于为用户带来便利和价值，帮助他们更好地利用信息和技术来提高效率
和个人发展。

无论是关于科技、历史、科学还是日常生活中的任何问题，只要你告诉我，我会尽力提供准确且有价值的答案或解决方案。

>>>
```







原生 Linux 是跑模型的王者，Windows 只是它的“体验卡”。

| **维度**             | **Windows (Native/Ollama)**  | **Linux (Ubuntu/Fedora)**       | **胜出者** |
| -------------------- | ---------------------------- | ------------------------------- | ---------- |
| **推理速度**         | 表现不错，但有背景进程干扰   | **快 15%-30%** (取决于模型大小) | **Linux**  |
| **内存管理**         | 比较“贪婪”，系统预留多       | **极度精准**，几乎全部喂给模型  | **Linux**  |
| **显存 (VRAM) 利用** | 容易遇到 WDDM 驱动层限制     | **原生 CUDA 驱动直接打通**      | **Linux**  |
| **稳定性**           | 容易因系统更新或后台任务掉速 | **坚如磐石**，适合 7x24 运行    | **Linux**  |
| **WSL2 性能**        | 方便，但有虚拟化层损耗       | (原生环境无需虚拟化)            | **Linux**  |



## Linux 系统

这里是马哥教育，不是霍格沃兹学院，不教魔法！

### 虚拟机配置建议 (Ubuntu 22.04/24.04 LTS)

为了在跑模型的同时，Windows 宿主机不卡死，建议这样分配资源：

- **CPU (处理器):** 分配 **4 - 6 个核心**。
  - 你的 i7-8750H 是 6 核 12 线程。分配 6 个核心给虚拟机能保证推理速度，留 6 个线程给 Windows 刷网页和看文档。
- **内存 (RAM):** 分配 **16GB**。
  - 你有 32GB 总量，分一半给 Ubuntu。这样你可以稳跑 7b（70亿参数）规模的模型，甚至可以尝试 14b 的精简版。
- **硬盘:** **50GB - 100GB** (固态硬盘 S790 分区)。
  - 每个模型文件大约 2GB-10GB，多留点空间放 Docker 镜像和日志。
- **虚拟化技术:** 务必在 BIOS 中开启 **Intel VT-x**，并在虚拟机设置里勾选“加速虚拟化”相关选项。

### AI 引擎推荐：Ollama (Linux 版)

- **推荐理由：** 它是目前的“行业标准”工具，一行命令安装，一行命令跑模型。在 Linux 下，它的内存管理比 Windows 版更高效。
- **安装命令：**

```powershell
curl -fsSL https://ollama.com/install.sh | sh
systemctl status ollama
ss -tulpn | grep 11434
ollama run deepseek-r1:1.5b
ollama run deepseek-r1:1.5b "请给出查看 Linux 系统平均负载的三个命令"
```

```powershell
回退
# 首先清除模型，然后停止引擎，最后卸载引擎；
ollama rm deepseek-r1:1.5b && ollama list
systemctl stop ollama && rm /etc/systemd/system/ollama.service 
# 删除服务配置文件、二进制文件（程序主体）
rm $(which ollama) && rm $(which ollama)
# 删除模型数据
rm -rf /usr/share/ollama
# 删除之前创建的 ollama 用户和组
userdel ollama
groupdel ollama
```

