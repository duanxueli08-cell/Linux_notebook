##   微服务

[[Docker]]
### 前言

- 将一个大型复杂应用程序拆分成多个小型、松耦合、可独立部署的服务的架构风格。
    

每个微服务具备以下典型特征（9大特征，几乎所有定义都绕不开这几点）：

|特征|具体含义|
|---|---|
|按业务能力拆分|按领域驱动设计（DDD）的Bounded Context划分，而不是按技术层（如MVC）|
|独立进程|每个服务独立运行在一个或多个进程里（通常是容器/Docker）|
|独立部署|改一个服务只重新部署这一个服务，不影响其他|
|独立数据库|Database per Service（每个服务拥有自己的数据库，互不共享表）|
|轻量级通信|通常用HTTP/REST、gRPC、消息队列（Kafka/RabbitMQ）|
|技术异构|一个服务可以用Java，另一个可以用Go、Node.js、Python，随意选|
|去中心化治理|没有统一的技术标准，也没有中央ESB总线|
|高容错与自动化|必须配合服务发现、熔断、限流、链路追踪、CI/CD流水线才能玩得转|
|产品而非项目思维|每个微服务由一个小型跨职能团队（5-10人）长期负责到底|

#### 契机

微服务架构产生的真正契机（痛点驱动）。微服务不是理论家坐在办公室里想出来的，而是互联网公司被现实打出来的“血泪进化史”。

|阶段|当时真实痛点（单体架构的灾难）|典型公司与年份|结果/转折点|
|---|---|---|---|
|2005年以前|单体还行，代码量小，团队小，部署慢点还能忍|传统企业|没动力拆|
|2008-2012|流量暴涨、功能暴涨，单体代码几十万→上百万行|淘宝、Amazon、Netflix|编译40分钟，改一行代码全站重启，崩溃频发|
|2011-2012|几百人同时改一个代码库，合并冲突、回归测试地狱|Netflix（2011已拆100+服务）|发现“只有拆成小服务才能活”|
|2012-2013|高峰期只能整体扩容，10%的热点功能拖垮90%的冷功能|Amazon、Gilt、Uber|必须做到“单独扩容某个服务”|
|2013-2014|Docker出现（2013），突然“独立部署”有了技术载体|Docker发布|微服务终于可以低成本落地|
|2014|Martin Fowler & James Lewis 发表《Microservices》论文|正式命名|微服务从“民间实践”变成“官方架构风格”|
|2014-2015|Kubernetes（2014）、服务网格、CI/CD成熟|Google开源K8s|大规模微服务集群变得可运维|
|2015年以后|国内互联网公司全面跟进|阿里、腾讯、京东、美团、滴滴|微服务成为云计算时代的事实标准|

代表性里程碑：

- 2011年：Netflix 已经跑了几百个微服务，每天部署几千次
    
- 2013年：Amazon 公开说“我们早就全部微服务化了”
    
- 2014年：Martin Fowler 正式给它取名“Microservices”
    
- 2015年：国内双11第一次全面依赖微服务架构（阿里、京东）
    

总结： 微服务不是为了好看，而是**在业务复杂度、团队规模、流量规模、迭代速度四重压力下，单体架构彻底失灵后产生的必然进化结果**。容器和DevOps工具的成熟，才让这种进化真正落地。

### 演变

- 从“一坨全放一起” → “分层” → “粗拆服务” → “细拆微服务”
    

|阶段|架构名称|核心特征|典型年代|主要解决什么问题|典型痛点（为什么又进化了）|
|---|---|---|---|---|---|
|1. 巨石时代|单体架构（Monolithic）|所有代码打包成一个大WAR/JAR，全部功能混在一起|2000年前|开发简单，初期快|代码大了以后编译慢、部署慢、改一行重启全站|
|2. 分层时代|MVC三层架构|前端+业务逻辑+数据库分层，但还在同一个项目里|2000~2008|代码结构更清晰|项目还是一个整体，规模大了依然改不动、扩不动|
|3. 粗拆时代|SOA面向服务架构|把系统拆成几个大服务（订单系统、用户系统），用ESB总线连|2008~2013|支持跨部门复用，解决重复造轮子|ESB太重、部署仍需协调、版本耦合、扩展性仍差|
|4. 细拆时代|微服务架构|拆成几十上百个小服务，每个服务独立部署、技术随便选|2014年至今|独立部署、独立扩容、技术自由、团队自治|分布式复杂性（调用链、事务、监控）大幅上升|

业务越复杂、团队越大、流量越高、迭代越快，就越要把系统拆得越小——从“一坨”→“三层”→“几个大服务”→“几百个小服务”。

### 优缺点

**优点（4个字）**：快、活、韧、省

- 快：独立部署、快速迭代
    
- 活：技术自由、团队自治
    
- 韧：故障隔离、高可用
    
- 省：热点服务单独扩容，省机器
    

**缺点（也是4个字）**：难、乱、多、贵

- 难：分布式事务、调用链调试难
    
- 乱：服务太多，治理难
    
- 多：运维、监控、日志成本翻倍
    
- 贵：前期投入大（人+基础设施）
    

### 微服务技术栈

- 当前最常见的微服务技术栈（2025年主流两套）
    

|层级|Java系（国内90%大厂）|Go/云原生系（新兴&中小厂）|
|---|---|---|
|微服务框架|Spring Cloud Alibaba|Go Micro / Istio + gRPC|
|服务注册发现|Nacos|Consul / Kubernetes Service|
|配置中心|Nacos / Apollo|Nacos / ConfigMap|
|网关|Spring Cloud Gateway / Zuul|Kong / Traefik / Envoy|
|熔断限流|Sentinel|Hystrix → Resilience4j / Istio|
|链路追踪|SkyWalking / Zipkin|Jaeger / OpenTelemetry|
|消息队列|RocketMQ|Kafka / NATS|
|容器编排|Kubernetes（K8s）|Kubernetes（K8s）|
|CI/CD|Jenkins / GitLab CI|ArgoCD + Tekton|
|典型代表公司|阿里、京东、腾讯、字节、美团|抖音部分团队、网易、云原生新公司|

国内90%互联网公司实际在用：Spring Boot + Spring Cloud Alibaba + Nacos + Sentinel + RocketMQ + K8s，这就是传说中的“SCA全家桶”。

### 微服务架构模式

- 最常见的3种微服务架构模式（2025年）
    

1. **经典Spring Cloud模式**（国内最常见） Spring Boot + Nacos + Gateway + OpenFeign + Sentinel + SkyWalking + Docker + K8s → 阿里、京东、拼多多、美团点评都在用
    
2. **Service Mesh模式**（下一代） 业务代码零入侵，所有流量走Sidecar（Istio/Envoy） → 字节跳动、腾讯云、华为云大规模落地
    
3. **Go + 云原生极简模式** Go Micro / Gin + Kubernetes + gRPC + Jaeger → 新创业公司、游戏公司、部分抖音业务
    

### 后微服务时代

**2014～2018 年 = 前微服务时代** 主题：怎么把单体拆成微服务？（Spring Cloud、Dubbo 大杀器）

**2018 年以后 = 后微服务时代** 主题：拆完了，几十上百个服务怎么管？怎么跑得又快又稳又省钱？

后微服务时代的三大核心命题 → 也正好对应三大新物种：

|命题|痛点来源|标志性产物|代表年份|
|---|---|---|---|
|1. 几百个服务怎么管通信？|调用链乱、熔断限流到处写|Service Mesh（Istio）|2018～|
|2. 几百个容器怎么管资源？|手动扩缩容、机器利用率低|Kubernetes Operator + 云原生|2019～|
|3. 能不能连容器都不管？|运维太重、冷启动太慢|Serverless（FaaS + Knative）|2020～|

Service Mesh 与 Serverless 概念比较

|项目|服务网格（Service Mesh）|无服务器（Serverless）|
|---|---|---|
|核心理念|“把微服务之间的通信全部接管”|“连服务器都不想管，我只写函数”|
|你管什么|只写业务代码，通信/熔断/监控全交给旁路Sidecar|只写业务函数，容器/扩缩容/冷启动全交给云厂商|
|典型代表|Istio、Linkerd、Envoy|AWS Lambda、阿里云函数计算、腾讯云SCF|
|部署方式|每个微服务旁边自动注入一个代理（Sidecar）|完全不部署服务器，上传函数代码就行|
|适用场景|已有大量微服务，想一键解决通信治理|事件驱动、短时任务、流量波动极大（如秒杀）|
|国内谁在用|字节跳动、腾讯、华为云、饿了么|支付宝小程序、抖音部分活动、微信小程序云开发|
|一句话评价|微服务的“超级外挂”，专治调用链各种乱|微服务的“终极懒人版”，连容器都不想维护|

Kubernetes 成为容器战争胜利者标志着后微服务时代的开端，但 Kubernetes 仍然没有能够完美解决全部的分布式问题！

|分布式顽疾|K8s 原生有没有完美解决？|现状（2025年）实际靠谁补洞|
|---|---|---|
|分布式事务（最终一致性）|完全没解决|靠业务代码+Saga、TCC、RocketMQ 事务消息手动实现|
|服务调用链追踪|只给 Pod 日志，没调用链|SkyWalking、Jaeger、OpenTelemetry|
|熔断、限流、降级|没有|Sentinel、Istio、Resilience4j|
|服务发现与负载均衡|有，但很原始|Nacos、Consul + L4/L7 智能路由|
|配置统一管理与动态刷新|ConfigMap 太弱|Nacos、Apollo、Spring Cloud Config|
|跨服务链路超时预算控制|没有|Istio 超时策略 + OpenTelemetry Trace|
|灰度发布、金丝雀、流量染色|Deployment 能凑合|Argo Rollouts + Istio/Flagger 真正好用|
|多集群、多地域服务治理|完全没解决|自研多集群联邦 + Istio Multi-Cluster|
|冷启动与极致弹性（秒级）|Pod 启动慢|Knative + Kourier 或直接上 Serverless|
|安全（mTLS、服务间加密）|要自己装证书|Istio 自动 mTLS 才是真香|

**真实大厂现在的做法（2023-2025）**

|公司|公开表态 / 真实做法|
|---|---|
|Amazon|CEO 亲自发邮件：新项目一律单体优先，能不拆就不拆|
|Shopify|公开喊了5年“我们后悔早拆微服务”，2024年把核心交易系统重新合回单体|
|字节跳动|新业务99%先用单体 + Gin/Fiber 起，火了再拆|
|阿里|淘系核心交易链2023年启动“回归单体”计划|
|Segment|经典案例：拆成500个微服务后崩溃，2020年全部合并回一个Rails单体|

**最终结论结语（记住这三句话就够了）**

1. **99% 的项目一辈子都用不着微服务**
    
2. **微服务不是目标，是“税”——你业务做大后不得不交的税**
    
3. **能用模块化单体解决的问题，绝不用微服务**
    

### ZooKeeper 2888/3888

#### 工作原理

ZooKeeper 采用的是推拉相结合的方式： 客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据。

ZooKeeper 具有以下两大特性: 客户端如果对ZooKeeper 的一个数据节点注册 Watcher监听，那么当该数据节点的内容或是其子节点列表发生变更时，ZooKeeper 服务器就会向已注册订阅的客户端发送变更通知。 对在ZooKeeper上创建的临时节点，一旦客户端与服务器之间的会话失效，那么该临时节点也就被自动清除。

- **推（Push）**：服务端只推送一个轻量级的“事件通知”，告诉客户端“有变化发生了”。这非常高效，网络开销小，服务端压力小。
    
- **拉（Pull）**：客户端收到通知后，自己决定何时以及如何获取最新数据。这给了客户端更大的灵活性，并且保证了数据的一致性（客户端总是主动去获取最新数据，避免了服务端推送可能带来的数据不一致或旧数据问题）。
    

#### 服务流程

1. 生产者启动
    
2. 生产者注册至zookeeper
    
3. 消费者启动并订阅频道
    
4. zookeeper 通知消费者事件
    
5. 消费者调用生产者
    
6. 监控中心负责统计和监控服务状态
    

![image-20251129102828036](file:///C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20251129102828036.png?lastModify=1764935320)

#### 单机安装部署

# 准备 JAVA 环境 (Ubuntu2404)  
apt update && apt -y install openjdk-21-jdk  
Java -version  
# 部署 ZooKeeper （这里使用二进制安装）  
wget https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.9.4/apache-zookeeper-3.9.4-bin.tar.gz  
tar xf /root/apache-zookeeper-3.9.4-bin.tar.gz -C /usr/local/  
ln -s /usr/local/apache-zookeeper-3.9.4-bin /usr/local/zookeeper  
注意：不支持软链接到/usr/local/bin 目录，此方法会导致配置文件无法找到，无法启动  
# 支持修改PATH变量  
echo 'PATH=/usr/local/zookeeper/bin:$PATH' > /etc/profile.d/zookeeper.sh  
. /etc/profile.d/zookeeper.sh  
ll /usr/local/zookeeper/  
ls /usr/local/zookeeper/bin/ && ls /usr/local/zookeeper/conf/  
# 准备配置文件、数据路径和日志路径  
cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg  
mkdir -p /usr/local/zookeeper/{data,logs}  
# 默认配置可不做修改,  
grep -v "#" /usr/local/zookeeper/conf/zoo.cfg  
tickTime=2000 #服务器与服务器之间的单次心跳检测时间间隔，单位为毫秒  
initLimit=10 #集群中leader 服务器与follower服务器初始连接心跳次数，即多少个 2000 毫秒  
syncLimit=5 #leader 与follower之间检测发送和应答的心跳次数，如果该follower在时间段5*2000不能与leader进行通信，此follower将不可用  
dataDir=/usr/local/zookeeper/data #自定义的zookeeper保存数据的目录  
dataLogDir=/usr/local/zookeeper/logs # 指定日志路径，强烈建议事务日志目录和数据目录分开  
clientPort=2181 #客户端连接 Zookeeper 服务器的端口，Zookeeper会监听这个端口，接受客户端的访问请求  
autopurge.snapRetainCount=3  # 只保留此最新3个快照和相应的事务日志,并分别保留在 dataDir 和 dataLogDir 中  
autopurge.purgeInterval=24  # 自动清理间隔是24小时；默认是 0，表示不开启自动清理功能  
# 提供prometheus监控功能  
metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider  
metricsProvider.httpHost=0.0.0.0  
metricsProvider.httpPort=7000  
metricsProvider.exportJvmInfo=true  
# 前台启动观察启动过程  
zkServer.sh start-foreground    
# 查看启动状态   
zkServer.sh status && ss -tunlp | egrep '7000|2181'  
# 查看是否能暴露指标  
curl 10.0.0.200:7000/metrics  
# 创建 service 文件  
cat > /lib/systemd/system/zookeeper.service <<EOF  
[Unit]  
Description=zookeeper.service  
After=network.target  
[Service]  
Type=forking  
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start  
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop  
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart  
[Install]  
WantedBy=multi-user.target  
EOF  
# 启动服务；查看服务状态  
systemctl start zookeeper && systemctl status zookeeper

####

#### 客户端访问

# 进入 zookeeper 客户端工具 (已经将bin目录下的文件在变量中定义了，所以这里直接运行！)  
zkCli.sh  
ls /zookeeper	# 查看文件目录  
get /zookeeper	# 查看文件内容；注意：文件夹既可以做目录也可以做文件存放数据；文件=文件夹  
create /sre		# 创建文件  
set /sre ceshi	# 对sre文件添加内容  
# 查看 zookeeper 数据存放路径  
tree /usr/local/zookeeper/data

图形化客户端

Linux 客户端

注意：此软件因年代久远不再更新，只支持 JAVA-8，不支持JAVA-11以上版本；  
且不支持Ubuntu20.04,但支持Ubuntu22.04和Rocky8  
# 范例： Ubuntu24.04 编译 zooinspector  
apt update && apt -y install openjdk-8-jdk maven  
# 查看已安装的 Java 版本，并选择 Java 版本  
root@ubuntu-200:~# update-alternatives --config java  
There are 2 choices for the alternative java (providing /usr/bin/java).  
  
  Selection    Path                                            Priority   Status  
------------------------------------------------------------  
* 0            /usr/lib/jvm/java-21-openjdk-amd64/bin/java      2111      auto mode  
  1            /usr/lib/jvm/java-21-openjdk-amd64/bin/java      2111      manual mode  
  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode  
  
Press <enter> to keep the current choice[*], or type selection number: 2  
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode  
# 检查 Java 版本  
java -version  
mvn -v  
# 编译加速   
# 参考网址：https://developer.aliyun.com/mirror/maven?spm=a2c6h.13651102.0.0.7be81b11KoLURJ  
vim /etc/maven/settings.xml  
<mirrors>  
    <mirror>  
      <!--阿里云镜像-->  
      <id>nexus-aliyun</id>  
      <mirrorOf>*</mirrorOf>  
      <name>Nexus aliyun</name>  
      <url>http://maven.aliyun.com/nexus/content/groups/public</url>  
    </mirror>  
<mirrors>  
  
git clone https://github.com/zzhang5/zooinspector.git  
cd zooinspector  
mvn clean package -Dmaven.test.skip=true  
chmod +x /root/zooinspector/target/zooinspector-pkg/bin/zooinspector.sh  
# 进入target目录下运行脚本  
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh  
# 此时会弹出一个图形界面，ip修改为zookeeper服所在的地址：10.0.0.200:2181  ，然后ok回车  
# 可以看到之前在zookeeper客户端中创建的文件以及文件内容；  
  
WindTerm —— 开启 X server ——   
WindTerm —— 标签栏：会话 —— 首选项 —— 会话设置 —— X11 —— 内部 X 显示 —— 保存

Windows 客户端

Java下载地址：[https://www.oracle.com/java/technologies/downloads/#license-lightbox](https://www.oracle.com/java/technologies/downloads/#license-lightbox)

# 打开 Windows powerShell 终端查看java环境： java -version  
# 如果没有java环境，根据上面的下载地址，下载 Windows 系统 Java 8 版本的包；  
ls /root/zooinspector/target  
# 将该目录下的 zooinspector-1.0-SNAPSHOT-pkg.tar 或者 zooinspector-pkg 文件夹传到 Windows 系统中；  
双击文件中bin目录下的bat文件：zooinspector.bat

#### 集群部署

###### 前言

- 基于 ZAB 协议，所以 zookeeper 集群节点数必须是不低于3的奇数；
    

节点角色状态：

- looking：寻找 Leader 状态，处于该状态需要进入选举流程
    
- leading：领导者状态，处于该状态的节点说明是角色已经是Leader
    
- following：跟随者状态，表示 Leader已经选举出来，当前节点角色是follower
    
- observer：观察者状态，表明当前节点角色是 observe
    

选举 ID：

- ZXID ：每个改变 Zookeeper状态的操作都会自动生成一个对应的zxid。ZXID最大的节点优先选为Leader
    
    - ZXID 值越大，数据越新；
        
- myid ：服务器的唯一标识(SID)，通过配置 myid 文件指定，集群中唯一,当ZXID一样时,myid大的节点优先选为Leader
    
- **epoch选举优先级最高，其次是zxid，myid的优先级最低**
    

在分布式系统中，有多种协议被设计来解决一致性问题，Paxos、Raft、ZAB 等分布式算法经常会被称作是“强一致性”的分布式共识协议；

- ZAB (原子广播协议) 保证了分布式过程中的消息顺序一致性和崩溃恢复能力。它主要用在主-备模式的系统中；
    
- Paxos （帕克索斯） 的描述和实现被公认为相对复杂，这也是Raft等其他一致性算法出现的原因。
    
- Raft通过选举算法确保了分布式系统中的领导者唯一性。所有的写操作都通过领导者完成，这样就可以确保所有复制节点上的数据一致性。
    

强一致指的是尽管系统内部节点可以存在不一致的状态，但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的；

###### 部署

- 10.0.0.200
    
- 10.0.0.201
    
- 10.0.0.202
    
- 三台 ubuntu2404；都安装JDK8或JDK11,JDK21
    

# 准备 JAVA 环境 (Ubuntu2404)  
apt update && apt -y install openjdk-21-jdk  
# 查看  
Java -version  
  
# 部署 ZooKeeper （这里使用二进制安装）  
wget https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.9.4/apache-zookeeper-3.9.4-bin.tar.gz  
tar xf /root/apache-zookeeper-3.9.4-bin.tar.gz -C /usr/local/  
ln -s /usr/local/apache-zookeeper-3.9.4-bin /usr/local/zookeeper  
echo 'PATH=/usr/local/zookeeper/bin:$PATH' > /etc/profile.d/zookeeper.sh  
. /etc/profile.d/zookeeper.sh  
ll /usr/local/zookeeper/  
ls /usr/local/zookeeper/bin/ && ls /usr/local/zookeeper/conf/  
cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg  
mkdir -p /usr/local/zookeeper/{data,logs}  
# 默认配置可不做修改,  
grep -v "#" /usr/local/zookeeper/conf/zoo.cfg  
tickTime=2000  
initLimit=10  
syncLimit=5  
dataDir=/usr/local/zookeeper/data  
dataLogDir=/usr/local/zookeeper/logs  
clientPort=2181  
autopurge.snapRetainCount=3    
autopurge.purgeInterval=24    
server.1=10.0.0.200:2888:3888  
server.2=10.0.0.201:2888:3888  
server.3=10.0.0.202:2888:3888  
# 格式: server.MyID服务器唯一编号=服务器IP:Leader和Follower的数据同步端口(只有leader才会打开):Leader和Follower选举端口(L和F都有)  
  
# 为当前Zookeeper节点分配一个在集群中唯一的数字标识，使节点能够正确识别自己在集群配置中的位置。  
echo 1 > /usr/local/zookeeper/data/myid  
# 创建 service 文件  
cat > /lib/systemd/system/zookeeper.service <<EOF  
[Unit]  
Description=zookeeper.service  
After=network.target  
[Service]  
Type=forking  
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start  
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop  
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart  
[Install]  
WantedBy=multi-user.target  
EOF  
  
# 三个节点配置文件相同  
scp /usr/local/zookeeper/conf/zoo.cfg 10.0.0.200:/usr/local/zookeeper/conf/zoo.cfg  
scp /usr/local/zookeeper/conf/zoo.cfg 10.0.0.202:/usr/local/zookeeper/conf/zoo.cfg  
# 为当前Zookeeper节点分配一个在集群中唯一的数字标识，使节点能够正确识别自己在集群配置中的位置。  
echo 2 > /usr/local/zookeeper/data/myid  
# 在 10.0.0.200 与 10.0.0.202 主机中分别配置数字标识  
echo 1 > /usr/local/zookeeper/data/myid  
echo 3 > /usr/local/zookeeper/data/myid  
  
# 启动服务，查看服务状态  
systemctl start zookeeper && systemctl status zookeeper  
/usr/local/zookeeper/bin/zkServer.sh status  
# 只有leader监听2888/tcp端口;follower会监听3888/tcp端口  
ss -tunlp | grep 888  
  
客户端（若没有，按照前面的进行客户端部署！）  
update-alternatives --config java  
# 进入target目录下运行脚本  
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh  
# 在 lead 节点中进行增删改，follow 节点不支持增删改，但是 lead 节点的增删改都会同步到 follow 节点中；

### Kafka 9092

#### 角色

- Producer：即生产者，消息的产生者，是消息的入口。负责发布消息到Kafka broker。
    
- Consumer：消费者，用于消费消息，即处理消息
    
- Broker：Broker是kafka实例，每个服务器上可以有一个或多个kafka的实例。
    
- Controller：是整个 Kafka 集群的管理者角色
    
- Topic ：消息的主题，可以理解为消息的分类；
    
    - 每个broker上都可以创建多个 topic
        
    - 一个Topic相当于数据库中的一张表,一条消息相当于关系数据库的一条记录
        
- Consumer group: 每个consumer 属于一个特定的consumer group
    
    - 同一topic的一条消息只能被同一个consumer group 内的一个consumer 消费，但多个 consumer group 可同时消费这一消息
        
- Partition：分区。表现形式就是一个一个的文件夹,该文件夹下存储该partition的数据和索引文件。
    
    - 同一个topic在不同的分区的数据是不重复的,一般Partition数不要超过节点数；
        
    - 同一个 partition 数据是有顺序的，但不同的 partition 则是无序的。
        

![image-20251129185748520](file:///C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20251129185748520.png?lastModify=1764935320)

Kafka的复制配置 Kafka的复制方式可以通过 acks 配置来实现：

1. acks=0：生产者发送消息后不等确认即认为成功。这种方式相当于没有复制，只有单个副本存储在Leader上。
    
2. acks=1：消息发送到Leader即认为成功，相当于异步复制。在这种情况下，生产者只需要等待Leader的确认，而不需要等待ISR（In-Sync Replicas，同步副本集）中其他副本的确认。
    
3. acks=all：需要等待ISR列表中所有同步的Slave都确认才认为成功，相当于同步复制。这种方式下，生产者需要等待ISR中所有副本都 写入成功后才认为消息发送成功。
    

总的来说，Kafka可以配置为提供强一致性，但这可能会影响其性能和吞吐量。在实际使用中，你需要根据自己的业务需求在一致性和性能之间进行权衡。

#### Kafka 工具

- 图形工具 Offset Explorer (Kafka Tool)
    
    - 下载链接：[https://www.kafkatool.com/download.html](https://www.kafkatool.com/download.html)
        
- 基于Web的Kafka集群监控系统 kafka-eagle （ 具体的看课件 ）
    

#### 单机部署

- 官方下载地址：[Apache Kafka](https://kafka.apache.org/downloads)
    
- 阿里云下载地址：[https://mirrors.aliyun.com/apache/kafka/3.9.1/kafka_2.13-3.9.1.tgz](https://mirrors.aliyun.com/apache/kafka/3.9.1/kafka_2.13-3.9.1.tgz)
    
- Kafka-v4.0 开始即将不再支持 Zookeeper
    
- Kafka 提供的脚本不支持创建软链接到/usr/local/bin/路径，只支持绝对路径或相对径执行
    
- Kafka 有内置的 zookeeper 服务，所以不需要再去单独部署 zookeeper 服务；
    

###### 基于 zookeeper 部署 kafka

wget https://mirrors.aliyun.com/apache/kafka/3.9.1/kafka_2.13-3.9.1.tgz  
# 注意主机名解析,默认主机名称反向解析IP进行访问  
root@ubuntu-200:~# hostname  
ubuntu-200  
echo "10.0.0.200 ubuntu-200" >> /etc/hosts  
apt update && apt install -y openjdk-21-jdk  
tar xf kafka_2.13-3.9.1.tgz -C /usr/local/  
cd /usr/local/ && ls  
ln -s /usr/local/kafka_2.13-3.9.1/ /usr/local/kafka && ls /usr/local/kafka  
# 修改配置文件，此目录无需手动创建，启动会自动创建  
sed -i "s#^dataDir.*#dataDir=/data/zookeeper#" /usr/local/kafka/config/zookeeper.properties  
# 临时启动  
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties  
# 修改配置文件，此目录无需手动创建，启动会自动创建  
sed -i "s#^log.dirs.*#log.dirs=/data/kafka-logs#" /usr/local/kafka/config/server.properties  
# 临时启动  
/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties  
# 查看端口以及图形化客户端  
ss -tunlp | grep 2181  
  
# 创建service文件  
cat > /lib/systemd/system/zookeeper.service <<EOF  
[Unit]  
Description=zookeeper.service  
After=network.target  
  
[Service]  
Type=forking  
ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh -daemon /usr/local/kafka/config/zookeeper.properties  
ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh  
ExecReload=/bin/kill -HUP \$MAINPID  
  
[Install]  
WantedBy=multi-user.target  
EOF  
# 创建service文件  
cat > /lib/systemd/system/kafka.service <<EOF  
[Unit]  
Description=kafka.service  
After=network.target zookeeper.service  
  
[Service]  
Type=forking  
ExecStart=/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties  
ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh  
ExecReload=/bin/kill -HUP \$MAINPID  
  
[Install]  
WantedBy=multi-user.target  
EOF  
  
systemctl  start zookeeper && systemctl  status zookeeper  
systemctl  start kafka && systemctl  status kafka  
# 补充：临时启动 zookeeper  
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties

###### 基于KRaft 部署 kafka单

- 主机：10.0.0.201
    
- 注意：Kafka-4.0.0 版本不再支持Zookeeper，使用Kraft
    

wget https://mirrors.aliyun.com/apache/kafka/4.1.1/kafka_2.13-4.1.1.tgz  
tar xf kafka_2.13-4.1.1.tgz -C /usr/local/  
cd /usr/local/ && ls  
ln -s kafka_2.13-4.1.1/ kafka  
# 生成集群唯一ID,Generate a Cluster UUID,集群内的多个节点需使用同一集群ID  
cd /usr/local/kafka && KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"  
echo KAFKA_CLUSTER_ID  
# 建议修改数据日志目录  
sed -i "s#^log.dirs.*#log.dirs=/data/kraft-combined-logs#" /usr/local/kafka/config/server.properties  
# 查看（对照）	ls /data/kraft-combined-logs  
# 格式化出事数据  
bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties  
# 查看（不出意外的话，就有内容生成了）	ls /data/kraft-combined-logs  
# Kafka 启动！  
bin/kafka-server-start.sh config/server.properties  
# 查看端口 9092 、 9093  
ss -tunlp   
# 基于service文件就不配置了，目前现网上主流还是以zookeeper为主的！

#### 集群部署

###### 基于 zookeeper 模式的集群

环境准备

- 注意：每个kafka节点的主机名称解析需要提前准备，否则会导致失败
    
    - node1：10.0.0.201
        
    - node2：10.0.0.202
        
    - node3：10.0.0.200
        

# 修改每个kafka节点的主机名  
hostnamectl set-hostname node1  
hostnamectl set-hostname node2  
hostnamectl set-hostname node3  
# 在所有kafka节点上实现主机名称解析  
cat >> /etc/hosts <<eof  
10.0.0.200 node3  
10.0.0.201 node1  
10.0.0.202 node2  
eof  
# 安装 JAVA  
apt update && apt -y install openjdk-21-jre  
java -version

tar xf kafka_2.13-3.9.1.tgz -C /usr/local/  
ln -s /usr/local/kafka_2.13-3.9.1/ /usr/local/kafka && ls /usr/local/kafka

mkdir -p /usr/local/kafka/data/  
# 集群配置必须配置时间相关 （三个节点就配置相同）  
vim /usr/local/kafka/config/zookeeper.properties  
#必须添加时间相关配置  
tickTime=2000  
initLimit=10  
syncLimit=5  
#保留下面内容  
clientPort=2181  
maxClientCnxns=0  
admin.enableServer=false  
#添加下面集群配置  
dataDir=/usr/local/kafka/data/  
server.1=10.0.0.201:2888:3888  
server.2=10.0.0.202:2888:3888  
server.3=10.0.0.200:2888:3888  
  
# 主机：10.0.0.200  
echo 3 > /usr/local/kafka/data/myid  
# 主机：10.0.0.201  
echo 1 > /usr/local/kafka/data/myid	  
# 主机：10.0.0.202  
echo 2 > /usr/local/kafka/data/myid	

# 各节点部署 Kafka 配置文件 （单机部署不需要改配置，但是集群部署必须要修改一些参数！）  
vi /usr/local/kafka/config/server.properties   
# 每个节点id号唯一 （10.0.0.200对应0；1对应1；2对应2）  
broker.id=0  
# kafka监听端口，默认9092 (每个节点写自身的ip地址)  
listeners=PLAINTEXT://10.0.0.200:9092  
log.dirs=/usr/local/kafka/data  
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.200:2181

# 创建service文件  
cat > /lib/systemd/system/zookeeper.service <<EOF  
[Unit]  
Description=zookeeper.service  
After=network.target  
  
[Service]  
Type=forking  
ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh -daemon /usr/local/kafka/config/zookeeper.properties  
ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh  
ExecReload=/bin/kill -HUP \$MAINPID  
  
[Install]  
WantedBy=multi-user.target  
EOF  
# 创建service文件  
cat > /lib/systemd/system/kafka.service <<EOF  
[Unit]  
Description=kafka.service  
After=network.target zookeeper.service  
  
[Service]  
Type=forking  
ExecStart=/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties  
ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh  
ExecReload=/bin/kill -HUP \$MAINPID  
  
[Install]  
WantedBy=multi-user.target  
EOF  
  
systemctl  start zookeeper && systemctl  status zookeeper  
systemctl  start kafka && systemctl  status kafka  
  
# 图形化客户端 （部署在了10.0.0.200主机）  
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties  
cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh  
# 进入图形客户端后，在brokers的ids文件夹中可以看到012这三个brokers编号！

下面对你提供的内容进行了**全面检查、修正错误、统一格式、补充说明、逻辑更顺**，并给出一份**最终可直接使用的 Kafka 读写+Topic 操作笔记**。

---

##### Topic 操作

###### 一、分区概念

- **分区数量尽量与节点（Broker）数量相等** 这样可以让每个 Broker 都承担 Leader 角色，提升整体吞吐性能。
    
- **Kafka 集群节点数量建议不少于 3 且为奇数** 便于进行选举，提高集群的高可用性。
    
- **副本数至少为 2（Leader + Follower）** Leader 负责读写，Follower 同步数据，当 Leader 宕机可自动切换。
    
- **分区 = 性能；副本 = 高可用** ✔ 分区数量越多 → 并发能力越高 ✔ 副本数量越多 → 数据容错能力越强
    

---

###### 二、创建 Topic

**创建流程（非常关键）**

- 你向 10.0.0.201 发出创建 Topic 请求
    
- 201 节点把创建请求转发给 **Kafka Controller**
    
- Controller 创建集群元数据中的 Topic duan
    
- Controller 向所有 Broker 下发创建指令
    

# 创建一个名字叫 duan 的 Topic  
# 分成 3 个分区（提高吞吐量）  
# 副本数为 2（高可用）  
# 新版用法需要 --bootstrap-server  
/usr/local/kafka/bin/kafka-topics.sh --create --topic duan --bootstrap-server 10.0.0.201:9092  --partitions 3 --replication-factor 2

**✔ 创建后数据落盘（目录变化说明）**

Kafka 各 broker 的数据目录一般为：

ls /usr/local/kafka/data

执行 ls /usr/local/kafka/data 你会看到类似：

- broker 0 的目录：duan-0 和 duan-2
    
- broker 1 的目录：duan-1 和 duan-2
    
- broker 2 的目录：duan-0 和 duan-1
    

说明数据分区在不同 broker 上分布，副本也同步成功。

**（可选）图形化查看 Topic**

cd /root/zooinspector/target && ./zooinspector-pkg/bin/zooinspector.sh

---

###### 三、查看 Topic 列表

/usr/local/kafka/bin/kafka-topics.sh  --list  --bootstrap-server 10.0.0.201:9092

---

###### 四、查看 Topic 详细信息（重点）

/usr/local/kafka/bin/kafka-topics.sh   --describe   --bootstrap-server 10.0.0.201:9092   --topic Shirley  
  
Topic: Shirley  TopicId: HMxVk-p5SF2O4bWWNlX0vw PartitionCount: 3       ReplicationFactor: 2    Configs:   
        Topic: Shirley  Partition: 0    Leader: 0       Replicas: 0,2   Isr: 0,2        Elr: N/ALastKnownElr: N/A  
        Topic: Shirley  Partition: 1    Leader: 2       Replicas: 2,1   Isr: 2,1        Elr: N/ALastKnownElr: N/A  
        Topic: Shirley  Partition: 2    Leader: 1       Replicas: 1,0   Isr: 1,0        Elr: N/ALastKnownElr: N/A

可以看到：

- 1 分区号，对应的 Leader 为 2 （这里的2指的是节点号），副本存放位置在 1节点中；（副本在1节点，那么2节点的follow就是1）
    

---

###### 五、生产消息（Producer）

正确命令：

/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server 10.0.0.201:9092  --topic duan

进入交互界面：

hello kafka  
this is message 1  
order_created

每敲一行就是一条消息。若此时你在消费信息的交互界面就能实时看到生产者发送的消息；

---

###### 六、消费消息（Consumer）

正确命令：

/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server 10.0.0.201:9092 --topic duan --from-beginning

- --from-beginning：从最早的 offset 开始读取
    

你会看到：

hello kafka  
this is message 1  
order_created

如果你继续在 producer 输入，consumer 会实时接收到新消息。

---

**消费组**

- 一个 Topic 也可以被多个不同组消费 ；
    
- 同一个组可以消费多个 Topic
    

/usr/local/kafka/bin/kafka-console-consumer.sh --topic duan --bootstrap-server 10.0.0.202:9092 --from-beginning --consumer-property group.id=group1  
# 启动一个消费组，这个消费组可以消费 Topic = duan 的消息；  
消费组的价值 不在“能否消费消息”，而在“消费的协调与进度管理”

1️⃣ **不指定消费组 vs 指定消费组的本质区别**

|方式|消费者行为|消息存储/offset|适用场景|
|---|---|---|---|
|**不指定 group**|临时消费者，每次启动都是全量消费 --from-beginning 或默认最新消息|Kafka 不记录消费进度|临时调试、测试、只想看看消息|
|**指定 group**|属于固定消费组，组内多消费者分摊分区|Kafka 会保存 offset，下次启动可以接着上次位置消费|真实业务生产环境，负载均衡，消息只被组内一个消费者处理|

###### 七、删除 Topic

**删除流程（非常关键）**

- 你向 10.0.0.201 发出删除 Topic 请求
    
- 201 节点把删除请求转发给 **Kafka Controller**
    
- Controller 删除集群元数据中的 Topic duan
    
- Controller 向所有 Broker 下发删除指令
    

/usr/local/kafka/bin/kafka-topics.sh --delete --bootstrap-server 10.0.0.201:9092  --topic duan

验证是否成功删除：

/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server 10.0.0.201:9092

- **不要手动删除 __consumer_offsets-* 等内部文件**
    
- 调整 **offsets.retention.minutes** 清理消费者偏移量
    
- 修改后，Kafka 会自动删除超过保留策略的日志文件。
    

# 按时间清理  
log.retention.hours=168       # 默认7天  
# 按大小清理  
log.retention.bytes=1073741824  # 1GB

##### 消息积压

为什么会发生消息积压？

- 消费者消费能力不足；
    
- 消息产生速度过快；
    
- 分区负载不均衡；
    
- 网络故障；硬件资源不足等
    

如何衡量消息积压？

Log End Offset - Current Offset = Lag > 0 ————> 消息积压

查看消费组详情及积压

/usr/local/kafka/bin/kafka-consumer-groups.sh   --bootstrap-server 10.0.0.201:9092   --describe   --group group1

输出示例：

GROUP     TOPIC    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG  CONSUMER-ID  
group1    duan     0          15              20              5    consumer-1

解释：

- CURRENT-OFFSET：消费组已消费到的偏移量
    
- LOG-END-OFFSET：主题当前的最新偏移量
    
- LAG：积压数量（未被消费的消息数）
    

> 积压就是 LOG-END-OFFSET - CURRENT-OFFSET。

### Nacos

主要功能：服务注册与发现、分布式配置管理 （目前主要学这两个）

下载链接：[Nacos Server 下载 | Nacos 官网](https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu)

服务提供者分类：

- 临时实例: 服务提供者定期周期主动向nacos 发送心跳监测报告,如果一段时间后,nacos无法收到心跳,则删除此实例；
    
- 非临时实例(永久实例): nacos 主动定时监测此类实例,如果提供者实例异常,则并不会删除只是标记此实例异常,等待此实例恢复
    

集群模式：

- Nacos 默认使用 AP 模式；存在非临时实例时,会采用CP模式；（AP：弱一致性；CP：强一致性）
    
- Eureka 采用 AP( Availability和Partiton tolerance)模式；
    
- CP模式基于强一致性协议 Raft
    
- AP模式基于阿里的Distro(基于Gossip和Eureka协议优化而来)最终一致性的AP 分布式协议
    

Nacos 支持三种部署模式

- 单机模式 - 用于测试和单机试用
    
- 集群模式 - 用于生产环境，确保高可用
    
- 多集群模式 - 用于多数据中心场景
    

环境准备

- 安装好 JDK，需要 1.8 及其以上版本
    
- 建议: 2核 CPU / 4G 内存 及其以上
    
- 建议: 生产环境 3 个节点 及其以上
    

下载链接：[Nacos Server 下载 | Nacos 官网](https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36nc4nT3)

2.X 版本与3.X 版本有较大的不同：

> 启动命令(standalone代表着单机模式运行，非集群模式):
> 
> sh startup.sh -m standalone
> 
> 如果您使用的是ubuntu系统，或者运行脚本报错提示[[符号找不到，可尝试如下运行：bash startup.sh -m standalone
> 
> 随后启动程序会提示您输入3个鉴权相关配置（Nacos从3.0.0版本开始默认启用控制台鉴权功能，因此如下3个鉴权相关配置必须填写）如下所示：
> 
> nacos.core.auth.plugin.nacos.token.secret.key is missing, please set with Base64 string: ${your_input_token_secret_key}  
> nacos.core.auth.plugin.nacos.token.secret.key Updated:  
> ----------------------------------  
> nacos.core.auth.server.identity.key is missing, please set: ${your_input_server_identity_key}  
> nacos.core.auth.server.identity.key Updated:  
> ----------------------------------  
> nacos.core.auth.server.identity.value is missing, please set: ${your_input_server_identity_key}  
> nacos.core.auth.server.identity.value Updated:  
> ----------------------------------
> 
> 若已经在 conf/application.properties 中设置过这3个配置，则不会提示输入。

> 老版本：10.0.200:8848/nacos
> 
> 新版本：10.0.0.200:8080

#### 单体部署

###### 二进制安装Nacos-2.X （Ubuntu2404）

apt update && apt -y install openjdk-8-jdk  
wget https://download.nacos.io/nacos-server/nacos-server-2.5.2.zip?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu&file=nacos-server-2.5.2.zip  
unzip nacos-server-2.5.2.zip -d /usr/local/  
# 添加PATH变量中，可选  
echo 'PATH=/usr/local/nacos/bin:$PATH' >> /etc/profile  
. /etc/profile  
# 测试：启动(standalone单机模式) ；关闭服务   
/usr/local/nacos/bin/startup.sh -m standalone	# 若做了上述的变量：bash startup.sh -m standalone  
bash shutdown.sh  
#  准备Sevice文件  
id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos  
chown -R nacos: /usr/local/nacos/ && ll /usr/local/nacos/  
cat > /lib/systemd/system/nacos.service <<eof  
[Unit]  
Description=nacos.service  
After=network.target  
[Service]  
Type=forking  
ExecStart=/usr/local/nacos/bin/startup.sh -m standalone  
ExecStop=/usr/local/nacos/bin/shutdown.sh  
User=nacos  
Group=nacos  
[Install]  
WantedBy=multi-user.target  
eof  
  
systemctl daemon-reload  
systemctl start nacos.service && systemctl status nacos.service  
# 查看日志，如果日志没有报错那服务启动就成功了！  
tail /usr/local/nacos/logs/startup.log  
# 浏览器访问：10.0.0.201:8848/nacos

###### 二进制安装Nacos-3.X （Ubuntu2404）

apt update && apt -y install openjdk-21-jdk  
wget https://hub.docker.com/r/nacos/nacos-server/tags?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu&page=1&name=3.1.1  
unzip nacos-server-3.1.1 -d /usr/local/  
# 添加PATH变量中 （可选）  
echo 'PATH=/usr/local/nacos/bin:$PATH' >> /etc/profile  
. /etc/profile  
# 测试：启动(standalone单机模式) ；关闭服务   
/usr/local/nacos/bin/startup.sh -m standalone	  
bash shutdown.sh  
这里就会显现差异了！Nacos 3.X 以后版本默认要求必须开启鉴权才能启动  
# 生成一个安全的 Base64 编码密钥（32字符以上）（如果使用默认密钥，这里就不用生成新的密钥了！）  
openssl rand -base64 32  
# 修改配置文件永久设置 （主要修改三行配置）  
vi /usr/local/nacos/conf/application.properties  
nacos.core.auth.server.identity.key=duan  
nacos.core.auth.server.identity.value=99  
...........   
nacos.core.auth.plugin.nacos.token.secret.key=VGhpc0lzTXlDdXN0b21TZWNyZXRLZXkwMTIzNDU2Nzg=

#  准备Sevice文件  
id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos  
chown -R nacos: /usr/local/nacos/ && ll /usr/local/nacos/  
cat > /lib/systemd/system/nacos.service <<eof  
[Unit]  
Description=nacos.service  
After=network.target  
[Service]  
Type=forking  
ExecStart=/usr/local/nacos/bin/startup.sh -m standalone  
ExecStop=/usr/local/nacos/bin/shutdown.sh  
User=nacos  
Group=nacos  
[Install]  
WantedBy=multi-user.target  
eof  
  
systemctl daemon-reload  
systemctl start nacos.service && systemctl status nacos.service  
# 查看日志，如果日志没有报错那服务启动就成功了！  
tail /usr/local/nacos/logs/startup.log  
# Nacos 3.0.0默认控制台使用8080端口,而8848端口用于API访问  
ss -tunlp |grep java  
# 浏览器访问：10.0.0.200:8080		账户密码都是：nacos

模拟：向Nacos发起服务注册申请

服务注册

# 服务注册 （临时实例）  
curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.duan.serviceName&ip=1.2.3.4&port=8080'  
# 再注册一个临时实例（对照）  
curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.duan.serviceName&ip=20.21.22.23&port=8090'  
# http://127.0.0.1:8848: Nacos 服务器地址和端口  
# serviceName=nacos.duan.serviceName: 要注册的服务名称  
# ip=1.2.3.4: 服务实例的 IP 地址  
# port=8080: 服务实例的端口号

服务发现

# 查询服务实例列表  
curl 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=nacos.duan.serviceName'  
  
"ephemeral": true - 注册的是临时实例  
需要持续心跳来维持注册状态  
心跳超时："instanceHeartBeatTimeOut": 15000 (15秒)  
心跳间隔："instanceHeartBeatInterval": 5000 (5秒)  
  
# 添加 ephemeral=false 参数明确删除持久化实例  
curl -X DELETE 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.duan.serviceName&ip=1.2.3.4&port=8080&ephemeral=false'

发布配置

curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"

获取配置

curl -X GET "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test"

#### **高可用集群部署**

- 下载链接：[https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36nc4nT3](https://nacos.io/download/nacos-server/?spm=5238cd80.47ee59c.0.0.189fcd36nc4nT3)
    
- 部署版本：nacos-server-3.1.1
    

###### 角色

|角色|IP 地址|备注|
|---|---|---|
|VIP|10.0.0.100|虚拟IP地址|
|haproxy1+keepalived|10.0.0.101|HAProxy 1 与 Keepalived 实例|
|haproxy2+keepalived|10.0.0.102|HAProxy 2 与 Keepalived 实例|
|nacos1|10.0.0.201|Nacos 服务节点1|
|nacos2|10.0.0.202|Nacos 服务节点2|
|nacos3|10.0.0.203|Nacos 服务节点3|
|MySQL|10.0.0.200|MySQL 数据库服务|

###### MySQL 服务器

apt update &&  apt install mysql-server  
# 修改远程监听端口  
sed -i '/127.0.0.1/s/^/#/' /etc/mysql/mysql.conf.d/mysqld.cnf  
systemctl restart mysql.service  
ss -tunlp |grep 3306  
mysql  
create database nacos;  
create user nacos@'%' identified with mysql_native_password by '123123';  
grant all on nacos.* to nacos@'%' ;

###### Nacos 服务器

- 在三台 nacos 服务器部署 nacos-server-3.1.1 版本的服务；（步骤参考上述的内容）
    
- nacos 服务默认数据库存储位置是在服务文件中的 data 目录下；（ 比如：/usr/local/nacos/data ）
    
- nacos 服务基于 MySQL 数据存储在 /usr/local/nacos/conf/mysql-schema.sql 文件中
    

# nacos 每个节点都需要下载 MySQL 客户端  
apt update && apt -y install mysql-client  
# Nacos1 服务器部署 MySQL 做为数据源  
# 导入数据库  
mysql -unacos -p123123 -h10.0.0.200 nacos < /usr/local/nacos/conf/mysql-schema.sql  
# 查看导入数据表  
mysql -unacos -p123123 -h10.0.0.200 -e 'show tables in nacos'  
# 重启服务后，进入浏览器 10.0.0.201:8080 显示初始化密码，这就证明数据库已经发生变化，初始化成功！  
# 查看 MySQL 数据库 users 表，会发现已经有一个用户 nacos 注册信息；  
mysql -unacos -p123123 -h10.0.0.200 -e "USE nacos; SELECT * FROM users;"

若使用MySQL数据源，需要对所有集群节点上修改下面文件 ;（若使用内置数据源无需修改配置）

vi /usr/local/nacos/conf/application.properties  
nacos.core.auth.caching.enabled=true    
# 在 3.1.1 版本中是默认开启的，在之前的老版本中需要手动开启；  
nacos.core.auth.server.identity.key=duan   
nacos.core.auth.server.identity.value=99   
nacos.core.auth.plugin.nacos.token.secret.key=VGhpc0lzTXlDdXN0b21TZWNyZXRLZXkwMTIzNDU2Nzg=   
# 在 3.1.1 版本中是默认开启的，在之前的老版本中需要手动添加这三段配置的字符；  
spring.sql.init.platform=mysql 	# 取消注释  
db.num=1 					  # 取消注释  
db.url.0=jdbc:mysql://10.0.0.200:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC  
db.user=nacos  
db.password=123123  
# 补充：这里使用的字符集是：utf8 ，与 MySQL 是兼容的，MySQL 的字符集是 utf8mb4 ，生产工作中建议：utf8mb4  
mysql -unacos -p123123 -h10.0.0.200 -e "show create database nacos;"

集群配置文件

cat > /usr/local/nacos/conf/cluster.conf <<eof  
# ip:port  
10.0.0.201:8848  
10.0.0.202:8848  
10.0.0.203:8848  
eof  
# 因为是集群模式，所以不能单独打开，需要修改配置文件  
sed -i 's|-m standalone||g' /lib/systemd/system/nacos.service  
# 所有节点同步配置  
chown -R nacos: /usr/local/nacos  
for ip in 202 203; do  
    scp -o StrictHostKeyChecking=no /usr/local/nacos/conf/application.properties 10.0.0.$ip:/usr/local/nacos/conf/application.properties  
done  
for ip in 202 203; do  
    scp -o StrictHostKeyChecking=no /usr/local/nacos/conf/cluster.conf 10.0.0.$ip:/usr/local/nacos/conf/cluster.conf  
done  
for ip in 202 203; do  
    scp -o StrictHostKeyChecking=no /lib/systemd/system/nacos.service 10.0.0.$ip:/lib/systemd/system/nacos.service  
done  
# 每个节点都要重启服务和重新加载配置文件  
systemctl  daemon-reload  
systemctl restart nacos && systemctl  status nacos  
浏览器 10.0.0.201:8080 集群管理————节点列表

###### 负载均衡服务器

配置 haproxy 和 keepalived 实现负载均衡和高可用

# 允许进程绑定到非本地IP地址（如VIP虚拟IP）  
echo net.ipv4.ip_nonlocal_bind = 1 >> /etc/sysctl.conf  
# 重新加载所有配置  
sysctl -p  
# 在两台服务器上安装配置haproxy实现负载均衡反向代理和高可用  
apt update && apt -y install haproxy  
# 编辑HAProxy配置文件  
vim /etc/haproxy/haproxy.cfg  
# 在配置文件中添加以下内容：  
cat >> /etc/haproxy/haproxy.cfg <<eof  
listen stats  
    mode http  
    bind 0.0.0.0:9999  
    stats enable  
    log global  
    stats uri /haproxy-status  
    stats auth admin:123123  
  
listen nacos-8848  
    bind 10.0.0.100:8848  
    server nacos01 10.0.0.201:8848 check  
    server nacos02 10.0.0.202:8848 check  
    server nacos03 10.0.0.203:8848 check  
  
listen nacos-9848  
    mode tcp  
    bind 10.0.0.100:9848  
    server nacos01 10.0.0.201:9848 check  
    server nacos02 10.0.0.202:9848 check  
    server nacos03 10.0.0.203:9848 check  
eof  
  
# 重新加载HAProxy配置  
systemctl reload haproxy   
systemctl start haproxy  && systemctl status haproxy 

配置keepalived实现高可用

# 在两台服务器上安装配置keepalived实现高可用  
apt update && apt -y install keepalived  
# 修改配置文件  
# 这台服务器优先级设置为100，如果这个服务有异常，就会在原有的服务优先级数值减30；  
# 另一台服务器优先级设置为80，那么服务器发生异常，优先级发生变化，主备就会发生切换！  
cat > /etc/keepalived/keepalived.conf <<eof  
global_defs {  
    router_id ka1  # 另一台主机上为 ka2  
}  
  
vrrp_script chk_haproxy {  
    script "killall -0 haproxy"  # cheaper than pidof  
    interval 1  
    weight -30  
}  
  
vrrp_instance VI_1 {  
    interface eth0  
    virtual_router_id 66  
    state MASTER  # 另一台主机上为 BACKUP  
    priority 100  # 另一台主机上为 80  
    advert_int 1  
    authentication {  
        auth_type PASS  
        auth_pass 123123  
    }  
    virtual_ipaddress {  
        10.0.0.100/24 dev eth0 label eth0:1  
    }  
    track_script {  
        chk_haproxy  
    }  
}  
eof  
systemctl  restart keepalived.service  && systemctl  status keepalived.service  
hostname -I

备用节点 （10.0.0.102）

cat > /etc/keepalived/keepalived.conf <<eof  
global_defs {  
    router_id ka2  
}  
  
vrrp_instance VI_1 {  
    interface eth0  
    virtual_router_id 66  
    state BACKUP  
    priority 80    
    advert_int 1  
    authentication {  
        auth_type PASS  
        auth_pass 123123  
    }  
      
    virtual_ipaddress {  
        10.0.0.100/24 dev eth0 label eth0:1  
    }  
}  
eof  
systemctl  restart keepalived.service  && systemctl  status keepalived.service  
hostname -I  
# 浏览器访问haproxy的管理页,用户名/密码:admin/123123  
http://10.0.0.100:9999/haproxy-status  
# 可以看到三个节点都是up状态

测试

# 创建配置，10.0.0.100的LB的VIP地址 （在 Windows 的终端 CMD 输入指令）  
curl -X POST "http://10.0.0.100:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"  
# 在三台任意一个节点 web 查看配置管理的信息变化 10.0.0.201:8080/  
# 可以在配置管理中看到 nacos.cfg.dataid 的ID ,到此 Nacos 的高可用架构就算完成了！

###### Prometheus 监控

> 默认没有开启Prometheus监控

- 配置打开 Promethues 监控功能
    

# Nacos 节点修改配置  
vi /usr/local/nacos/conf/application.properties  
# 取消下面行注释开启prometheus监控，指标路径：http://127.0.0.1:8848/nacos/actuator/prometheus  
management.endpoints.web.exposure.include=prometheus,health  
# 重启服务  
systemctl restart nacos  
# 验证访问  
curl -s http://127.0.0.1:8848/nacos/actuator/prometheus | grep -v '^#'|grep nacos

- 配置 Prometheus 监控 Nacos
    

cat >> /usr/local/prometheus/conf/prometheus.yml <<eof  
- job_name: 'nacos'  
  metrics_path: '/nacos/actuator/prometheus'  
  static_configs:  
    - targets: ["nacos1:8848","nacos2:8848","nacos3:8848"]  
eof  
# 编辑hosts文件  
vi /etc/hosts  
10.0.0.201 nacos1  
10.0.0.202 nacos2  
10.0.0.203 nacos3

#### Nacos 项目服务注册和发现

##### 角色

- Nacos 服务：10.0.0.100:8848
    
- 生产者：10.0.0.101:8001
    
- 消费者：10.0.0.102:8002
    

##### Nacos 部署

apt update && apt install -y docker.io  
# 配置 Docker 镜像加速器（如果还没配置）  
cat > /etc/docker/daemon.json << 'EOF'  
{  
  "registry-mirrors": [  
    "https://docker.mirrors.ustc.edu.cn",  
    "https://hub-mirror.c.163.com",  
    "https://mirror.baidubce.com",  
    "https://registry.docker-cn.com"  
  ]  
}  
EOF  
  
systemctl restart docker  
  
# 尝试拉取  
docker pull nacos/nacos-server:v2.4.3  
docker images  
docker run --name nacos -e MODE=standalone -p 8848:8848 -p 9848:9848 -d --restart always nacos/nacos-server:v2.4.3  
docker ps -a

##### 运行服务提供者的JAVA应用 ( 10.0.0.101 )

# Nacos服务主机的名称解析  
echo "10.0.0.100 nacos.duan.org" >> /etc/hosts  
  
# 安装JDK21,17,8都支持  
apt update && apt -y install openjdk-21-jdk  
apt update && apt -y install openjdk-17-jdk  
apt update && apt -y install openjdk-8-jdk  
apt update &&apt -y install maven  
mvn -v  
  
# 配置nacos服务相关信息  
root@hp-kp-101:~/nacos-provider/src/main/resources# ls  
application.properties  
  
cat /root/nacos-provider/src/main/resources/application.properties   
server.port=8001  
spring.application.name=nacos-provider-api  
spring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848  
# 编译打包  
mvn clean package -Dmaven.test.skip=true  
# 运行JAVA应用  
java -jar /root/nacos-provider/target/nacos-provider-1.0-SNAPSHOT.jar  
ss -ntlp|grep 8001  
# 如果临时修改nacos地址信息 (因为我修改了地址为nacos.duan.org，所以用这个启动，否则启动失败！)  
java -Dspring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848 -Dspring.cloud.nacos.config.server-addr=nacos.duan.org:8848 -jar /root/nacos-provider/target/nacos-provider-1.0-SNAPSHOT.jar  
# 测试与 Nacos 通信是否正常  
curl -v http://nacos.duan.org:8848/nacos/  
# 登录 Nacos 网页，可以查看到服务管理——服务列表中有一个服务已经注册！

##### 运行服务消费者的JAVA应用 （ 10.0.0.102 ）

# Nacos服务主机的名称解析  
echo "10.0.0.100 nacos.duan.org" >> /etc/hosts  
  
# 安装JDK21,17,8都支持  
apt update && apt -y install openjdk-21-jdk  
apt update && apt -y install openjdk-17-jdk  
apt update && apt -y install openjdk-8-jdk  
apt update &&apt -y install maven  
mvn -v  
  
# 准备JAVA应用  
mkdir -p /root/nacos-consumer/src/main/resources && mkdir -p /root/nacos-consumer/target  
# 配置nacos服务相关信息  
cat > /root/nacos-consumer/src/main/resources/application.properties <<eof  
server.port=8002  
spring.application.name=nacos-consumer-api  
spring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848  
eof  
# 操作是这样！但是我没有拿到源码包，只有编译打包好的jar包，上面那个也是如此！  
cd /root/nacos-consumer &&  mvn clean package -Dmaven.test.skip=true  
java -jar /root/nacos-consumer/target/nacos-consumer-1.0-SNAPSHOT.jar  
ss -ntlp|grep 8002  
# 如果临时修改nacos地址信息  
java -Dspring.cloud.nacos.discovery.server-addr=nacos.duan.org:8848 -Dspring.cloud.nacos.config.server-addr=nacos.duan.org:8848 -jar /root/nacos-consumer/target/nacos-consumer-1.0-SNAPSHOT.jar  
# 测试与 Nacos 通信是否正常  
curl -v http://nacos.duan.org:8848/nacos/  
# 登录 Nacos 网页，可以查看到服务管理——服务列表中共有两个服务已经注册！  
# 也可以用Windows系统自带的CMD终端测试：  
curl 10.0.0.101:8001/echo/duanxueli  
curl 10.0.0.102:8001/echo/duanxueli

### Seata Server 安装

#### 二进制部署

#### 使用 Docker 部署

### Spring Cloud Gateway

示例: 路由配置  
  
#微服务项目中的application.yml内容  
server:  
    port: 10010 #网关端口  
spring:  
    application:  
    name: gateway #服务名称  
cloud :  
    nacos:  
    server-addr: localhost:8848 # nacos地址  
gateway:  
    routes: #网关路由配置  
    - id: user-service #路由id,自定义值,必须唯一  
    uri: lb:/userservice #路由的目标地址Lb表示负载均衡,后面跟Nacos中微服务名称  
    # uri: http://127.0.0.1:8081 #路由的目标地址,如果使用http表示固定地址  
    predicates: #路由断言,判断请求是否符合路由规则的条件  
    - Path=/user/*# #这个是按照路径匹配,只要以/user/开头就符合要求 http://127.0.0.1:10010/user/xxx/yyy  
    - id: order-service  
    uri: lb:/orderservice #Nacos中微服务名称  
    predicates:  
    - Path=/order/*# #这个是按照路径匹配,只要以/order/开头就符合要求 http://127.0.0.1:10010/order/xxx/yyy  
    filters:  
    - StripPrefix=1 #从请求路径中删除第一个前缀/order,再转发给后端  
    #原始请求: http://127.0.0.1:10010/order/xxx/yyy  
    #后端微服务实际得到http://127.0.0.1:10010/xxx/yyy

### 若依微服务项目

- 10.0.0.100 [www.duan.org](www.duan.org) nginx 反向代理和所有若依项目前后端应用的微服务；
    
- 10.0.0.101 nacos.duan.org mysql.duan.org redis.duan.org 基础服务：nacos、MySQL、Redis
    

# 所有主机节点通过/etc/hosts实现实现域名解析  
echo '10.0.0.101 nacos.duan.org mysql.duan.org redis.duan.org' >> /etc/hosts

##### 基础环境准备

下载项目源代码并修改Nacos和数据库连接配置 (10.0.0.100)

apt update && apt -y install git maven npm docker.io docker-compose python3-pip  
# 下载若依微服务项目源码文件  
git clone https://gitee.com/y_project/RuoYi-Cloud.git  
# 查看Nacos服务地址  
cd /root/RuoYi-Cloud && grep -r ruoyi-nacos:8848 *  
# 修改RuoYI应用的Nacos地址为nacos.duan.org  
cd /root/RuoYi-Cloud && grep -r ruoyi-nacos:8848 * |awk -F: '{print $1}'|uniq |xargs sed -i 's#ruoyi-nacos:8848#nacos.duan.org:8848#'  
# 确认地址修改完成  
cd /root/RuoYi-Cloud && grep -r nacos.duan.org:8848 *  
  
# 查看数据库脚本,其中ry_config_20231204.sql是Nacos的配置中心保存的RuoYi的配置信息  
ls /root/RuoYi-Cloud/sql  
quartz.sql ry_20231130.sql ry_config_20231204.sql ry_seata_20210128.sql  
# 查看Nacos配置中心的Ruoyi的数据库连接信息  
cd /root/RuoYi-Cloud && grep -E "mysql|redis" sql/ry_config_20231204.sql  
# 修改MySQL和Redis连接配置信息  
cd /root/RuoYi-Cloud && sed -i -e 's#ruoyi-mysql:3306#mysql.duan.org#g' -e 's#ruoyi-redis#redis.duan.org#g' sql/ry_config_20231204.sql

安装和配置 Redis (10.0.0.101)

apt update && apt -y install redis  
# 修改redis配置  
sed -i.bak -e 's/^bind .*/bind 0.0.0.0/' -e 's/^protected-mode.*/protected-mode no/' /etc/redis/redis.conf  
  
systemctl restart redis  
# 测试连接  
redis-cli info Server

安装和配置 MySQL (10.0.0.101)

# 安装和配置MySQL  
apt update && apt -y install mysql-server  
sed -i '/127.0.0.1/s/^/#/' /etc/mysql/mysql.conf.d/mysqld.cnf  
  
systemctl restart mysql  
# 准备数据库表和数据及用户  
mysql  
# 根据ry_config_20231204.sql中的信息准备RuoYi应用所需的数据库配置  
create database ry-cloud;  
create user root@'%' identified by '123123';  
grant all on ry-cloud.* to root@'%';  
grant all on ry-seata.* to root@'%';  
# 导入RuoYi应用所需的数据表  
use ry-cloud  
source /root/RuoYi-Cloud/sql/quartz.sql  
source /root/RuoYi-Cloud/sql/ry_20231130.sql  
# 创建Seata服务需要的数据库和表(可选)  
source /root/RuoYi-Cloud/sql/ry_seata_20210128.sql  
# Nacos服务所需的数据库配置  
source /root/RuoYi-Cloud/sql/ry_config_20231204.sql  
# 需要指定验证插件，不支持默认插件caching_sha2_password  
create user nacos@'%' identified with mysql_native_password by '123123';  
grant all on ry-config.* to nacos@'%';  
# 查看生成三个数据库  
show databases;  
# 查看创建的用户  
select user,host from mysql.user;  
  
# 测试用户访问Nacos的表  
mysql -unacos -p123123 -hmysql.duan.org -e 'show tables in ry-config'  
# 测试用户访问RuoYi的表  
mysql -uroot -p123123 -hmysql.duan.org -e 'show tables in ry-cloud'  
mysql -uroot -p123123 -hmysql.duan.org -e 'show tables in ry-seata'

安装和配置 Nacos (10.0.0.101)

- 注意:需要安装Nacos-2.4.3以前版本,Nacos-2.5.X以后版本不支持
    

# 安装Nacos  
apt update && apt -y install openjdk-8-jdk  
wget https://download.nacos.io/nacos-server/nacos-server-2.5.2.zip?spm=5238cd80.47ee59c.0.0.189fcd36iQTqYu&file=nacos-server-2.5.2.zip  
unzip nacos-server-2.5.2.zip -d /usr/local/  
#  准备Sevice文件  
id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos  
chown -R nacos: /usr/local/nacos/ && ll /usr/local/nacos/  
cat > /lib/systemd/system/nacos.service <<eof  
[Unit]  
Description=nacos.service  
After=network.target  
[Service]  
Type=forking  
ExecStart=/usr/local/nacos/bin/startup.sh -m standalone  
ExecStop=/usr/local/nacos/bin/shutdown.sh  
User=nacos  
Group=nacos  
[Install]  
WantedBy=multi-user.target  
eof  
  
systemctl daemon-reload  
# 修改Nacos使用MySQL数据库  
vim /usr/local/nacos/conf/application.properties  
spring.sql.init.platform=mysql  
db.num=1  
db.url.0=jdbc:mysql://127.0.0.1:3306/ry-config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC  
db.user.0=nacos  
db.password.0=123123  
  
systemctl restart nacos.service && systemctl status nacos.service  
# 查看日志  
tail /usr/local/nacos/logs/startup.log  
# 浏览器访问：10.0.0.101:8848/nacos

登录 Nacos 查看到配置中心的配置如下：

![image-20251202162334374](file:///C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20251202162334374.png?lastModify=1764935320)

##### 实验故障总结：

🔧 问题根源  
Nacos 2.5.2 需要 config_info_gray 表，但数据库中没有创建该表  
📋 排查过程  
错误现象：Nacos 启动失败，日志报错 Table 'ry-config.config_info_gray' doesn't exist  
  
验证确认：  
# 测试数据库连接  
mysql -hnacos.duan.org -unacos -p123123 -e "USE \ry-config\; SELECT 1;"  
  
💡 解决方案  
在 ry-config 数据库中创建缺失的表：  
mysql -uroot -p << 'EOF'  
USE ry-config;  
  
CREATE TABLE IF NOT EXISTS config_info_gray (  
  id bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id',  
  data_id varchar(255) NOT NULL COMMENT 'data_id',  
  group_id varchar(128) DEFAULT NULL,  
  tenant_id varchar(128) DEFAULT '' COMMENT '租户字段',  
  content longtext NOT NULL COMMENT 'content',  
  md5 varchar(32) DEFAULT NULL COMMENT 'md5',  
  src_ip varchar(50) DEFAULT NULL COMMENT 'source ip',  
  src_user varchar(128) DEFAULT NULL COMMENT 'source user',  
  app_name varchar(128) DEFAULT NULL,  
  type varchar(32) DEFAULT NULL,  
  encrypted_data_key text NOT NULL COMMENT '密钥',  
  gmt_create datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',  
  gmt_modified datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间',  
  PRIMARY KEY (id),  
  UNIQUE KEY uk_configinfogray_datagrouptenant (data_id,group_id,tenant_id)  
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='config_info_gray';  
  
SHOW TABLES LIKE 'config_info_gray';  
EOF  
  
✅ 修复结果  
创建 config_info_gray 表成功  
重新启动 MySQL、Nacos  
服务恢复可用

##### 构建后端JAVA微服务

# maven加速配置(可选,因为RuoYi本身项目已有配置加速,此步可不做)  
vim /etc/maven/settings.xml  
.....  
    <mirror>  
      <id>nexus-aliyun</id>  
      <mirrorOf>*</mirrorOf>  
      <name>Nexus aliyun</name>  
      <url>http://maven.aliyun.com/nexus/content/groups/public</url>  
    </mirror>  
  
# 构建后端微服务项目  
cd /root/RuoYi-Cloud && mvn clean package -Dmaven.test.skip=true  
# 验证编译结果   
cd /root/RuoYi-Cloud && find ruoyi-*/ -name "*.jar"  
# 完成复制相关的jar文件到目录/srv中  
cd /root/RuoYi-Cloud/docker && bash copy-jar.sh  && ls /srv  
  
# 按顺序启动  
nohup java -jar /srv/ruoyi-gateway.jar &>/dev/null &  
nohup java -jar /srv/ruoyi-auth.jar &>/dev/null &  
nohup java -jar /srv/ruoyi-modules-system.jar &>/dev/null &  
nohup java -jar /srv/ruoyi-modules-gen.jar &>/dev/null &  
nohup java -jar /srv/ruoyi-modules-job.jar &>/dev/null &  
nohup java -jar /srv/ruoyi-modules-file.jar &>/dev/null&  
nohup java -jar /srv/ruoyi-visual-monitor.jar &>/dev/null &  
  
ss -ntlp|grep java  
# 每启动一个服务，在 Nacos web界面的服务管理的服务列表就有一个注册信息生成；  
# 虽然开很多服务，但是最需要访问API网关，可以在 Nacos web界面查看  ruoyi-gateway 服务信息；  
# 也可以在终端通过 ps aux|grep java 和 ss -ntlp|grep java 对照查看得到API网关信息；

##### 实验故障总结：

# 在运行到 java -jar /srv/ruoyi-modules-system.jar 服务时遇到认证失败的故障  
# 所以我要将密码回退到王老师给定的密码；-- 修改 root@'%' 用户的密码为 'password'  
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'password';  
FLUSH PRIVILEGES;  
我的猜想果然没错，这个root用户的密码不能改！否则启动服务时就认证失败！  
如果不想改 MySQL设定好的密码，可以在web网页这个模块中进行编辑，修改服务对应jar包中的认证密码！  
也可以对MySQL中的数据库文件进行修改：grep "password: password" /root/RuoYi-Cloud/sql/ry_config_20231204.sql  
通过过滤发现三处符合，那么这三处 password 都要修改！  
sed -i "s#password: password#password: 123123#" /root/RuoYi-Cloud/sql/ry_config_20231204.sql  
然后重新导入数据库：  
mysql  
use ry-cloud  
source /root/RuoYi-Cloud/sql/ry_config_20231204.sql  
FLUSH PRIVILEGES;

![image-20251202170158343](file:///C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20251202170158343.png?lastModify=1764935320)

##### 构建前端应用

# 下载若依微服务项目源码文件  
git clone https://gitee.com/y_project/RuoYi-Cloud.git  
apt update && apt -y install npm  
npm -v  
node -v  
# 进入前端项目下  
cd /root/RuoYi-Cloud/ruoyi-ui  
# 安装依赖包,ubuntu2404默认不支持当前版本  
npm install --registry=https://registry.npmmirror.com  
# 为解决 Node.js 17+ 版本与 OpenSSL 3.0 的兼容性问题，设置变量  
export NODE_OPTIONS=--openssl-legacy-provider  
# 构建前端项目  
npm run build:prod  
# 构建存放在dist目录  
du -sh /root/RuoYi-Cloud/ruoyi-ui/dist/ && ls /root/RuoYi-Cloud/ruoyi-ui/dist

##### 反向代理配置

apt update && apt -y install nginx  
mkdir -p /data/ruoyi && cp -r /root/RuoYi-Cloud/ruoyi-ui/dist/*  /data/ruoyi && ls /data/ruoyi  
# 查看配置文件路径  
nginx -T|grep include  
# 实现Nginx反向代理配置文件  
vim /etc/nginx/conf.d/www.duan.org.conf  
server {  
    listen 80;  
    server_name www.duan.org;  
      
    # 前端静态文件  
    location / {  
        root /data/ruoyi/;  
        try_files $uri $uri/ /index.html;  
        index index.html index.htm;  
    }  
      
    # 后端API代理  
    location /prod-api/ {  
        proxy_pass http://10.0.0.101:8080/;  
        proxy_set_header Host $http_host;  
        proxy_set_header X-Real-IP $remote_addr;  
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  
        proxy_set_header X-Forwarded-Proto $scheme;  
    }  
}  
  
nginx -s reload

##### 测试访问 RuoYI

www.duan.org  
# 配置域名解析(Windows系统也要做哦！)  
echo "10.0.0.100 www.duan.org" >> /etc/hosts  
# 使用默认用户名密码:admin/admin123 登录RuoYi应用；  
登录成功，至此若依微服务架构结束！  
# 但是那么多服务，通常是需要 docker 容器化方便管理；

四种软件发布模型：

- 蓝绿部署；
    
- 金丝雀部署；
    
- 果冻部署；
    
- A/B 测试；
    

## 存储

### MinIO

> 企业级自建版 S3，对象存储中的“瑞士军刀”。

#### 背景

过去常用 **NFS** 或 **NAS**，但随着服务增多、容器化普及：

- 共享文件系统易锁死
    
- 单点问题严重
    
- 性能差、扩展差
    

MinIO 刚好解决这些痛点：

✓ 避免文件系统锁问题 ✓ S3 API 天生适合微服务 ✓ 多服务并发访问无压力 ✓ 容器环境极其友好（K8s 推荐方案）

|类型|访问方式|协议|场景|类比|
|---|---|---|---|---|
|**DAS**|本地直连（块）|SATA/SAS/USB|单机|“本地硬盘”|
|**NAS**|网络访问（文件）|SMB/NFS|局域网（文件共享）|“共享文件夹服务器”|
|**SAN**|专用网络（块）|FC / iSCSI|企业核心业务 （专用光纤网络）|“远程高性能硬盘”|

|**特点**|**存储桶 (Bucket)**|**传统文件系统（顶级文件夹）**|
|---|---|---|
|**层级结构**|**扁平结构：** Bucket 内部没有真实的文件夹层级，所有对象都是平级的。|**树状结构：** 文件夹内部可以嵌套更多文件夹，形成深度层级。|
|**管理策略**|**面向策略：** 权限、地域、版本控制等策略是针对整个 Bucket 设置的。|**面向目录：** 权限可以精确到每个文件夹或文件。|
|**访问方式**|通过 **HTTP/S API 和唯一的 URL** 访问。|通过文件路径和网络共享协议（如 SMB/NFS）访问。|
|**容量限制**|理论上**无限**。|受限于物理磁盘或文件系统（如 ext4, NTFS）的大小限制。|

相比块存储，对象存储的成本通常更低，适合存储不常变动的冷数据或备份。 数据不再放在传统的文件夹目录中，而是放在一个巨大的扁平的地址空间 （存储桶）

对象存储，每个对象包含如下三个部分：

- 数据：实际存放数据（比如：图片、视频等）
    
- 键值：数据全局唯一标识符；
    
- 元数据：数据对象的描述信息（比如创建日期、类型等）
    

#### 部署

##### server

- minio server 的 standalone 模式，即单机模式，所有管理的磁盘都在一个主机上；（仅作为实验环境）
    
- Debian/Ubuntu 二进制安装
    
- 官方下载链接（两个都行）：
    
    - [MinIO Download Server](https://dl.min.io/server/minio/release/linux-amd64/archive)
        
- 安装指定版本,建议使用 minio.RELEASE.2025-04-22T22-12-26Z 学习，注意：minio-05-24T17-08-30Z 版本以后界面功能功能缺失；
    

###### **包安装（参考）**

wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio_20250907161309.0.0_amd64.deb  
dpkg -i  minio_20250907161309.0.0_amd64.deb  
cat /lib/systemd/system/minio.service  
ls /etc/default/minio  
useradd -M -r -s /sbin/nologin minio-user  
cat > /etc/default/minio <<eof  
MINIO_ROOT_USER=admin  
MINIO_ROOT_PASSWORD=12345678  
MINIO_VOLUMES="/data/minio{1...4}"  
MINIO_OPTS='--console-address :9001'  
eof  
ls /etc/default &>/dev/null || mkdir -p /etc/default  
chown -R minio-user:minio-user /data/  
systemctl start minio && systemctl status minio

- 上述是包安装的操作，仅作为参考，并不推荐；因为deb、rpm等包不具有通用性，而二进制包只要是Linux系统就可以安装部署使用！
    
- 在 /lib/systemd/system/minio.service 配置文件中虽然添加了创建用户和用户组，但是实际并没有创建，需要手工创建，并手工修改文件所属主和所属组；
    
    - useradd -M -r -s /sbin/nologin minio 创建一个没有家目录、不能登录、专门用于服务运行的系统账户：minio-user
        
    - 创建完成记得修改数据卷的所属组和所属主为：chown -R minio-user:minio-user /data/
        
- 在 /lib/systemd/system/minio.service 配置文件中看到启动需要两个变量，而这两个变量在配置文件被定义在了 /etc/default/minio 文件中；
    
    - ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES
        
    - $MINIO_VOLUMES —— MinIO 的数据存储路径（必须）；不指定数据卷路径启动会失败！
        
    - $MINIO_OPTS —— MinIO 的 Console 控制台端口 ；不指定该参数不影响启动，但是每次启动的控制台端口是随机的；
        
- 这个文件 /etc/default/minio 需要手动创建，变量也需要手动添加！（这个包安装是不是很鸡肋呀？？？）
    
    - MINIO_ROOT_USER=admin #默认minioadmin MINIO_ROOT_PASSWORD=12345678 #默认minioadmin，密码不能低于8位 MINIO_VOLUMES="/data/minio{1...4}" #必选项 MINIO_OPTS='--console-address :9001' #默认使用随机端口，可以访问9000进行跳转到此随机端口 MINIO_PROMETHEUS_AUTH_TYPE="public" #支持promethues监控：curl -s
        
- 那么不创建这个文件可以吗？可以！ 只需要在启动时进行存储路径和控制台端口的参数指定
    
    - /usr/local/bin/minio server --console-address 0.0.0.0:9001 /data/minio
        
    - 手动指定数据存放路径；默认账户名和密码：minioadmin
        
    - 登录：10.0.0.201:9001
        
    - 这个 /data/minio 文件夹是启动时自动创建的；在网页中创建的存储桶 （Buckets）文件都会永久持续保存在 /data/minio 目录下；
        

###### **二进制安装**

wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio.RELEASE.2025-04-22T22-12-26Z  
install minio.RELEASE.2025-04-22T22-12-26Z  /usr/local/bin/minio  
minio --version  
which minio  
useradd -M -r -s /sbin/nologin minio-user  
cat > /lib/systemd/system/minio.service <<eof  
[Unit]  
Description=Minio  
After=systemd-networkd.service systemd-resolved.service  
Documentation=https://min.io  
[Service]  
Type=simple  
Environment=MINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=12345678  
ExecStart=/usr/local/bin/minio server /data/minio --console-address ":9001"  
Restart=on-failure  
User=minio-user  
Group=minio-user  
LimitNOFILE=infinity  
LimitNPROC=infinity  
LimitCORE=infinity  
OOMScoreAdjust=-1000  
[Install]  
WantedBy=multi-user.target  
eof  
  
ls /data &>/dev/null || mkdir -p /data  
chown -R minio-user:minio-user /data/  && ll /data  
systemctl daemon-reload  
systemctl start minio.service && systemctl status minio.service && sleep 1 && ss -tunlp | grep 9001  
journalctl -u minio -f

> install 安装会自动把文件权限设置为 755 ; 自动创建不存在的目录 ;
> 
> 在 minio web 界面创建一个 Buckets （自定义名称 duanxueli），在这个存储桶中上传文件，观察 tree /data/minio/duanxueli 目录下的内容；

###### 模拟多块硬盘

- 也可以添加硬盘（反正也是虚拟硬盘！）
    

rm -rf /data/minio && tree /data  
mkdir /data/minio{1..4} && tree /data  
systemctl stop minio && systemctl status minio  
sed -i 's#/data/minio#/data/minio{1...4}#' /lib/systemd/system/minio.service && grep "ExecStart" /lib/systemd/system/minio.service  
systemctl daemon-reload  
# 添加四块磁盘（因为是做实验，所以用逻辑卷也可以达到效果）  
df -h  
lsblk  
vgs  
lvs  
lvcreate -n minio1 -L 2G ubuntu-vg  
lvcreate -n minio2 -L 2G ubuntu-vg  
lvcreate -n minio3 -L 2G ubuntu-vg  
lvcreate -n minio4 -L 2G ubuntu-vg  
mkfs.ext4 /dev/ubuntu-vg/minio1  
mkfs.ext4 /dev/ubuntu-vg/minio2  
mkfs.ext4 /dev/ubuntu-vg/minio3  
mkfs.ext4 /dev/ubuntu-vg/minio4  
echo "/dev/ubuntu-vg/minio1 /data/minio1 ext4 defaults 0 0" >> /etc/fstab  
systemctl daemon-reload && mount -a  
df -h  
chown -R minio-user:minio-user /data/  && ll /data  
systemctl restart minio && systemctl status minio && sleep 2 && ss -tunlp | grep 9001

for i in {1..4}; do lvcreate -n minio$i -L 2G ubuntu-vg; done  
  
for i in {1..4}; do mkfs.ext4 /dev/ubuntu-vg/minio$i; done  
  
for i in {1..4}; do echo "/dev/ubuntu-vg/minio$i /data/minio$i ext4 defaults 0 0" >> /etc/fstab; done

> - 对于重复的指令，我在下面做一个能达到同样效果的 for 循环；
>     
> - 通过指令：df -h && lvs 查看卷组和规划划分多少逻辑卷！
>     
> - fstab 修改后 systemd 不会自动重载配置，需要手动执行 `systemctl daemon-reload` 来更新 systemd 的 mount units，否则 systemd 会继续使用旧的挂载配置。
>     
> - 创建一个 Buckets （自定义名称 library01），在这个存储桶中上传一个文件，查看目录结构 tree /data ，文件被切分成若干数据块和校验块，分布在四个磁盘中，用于容错和数据冗余。
>     
> - 查看这个目录下的数据存储变化，上传 100M 文件后，磁盘占用约 200M，这是因为 MinIO 使用了纠删码，生成了冗余块来保证容错。
>     
> - 模拟故障：删除 /data/minio3 磁盘后，MinIO 仍然可以正常访问对象，因为纠删码允许容忍单个磁盘故障。
>     

###### **Client**

- 下载地址：[https://www.min.io/download](https://www.min.io/download)
    
- **编程语言API访** —— Python 访问
    

apt update && apt -y install python3-pip  
python3 -m pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/  
pip3 install minio --break-system-packages  
pip3 install minio

cat > access_minio.py <<eof  
#!/usr/bin/python3  
import os  
from minio import Minio  
from minio.error import S3Error  
# from minio.error import (ResponseError, BucketAlreadyOwnedByYou, BucketAlreadyExists)  
# ↑ 旧版 MinIO SDK 使用 ResponseError，新版已经统一为 S3Error  
  
# 初始化 MinIO 客户端  
# endpoint：MinIO 服务监听的地址（服务器IP:端口）  
# access_key / secret_key：MinIO 的登录凭据  
# secure=False：关闭 https（因为你本地 9000 是 http）  
minio_client = Minio(  
    endpoint='10.0.0.201:9000',  
    access_key="B1veDyVuy1alQjSSorbG",  
    secret_key="fLhxFx6mZ4EbjXPbxBklw0dCGsaPGC47T4ayv1HO",  
    secure=False  
)  
  
# 创建存储桶，定义存储桶名、对象名、本地文件路径  
bucket_name = "mybucket"  
object_name = "mc"  
local_file_path = "/root/mc"         
# 将 /root/mc 上传为 mc  
  
try:  
    # 检查 bucket 是否存在  
    # 若不存在，创建一个同名 bucket  
    if not minio_client.bucket_exists(bucket_name):  
        minio_client.make_bucket(bucket_name)  
        print(f"Bucket '{bucket_name}' created successfully.")  
    else:  
        print(f"Bucket '{bucket_name}' already exists.")  
  
    # 使用 fput_object 上传文件  
    # 参数：bucket、对象名、本地路径  
    minio_client.fput_object(bucket_name, object_name, local_file_path)  
    print(f"Successfully uploaded '{local_file_path}' to '{object_name}'.")  
  
except S3Error as e:  
    # 捕获 MinIO/S3 API 的通用异常  
    print(f"An error occurred: {e}")  
  
# 定义下载后的文件名  
downloaded_file_path = "example-object.txt"  
  
try:  
    # 从 MinIO 下载对象到本地  
    minio_client.fget_object(bucket_name, object_name, downloaded_file_path)  
    print(f"Successfully downloaded '{object_name}' to '{downloaded_file_path}'.")  
  
except S3Error as e:  
    # 捕获下载失败的异常  
    print(f"An error occurred while downloading '{object_name}': {e}")  
eof

python3 access_minio.py  
install mc /usr/local/bin/  
mc --autocompletion  
exit  
mc --version  
mc alias ls  
echo "10.0.0.201 minio.duan.org" >> /etc/hosts  
mc alias set minio-server http://minio.duan.org:9000 B1veDyVuy1alQjSSorbG fLhxFx6mZ4EbjXPbxBklw0dCGsaPGC47T4ayv1HO  
mc ls minio-server  
mc ls minio-server/mybucket  
dd if=/dev/zero of=test.img bs=1M count=50  
mc cp test.img minio-server/mybucket/test2.img  && mc ls minio-server/mybucket  
mc cp minio-server/mybucket/test2.img /root/test3.img   && ll -sh /root/test3.img  
mc rm minio-server/mybucket/test2.img  && mc ls minio-server/mybucket  
mc admin info minio-server  
mc du minio-server/mybucket

> - 安装、加速安装、pip3 安装 MinIO Python 客户端库——（二选一：强制安装、标准安装方式）
>     
> - 登录网页，Access Keys —— Create access key —— Create 将 Access Key 和 Secret Key 两个访问秘钥写入上述的 Python 脚本中；
>     
>     - B1veDyVuy1alQjSSorbG
>         
>     - fLhxFx6mZ4EbjXPbxBklw0dCGsaPGC47T4ayv1HO
>         
> - Python 脚本运行成功，服务器查看文件 tree /data/minio/ 客户端查看下载地址 ll -sh /root
>     
> - 安装 MC 客户端：install mc /usr/local/bin/
>     
> - 安装 MC 客户端命令自动补全功能：mc --autocompletion 这个指令本质上是修改 .bashrc 文件；安装完成后需要重新登录才能生效！
>     
>     - 查看版本信息：mc --version
>         
>     - 查看默认连接信息：mc alias ls
>         
>     - 查看指定的存储桶已使用的磁盘空间：mc du minio-server/mybucket
>         
>     - 查看 MinIO 服务器或集群的整体状态信息 （重要） ：mc admin info minio-server
>         
> - 创建客户端与10.0.0.201服务器之间的认证指令，指令成功执行后会将信息刷入到 /root/.mc/config.json 文件中，再次查看会看到一个minio-server 模块
>     
>     - 列出指定 MinIO 服务中的所有 Bucket（存储桶）：mc ls minio-server
>         
>     - 上传文件到指定的 Bucket 并命令为 test2.img ：mc cp test.img minio-server/mybucket/test2.img && mc ls minio-server/mybucket
>         
>     - 下载指定的 Bucket 文件到客户端并命令为 test3.img ：mc cp minio-server/mybucket/test2.img test.img && mc ls minio-server/mybucket
>         
>     - 删除指定的 Bucket 文件 ：mc rm minio-server/mybucket/test2.img && mc ls minio-server/mybucket
>         

#### 高可用集群架构部署

- 客户端：10.0.0.200
    
- 反向代理服务器：10.0.0.100
    
- Minio1服务器：10.0.0.201
    
- Minio2服务器：10.0.0.202
    
- Minio3服务器：10.0.0.203
    

###### Minio 服务器

wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio.RELEASE.2025-04-22T22-12-26Z  
install minio.RELEASE.2025-04-22T22-12-26Z  /usr/local/bin/minio  
minio --version && which minio  
useradd -M -r -s /sbin/nologin minio && id minio  
mkdir -p /data/minio{1..4} && tree /data  
  
df -h && vgs && lvs  
for i in {1..4}; do lvcreate -n minio$i -L 2G ubuntu-vg; done   
for i in {1..4}; do mkfs.ext4 /dev/ubuntu-vg/minio$i; done   
for i in {1..4}; do echo "/dev/ubuntu-vg/minio$i /data/minio$i ext4 defaults 0 0" >> /etc/fstab; done   
systemctl daemon-reload && mount -a && df -h  
  
cat >> /etc/hosts <<eof  
10.0.0.201 minio1.duan.org  
10.0.0.202 minio2.duan.org  
10.0.0.203 minio3.duan.org  
eof  
cat > /etc/default/minio <<eof  
MINIO_ROOT_USER=admin  
MINIO_ROOT_PASSWORD=12345678  
MINIO_VOLUMES='http://minio{1...3}.duan.org:9000/data/minio{1...4}'  
MINIO_OPTS='--console-address :9001'  
MINIO_PROMETHEUS_AUTH_TYPE="public"  
eof  
cat > /lib/systemd/system/minio.service <<'eof'  
[Unit]  
Description=MinIO  
Documentation=https://docs.min.io  
Wants=network-noline.target  
After=network-noline.target  
[Service]  
WorkingDirectory=/usr/local  
User=minio  
Group=minio  
EnvironmentFile=-/etc/default/minio  
ExecStartPre=/bin/bash -c "if [ -z \"${MINIO_VOLUMES}\" ]; then echo \"Variable MINIO_VOLUMES not set in /etc/default/minio\"; exit 1; fi"  
ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES  
Restart=always  
LimitNOFILE=1048576  
TasksMax=infinity  
OOMScoreAdjust=-1000  
[Install]  
WantedBy=multi-user.target  
eof  
  
  
systemctl daemon-reload   
chown -R minio:minio /data/minio* && ll /data  
systemctl start minio.service ; systemctl status minio.service && sleep 1 && ss -tunlp | grep 9001  
journalctl -u minio -f

###### 反向代理服务器

apt update && apt -y install haproxy  
systemctl  status haproxy.service  
cat > /etc/haproxy/haproxy.cfg <<'eof'  
listen stats  
    mode http  
    bind 0.0.0.0:9999  
    stats enable  
    stats uri /haproxy-status  
    stats realm HAProxy\ Statistics  
    stats auth admin:123123  
  
listen minio  
    bind 10.0.0.100:9000  
    mode http  
    log global  
    balance roundrobin  
    option httpchk  
    server minio1 10.0.0.201:9000 check inter 3000 fall 2 rise 5  
    server minio2 10.0.0.202:9000 check inter 3000 fall 2 rise 5  
    server minio3 10.0.0.203:9000 check inter 3000 fall 2 rise 5  
  
listen minio_console  
    bind 10.0.0.100:9001  
    mode http  
    log global  
    balance roundrobin  
    option httpchk  
    server console1 10.0.0.201:9001 check inter 3000 fall 2 rise 5  
    server console2 10.0.0.202:9001 check inter 3000 fall 2 rise 5  
    server console3 10.0.0.203:9001 check inter 3000 fall 2 rise 5  
eof

###### Minio 客户端

echo "10.0.0.100 minio.duan.org" >> /etc/hosts  
mc alias ls  
mc alias set minio-cluster http://minio.duan.org:9000 e9ybiEOnT0tKavOaKjyj uVUJKa3gofMkdIgNMyTqtU0sOkEm15vUvcT4Od4u  
mc admin info minio-cluster  
mc mb minio-cluster/testbucket  
mc mb minio-cluster/testbucket

- 集群部署：三个节点，每个节点四块硬盘；
    
    - 通过指令：df -h && lvs 查看卷组和规划划分多少逻辑卷！
        
- 准备三台主机,在所有节点上做好名称解析；
    
- 创建四块硬盘，或者创建四个逻辑卷；每个主机的四块磁盘格式化,并分别挂载到 /data/minio{1..4}
    
- ExecStartPre 用于 启动前检查 MINIO_VOLUMES 是否已经配置，如果没有配置就阻止 MinIO 启动并给出提示。
    
- 查看日志：journalctl -xeu minio.service 或者 journalctl -u minio -f
    
- 查看集群：进入网站 —— Monitoring —— Metrics
    
- 配置反向代理,通过 Haproxy 或者 Nginx 实现 minio 的反向代理；
    
    - 登录：10.0.0.100:9999/haproxy-status
        
    - 账户密码：admin 123123
        
- MinIO 客户端 mc 创建别名，用于登录并保存连接信息；
    
    - mc alias set minio-cluster [http://minio.duan.org:9000](http://minio.duan.org:9000/) e9ybiEOnT0tKavOaKjyj uVUJKa3gofMkdIgNMyTqtU0sOkEm15vUvcT4Od4u
        
    - 如果有原先的实验残留，记得删除：mc alias ls | grep -q minio-server && mc alias rm minio-server
        
    - 客户端查看集群信息：mc admin info minio-cluster
        
    - 客户端以指令方式创建一个存储桶：mc mb minio-cluster/testbucket
        

#### 模拟故障

dd if=/dev/zero of=test.img bs=1M count=50  
mc cp test.img minio-cluster/testbucket/test1.img  && mc ls minio-cluster/testbucket  
systemctl stop minio && systemctl  status minio  
mc admin info minio-cluster  
mc cp minio-cluster/testbucket/test1.img test2.img && ll ./  
mc cp test.img minio-cluster/testbucket/test3.img  && mc ls minio-cluster/testbucket

- 客户端上传文件；
    
- 在 Minio3 （10.0.0.203） 节点执行 systemctl stop minio 模拟 Minio 节点故障；
    
- 客户端下载文件；测试在单节点故障时数据是否受影响；
    
- 客户端上传文件；测试在单节点故障时的写入功能；
    

#### 故障恢复

- 如果出现节点故障无法启动，可以直接服务器重置，重新部署 Minio Server 服务；
    
    - 若 IP 地址发生变动，对应的域名解析也要进行修改；
        
- 如果其他设备有安装包直接 scp 传输，就不重新下载了！
    
    - scp minio.RELEASE.2025-04-22T22-12-26Z 10.0.0.204:/root
        

二进制部署 Minio Server 服务

wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio.RELEASE.2025-04-22T22-12-26Z  
install minio.RELEASE.2025-04-22T22-12-26Z  /usr/local/bin/minio  
minio --version && which minio  
useradd -M -r -s /sbin/nologin minio && id minio  
mkdir -p /data/minio{1..4} && tree /data  
  
df -h && vgs && lvs  
for i in {1..4}; do lvcreate -n minio$i -L 2G ubuntu-vg; done && sleep 1  
for i in {1..4}; do mkfs.ext4 /dev/ubuntu-vg/minio$i; done && sleep 1  
for i in {1..4}; do echo "/dev/ubuntu-vg/minio$i /data/minio$i ext4 defaults 0 0" >> /etc/fstab; done   
systemctl daemon-reload && mount -a && df -h  
  
若 IP 地址发生变动，对应的域名解析也要进行修改；对应的 Haproxy 或者 Nginx 反向代理服务配置也需要修改；  
  
cat >> /etc/hosts <<eof  
10.0.0.201 minio1.duan.org  
10.0.0.202 minio2.duan.org  
10.0.0.203 minio3.duan.org  
eof  
cat > /etc/default/minio <<eof  
MINIO_ROOT_USER=admin  
MINIO_ROOT_PASSWORD=12345678  
MINIO_VOLUMES='http://minio{1...3}.duan.org:9000/data/minio{1...4}'  
MINIO_OPTS='--console-address :9001'  
MINIO_PROMETHEUS_AUTH_TYPE="public"  
eof  
cat > /lib/systemd/system/minio.service <<'eof'  
[Unit]  
Description=MinIO  
Documentation=https://docs.min.io  
Wants=network-noline.target  
After=network-noline.target  
[Service]  
WorkingDirectory=/usr/local  
User=minio  
Group=minio  
EnvironmentFile=-/etc/default/minio  
ExecStartPre=/bin/bash -c "if [ -z \"${MINIO_VOLUMES}\" ]; then echo \"Variable MINIO_VOLUMES not set in /etc/default/minio\"; exit 1; fi"  
ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES  
Restart=always  
LimitNOFILE=1048576  
TasksMax=infinity  
OOMScoreAdjust=-1000  
[Install]  
WantedBy=multi-user.target  
eof  
  
systemctl daemon-reload   
chown -R minio:minio /data/minio* && ll /data  
systemctl start minio.service ; systemctl status minio.service

#### 扩容

##### 基于 LVM 扩容

vgs  
for i in {1..4}; do lvextend -L +2G -r /dev/mapper/ubuntu--vg-minio$i; done   
df -Th  
mc admin info minio-cluster

- 查看卷组剩余使用容量空间：vgs
    
- 逻辑卷增加 2GB，并自动扩容文件系统：lvextend -L +2G -r /dev/mapper/ubuntu--vg-minio1
    
- 查看磁盘空间、文件系统类型及扩容是否成功：df -Th
    
- 扩展知识点——删除逻辑卷：
    
    - 检查是否挂载：df -Th | grep minio
        
        - 如果有挂载，必须先关闭服务：systemctl stop minio
            
        - 然后卸载：umount /data/minio1
            
    - 删除逻辑：for i in {1..4}; do lvremove /dev/ubuntu-vg/minio$i; done
        
        - 删除过程中系统会询问：Do you really want to remove active logical volume ...? [y/n] 输入 `y` 即可。
            
    - 确认删除结果：lvs && vgs
        

##### 增加节点扩容

- 增加相同规格的集群（如果原有的规模是3个，那么扩容数量必须也是3的倍数）
    
- IP 地址发生变动，集群所有的节点对应的域名解析都要进行修改；对应的 Haproxy 或者 Nginx 反向代理服务配置也需要修改；
    
- 修改所有节点 /etc/default/minio 和 /etc/hosts 文件；
    
- 停止原有集群服务；
    
- 二进制部署MinIO 实现3节点的分布式高可用集群的环境扩容；
    
- 这里就用其他节点的安装包，网络下载仅做参考；
    

systemctl stop minio.service  
  
wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio.RELEASE.2025-04-22T22-12-26Z  
install minio.RELEASE.2025-04-22T22-12-26Z  /usr/local/bin/minio  
minio --version && which minio  
useradd -M -r -s /sbin/nologin minio && id minio  
mkdir -p /data/minio{1..4} && tree /data  
  
df -h && vgs && lvs  
for i in {1..4}; do lvcreate -n minio$i -L 2G ubuntu-vg; done && sleep 1  
for i in {1..4}; do mkfs.ext4 /dev/ubuntu-vg/minio$i; done && sleep 1  
for i in {1..4}; do echo "/dev/ubuntu-vg/minio$i /data/minio$i ext4 defaults 0 0" >> /etc/fstab; done   
systemctl daemon-reload && mount -a && df -h  
  
cat >> /etc/hosts <<eof  
10.0.0.201 minio1.duan.org  
10.0.0.202 minio2.duan.org  
10.0.0.203 minio3.duan.org  
10.0.0.204 minio4.duan.org  
10.0.0.205 minio5.duan.org  
10.0.0.206 minio6.duan.org  
eof  
cat > /etc/default/minio <<eof  
MINIO_ROOT_USER=admin  
MINIO_ROOT_PASSWORD=12345678  
MINIO_VOLUMES='http://minio{1...3}.duan.org:9000/data/minio{1...4} http://minio{4...6}.wang.org:9000/data/minio{1...4}'  
MINIO_OPTS='--console-address :9001'  
MINIO_PROMETHEUS_AUTH_TYPE="public"  
eof  
cat > /lib/systemd/system/minio.service <<'eof'  
[Unit]  
Description=MinIO  
Documentation=https://docs.min.io  
Wants=network-noline.target  
After=network-noline.target  
[Service]  
WorkingDirectory=/usr/local  
User=minio  
Group=minio  
EnvironmentFile=-/etc/default/minio  
ExecStartPre=/bin/bash -c "if [ -z \"${MINIO_VOLUMES}\" ]; then echo \"Variable MINIO_VOLUMES not set in /etc/default/minio\"; exit 1; fi"  
ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES  
Restart=always  
LimitNOFILE=1048576  
TasksMax=infinity  
OOMScoreAdjust=-1000  
[Install]  
WantedBy=multi-user.target  
eof  
  
systemctl daemon-reload   
chown -R minio:minio /data/minio* && ll /data  
systemctl start minio.service ; systemctl status minio.service

修改haproxy代理配置

apt update && apt -y install haproxy  
systemctl  status haproxy.service  
cat > /etc/haproxy/haproxy.cfg <<'eof'  
listen stats  
    mode http  
    bind 0.0.0.0:9999  
    stats enable  
    stats uri /haproxy-status  
    stats realm HAProxy\ Statistics  
    stats auth admin:123123  
  
listen minio  
    bind 10.0.0.100:9000  
    mode http  
    log global  
    balance roundrobin  
    option httpchk  
    server minio1 10.0.0.201:9000 check inter 3000 fall 2 rise 5  
    server minio2 10.0.0.202:9000 check inter 3000 fall 2 rise 5  
    server minio3 10.0.0.203:9000 check inter 3000 fall 2 rise 5  
    server minio1 10.0.0.204:9000 check inter 3000 fall 2 rise 5  
    server minio2 10.0.0.205:9000 check inter 3000 fall 2 rise 5  
    server minio3 10.0.0.206:9000 check inter 3000 fall 2 rise 5  
  
listen minio_console  
    bind 10.0.0.100:9001  
    mode http  
    log global  
    balance roundrobin  
    option httpchk  
    server console1 10.0.0.201:9001 check inter 3000 fall 2 rise 5  
    server console2 10.0.0.202:9001 check inter 3000 fall 2 rise 5  
    server console3 10.0.0.203:9001 check inter 3000 fall 2 rise 5  
    server minio1 10.0.0.204:9000 check inter 3000 fall 2 rise 5  
    server minio2 10.0.0.205:9000 check inter 3000 fall 2 rise 5  
    server minio3 10.0.0.206:9000 check inter 3000 fall 2 rise 5  
eof

- 登录 Haproxy 查看状态
    

#### 缩容

- 缩容有数据丢失的风险；
    
- 如果MinIO不是基于LVM的存储，缩容相当于重新部署集群；
    
- 缩容前所有节点的数据备份,再在配置中删除上面的添加的节点,只保留部分节点,重启服务后,再恢复数据
    
- 注意：缩容需要重建数据和Access Key 信息
    

systemctl stop minio && systemctl status minio  
  
vim /etc/default/minio  
MINIO_ROOT_USER=admin  
MINIO_ROOT_PASSWORD=12345678  
# MINIO_VOLUMES='http://minio{1...3}.duan.org:9000/data/minio{1...4} http://minio{4...6}.wang.org:9000/data/minio{1...4}'  
MINIO_VOLUMES='http://minio{1...3}.duan.org:9000/data/minio{1...4}'  
MINIO_OPTS='--console-address :9001'  
MINIO_PROMETHEUS_AUTH_TYPE="public"  
  
systemctl restart minio ; systemctl status minio

mc admin info minio-cluster

- 备份数据：略
    
- 还原数据：略
    

#### 备份恢复

#### 监控

## ELK Stack

### 理论概念：

##### **ELK Stack 由四个主要组件组成：**

1. **Elasticsearch (E):** 存储、搜索和分析数据的核心引擎。
    
2. **Logstash (L):** 数据采集和转换管道。
    
3. **Kibana (K):** 数据可视化和用户界面。
    
4. **Beats (B):** 轻量级单用途数据采集器（如 Filebeat, Metricbeat）。
    

##### 基础概念：

- 数据以 **文档** 的形式存储。
    
- 多个相似的文档构成一个 **索引**。
    
- **索引** 被分成多个 **主分片**，以实现分布式存储和处理。
    
- 每个 **主分片** 都有一个或多个 **副本**，以实现高可用和高查询性能。
    

|**概念**|**含义**|**作用**|
|---|---|---|
|**索引 (Index)**|类似关系数据库中的“数据库”。它是具有相似特征的文档集合。|数据的逻辑分组，是执行搜索、更新和删除操作的入口点。|
|**文档 (Document)**|类似关系数据库中的“行”。它是可以被索引的基本信息单元，以 JSON 格式存储。|实际存储的数据载体，是 Elasticsearch 中最小的存储和搜索单元。|
|**分片 (Shard) / 主分片 (Primary Shard)**|索引被**水平切分**成若干个分片。每个分片都是一个独立的、功能完整的搜索引擎实例。|**实现数据的分布式存储和处理**。它允许索引容量突破单个节点的限制，并支持并行操作以**提高性能**。主分片的数量在索引创建时确定且不可更改。|
|**副本 (Replica Shard)**|主分片的**精确拷贝**，可以有零个或多个副本。|1. **高可用性/故障转移**：当主分片节点失效时，副本分片会被提升为新的主分片，确保数据不丢失。 2. **负载均衡/提高性能**：搜索请求可以由主分片或副本分片处理，Elasticsearch 会自动对搜索请求进行**负载均衡**，提升查询并发能力。副本分片的数量可以动态调整。|

##### ELK 工作流程

> - **Filebeat:** 轻量级采集器，对服务器资源占用小。
>     
> - **Kafka:** 引入消息队列作为**缓冲区 (Buffer)**，实现了**削峰填谷**，防止 Logstash/ES 处理不过来时日志丢失，这是生产环境的**最佳实践**。
>     
> - **Logstash:** 负责复杂的数据清洗和标准化。
>     
> - **Elasticsearch:** 负责高速存储和分析。
>     
> - **Kibana:** 提供了最终的用户界面和可视化能力。
>     

┌─────────────────┐  
│   应用服务器     │  
│   ┌─────────┐   │  
│   │ 日志文件 │   │  
│   └────┬────┘   │  
└────────┼────────┘  
         │ Filebeat  
         ▼  
┌─────────────────┐  
│     Kafka       │◄──缓冲/解耦  
│   (消息队列)    │  
└────────┬────────┘  
         │  
         ▼  
┌─────────────────┐  
│    Logstash     │◄──过滤/解析/转换  
│   (数据处理)    │  
└────────┬────────┘  
         │  
         ▼  
┌─────────────────┐  
│ Elasticsearch   │◄──存储/索引/搜索  
│   (数据存储)    │  
└────────┬────────┘  
         │ API  
         ▼  
┌─────────────────┐  
│     Kibana      │◄──仪表盘/查询/分析  
│   (可视化)      │  
└─────────────────┘

##### 常见的集群模式

- 将核心职责分离，以保障集群的稳定性和高可用性。这是生产环境推荐的最小化高可用配置。
    
- 在 Elasticsearch 中，通过配置 `node.roles` 数组，您可以灵活地为集群中的每个服务器定义其职责。
    
- **协调节点**是一个**隐式**的角色。**任何一个接收到客户端请求的节点，即使它没有被明确配置为 `coordinating` 角色，也会扮演协调节点的职责**。
    

|**节点**|**角色分配**|**数量**|**作用**|
|---|---|---|---|
|**专有主节点**|`master`|$3$ 个（推荐奇数）|专职负责集群管理，保障集群状态稳定。|
|**数据节点**|`data, ingest`|$N$ 个（至少 $2$ 个）|专职负责数据存储和查询。|
|**协调节点**|`coordinating`|$M$ 个（可选）|专职负责客户端请求的路由和聚合。|

#### Beats 收集数据

##### Filebeat 收集日志

- Logstash 也可以直接收集日志,但需要安装JDK并且会占用至少 500M 以上的内存；
    
- 生产一般使用filebeat代替logstash, 基于go开发,部署方便,重要的是只需要10M多内存,比较节约资源；
    

###### Filebeat 安装

> - 下载地址：[Download Filebeat | Elastic](https://www.elastic.co/cn/downloads/beats/filebeat)
>     
> - 官方文档：[https://www.elastic.co/docs/reference/beats/filebeat](https://www.elastic.co/docs/reference/beats/filebeat)
>     

dpkg -i filebeat-7.6.2-amd64.deb  
systemctl start filebeatps aux|grep filebeat

- filebeat 服务以 root 身份启动
    

###### Filebeat 配置

#### Elasticsearch

- 下载地址：[https://www.elastic.co/cn/downloads/elasticsearch](https://www.elastic.co/cn/downloads/elasticsearch)
    
- 内置JAVA
    
    - 基于 JAVA 的应用，而且安装时会自动内嵌 JAVA 环境，但是 Elasticsearch 的 JAVA 环境仅仅自己能用，不能共享；
        
    - /usr/share/elasticsearch/jdk/bin/java -version
        
- dpkg 与 apt install 安装包有什么区别？
    
    - dpkg 只安装包本身，不会自动解决依赖。
        
    - apt 会自动解决依赖、自动下载缺少的包，并更安全；（实际最常用）
        
    - dpkg 负责解包，apt 负责依赖。线上用 apt，离线用 dpkg。
        
- JVM优化
    
    - 编辑 /etc/elasticsearch/jvm.options 文件，添加或修改参数：-Xms128m 和 -Xmx128m
        
    - `-Xms` = JVM 初始堆大小
        
        `-Xmx` = JVM 最大堆大小
        
    - 生产环境设置规则：
        
        - **Elasticsearch 要求 Xms 和 Xmx 保持一致**，否则频繁扩容堆会导致 Stop-The-World 卡顿。
            
        - **堆大小占系统物理内存的 50% 左右** （官方建议）。
            
        - 最大不要超过 30GB ；超过 30GB 就会失效 → 性能立刻下降 ；日志类业务更依赖磁盘吞吐，而不是堆大小！
            
        - 堆越大 → GC 扫描越多 → STW 时间更长
            
        - 堆超过 30GB → 禁用 Compressed Oops → 内存占用变大 → GC 负担剧增 → STW 更长
            
        - 若是容器启动，则另说！
            
- 下载安装包建议先查看产品文档，当前的系统版本与 Elasticsearch 的兼容性；[支持矩阵 | Elastic](https://www.elastic.co/cn/support/matrix)
    
- 强制要求：必须以普通用户身份启动，不能以超级用户身份启动，否则失败；
    
- Elasticsearch JAVA 比较吃内存，实验时建议设置为4GB内存；
    
- Elasticsearch 安装完成时会显示登录密码；
    
    - 重置密码：/usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
        
- 默认 9.X 开启 xpack 安全，导致无法直接访问 curl 127.0.0.1:9200 这样直接访问会失败！
    
    - 关闭XPACK功能，就可以直接访问 curl 127.0.0.1:9200
        
        - 修改 /etc/elasticsearch/elasticsearch.yml 文件 xpack.security.enabled: false # 修改为false
            
- 登录：curl -u"elastic:$PASSWORD" -k [https://127.0.0.1:9200](https://127.0.0.1:9200/)
    

##### 包安装（单机）

wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-9.2.1-amd64.deb  
apt install ./elasticsearch-9.2.1-amd64.deb  
free -h  
/usr/share/elasticsearch/jdk/bin/java -version  
systemctl start elasticsearch.service  
curl https://127.0.0.1:9200 -k  
/usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic  
PASSWORD=cz+1XmXs9lPgOYWockJq  
curl -u"elastic:$PASSWORD" -k https://127.0.0.1:9200  
sed -i 's#^xpack.security.enabled#xpack.security.enabled: false#' /etc/elasticsearch/elasticsearch.yml  
vim /etc/elasticsearch/jvm.options  
-Xms2G  
-Xmx2G  
sudo sed -i '/Xmx4g$/a -Xms2G\n-Xmx2G' /etc/elasticsearch/jvm.options  
sudo sed -i 's#128m#2G#' /etc/elasticsearch/jvm.options  
grep -- Xm /etc/elasticsearch/jvm.options  
  
systemctl restart elasticsearch.service  
ps aux | grep java | grep Xms

curl 127.0.0.1:9200/_cat/health

##### 集群部署

- 8.X 以上版本集群配置
    
- 编辑 /etc/elasticsearch/elasticsearch.yml 文件配置
    
    - node.name: node-1 修改此行，每个节点不同；
        
    - network.host: 0.0.0.0 集群模式必须修改此行，否则集群节点无法通过9300端口通信；每个节点相同；
        
    - 去注释 cluster.name: my-application ；这是集群名，名称无所谓，但是同集群内这个名称必须唯一；
        
    - discovery.seed_hosts: ["10.0.0.201", "10.0.0.202","10.0.0.203"] 修改此行，每个节点相同；
        
    - cluster.initial_master_nodes: ["10.0.0.201", "10.0.0.202","10.0.0.203"] 修改此行，每个节点相同；这个参数有两个，记得要注释其中一个；
        
        - 仅仅生效于第一次选举；第二次第三次以后以后的选举与此配置无关；
            
    - xpack.security.enabled: false 修改此行，每个节点相同；
        
    - bootstrap.memory_lock: true （优化）内存锁；开启此功能导 8.X 致集群模式无法启动，但单机模式可以启动；
        
        - sudo sed -i '/^\[Service\]/a LimitMEMLOCK=infinity' /usr/lib/systemd/system/elasticsearch.service
            
            作用：允许 Elasticsearch 进程锁定不限量内存，否则启用 bootstrap.memory_lock 会直接报错。
            

wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-9.2.1-amd64.deb  
apt install ./elasticsearch-9.2.1-amd64.deb  
  
vi /etc/elasticsearch/elasticsearch.yml  
node.name: node-1  
network.host: 0.0.0.0  
xpack.security.enabled: false  
cluster.name: my-application  
discovery.seed_hosts: ["10.0.0.201","10.0.0.202","10.0.0.203"]  
cluster.initial_master_nodes: ["10.0.0.201","10.0.0.202","10.0.0.203"]  
#cluster.initial_master_nodes: ["MINIO-201"]	# 有两个相同的参数，只能留一个！  
  
for i in {201..203} ; do scp /etc/elasticsearch/elasticsearch.yml 10.0.0.$i:/etc/elasticsearch/elasticsearch.yml ; done  
sed -i 's#^node.name.*#node.name: node-2#' /etc/elasticsearch/elasticsearch.yml && grep 'node-2' /etc/elasticsearch/elasticsearch.yml  
sed -i 's#^node.name.*#node.name: node-3#' /etc/elasticsearch/elasticsearch.yml && grep 'node-3' /etc/elasticsearch/elasticsearch.yml  
grep cluster.initial_master_nodes /etc/elasticsearch/elasticsearch.yml  
  
sudo sed -i 's#2G#1G#' /etc/elasticsearch/jvm.options && grep -- Xm /etc/elasticsearch/jvm.options  
  
sudo systemctl daemon-reload  
systemctl restart elasticsearch.service && systemctl status elasticsearch.service  
ss -ntlp |grep -E '9200|9300'

curl http://10.0.0.201:9200/_cat/health

##### 索引

> **注意：ES的副本指不包括主分片的其它副本,即只包括备份，这与Kafka是不同的**
> 
> 创建索引，安装插件，通过插件 Head 查看索引，以及分片和副本
> 
> 创建的分片最佳数量是与节点数量相等；

# 创建索引index1,简单输出  
curl -XPUT 'http://10.0.0.201:9200/index1'  
# Elasticsearch 默认数据存储路径  
ls /var/lib/elasticsearch  
# 查看 Elasticsearch 全部索引列表  
curl 'http://10.0.0.201:9200/_cat/indices?v'  
# 创建3个分片和2个副本的索引  
curl -X PUT "10.0.0.202:9200/index2" -H "Content-Type: application/json" -d '{  
    "settings": {  
      "index": {  
        "number_of_shards": 3,  
        "number_of_replicas": 1  
      }  
    }  
  }'

##### 插件

- 通过使用各种插件可以实现对 ES 集群的状态监控, 数据访问, 管理配置等功能;
    
- ES集群状态：
    
    - green 绿色状态：表示集群各节点运行正常，而且没有丢失任何数据；
        
    - yellow 黄色状态：node节点无法连接、所有主分片都正常分配,有副本分片丢失，但是还没有丢失任何数据；
        
    - red 红色状态：主分片丢失及数据丢失,但仍可读取数据和存储；
        

Cerebro 插件

- 个人不喜欢这个插件，所以没有做这个服务！
    

Head 插件

- git地址：[https://github.com/mobz/elasticsearch-head](https://github.com/mobz/elasticsearch-head)
    
- 浏览器安装：管理扩展 —— 添加扩展 —— 添加完成后，点击该插件使用
    

![image-20251204220901976](file:///C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20251204220901976.png?lastModify=1764935320)

##### 故障自愈

- 当节点宕机后，Elasticsearch 会自动检测到分片丢失，并在其他节点重建副本，恢复节点后再进行分片均衡，从而实现真正意义上的“故障自愈”。
    

> 准备工作：完成上面的集群；安装插件；
> 
> # 创建一个索引 index2，包含 3 个主分片（P）+ 1 个副本分片（R）：  
> curl -X PUT "10.0.0.202:9200/index2" -H "Content-Type: application/json" -d '{  
>  "settings": {  
>    "index": {  
>      "number_of_shards": 3,  
>      "number_of_replicas": 1  
>    }  
>  }  
> }'  
> systemctl stop elasticsearch
> 
> 此时在 3 节点集群中，**每个主分片 P 都会被分布到不同的节点上，每个副本 R 也会分布在不同节点**，确保没有 P 和 R 落在同一节点上。

> 模拟节点故障（下线 10.0.0.201）
> 
> systemctl stop elasticsearch
> 
> **健康状态从 green → yellow**
> 
> - 某个节点宕机后，落在该节点上的 **副本分片（Replica）不可用**。
>     
> - 主分片仍在其他节点上，所以 **数据依然能读写**。
>     
> - 副本分片缺失 → 集群状态变为 **Yellow（部分副本缺失）**。
>     
> 
> 👉 **Yellow 表示数据可用，但冗余不足。**

> **Elasticsearch 自动故障自愈**
> 
> 当节点 10.0.0.201 下线后，过一段时间（默认 1 分钟左右），ES 会触发以下动作：
> 
> **自动重新分配副本（Replica Reallocation）**
> 
> - ES 会检测到某节点不可用
>     
> - 集群会在剩余健康的节点上 **自动重建副本分片**
>     
> - 冗余恢复完整后，状态从 **Yellow → Green**
>     
> 
> 👉 此过程称为：
> 
> **❗ 自动分片重分配（Auto Shard Reallocation）**
> 
> 也被称为：
> 
> - 副本自愈
>     
> - 分片修复机制
>     
> - 集群再均衡
>     

> 恢复节点后（重新启动 10.0.0.201）
> 
> systemctl start elasticsearch
> 
> 节点重新加入集群后：
> 
> **集群重新平衡（Shard Rebalancing）**
> 
> - ES 会将部分主分片或副本分片迁回到这个节点
>     
> - 最终再次达到分片均衡状态（平衡负载）
>     
> 
> 👉 这称为：
> 
> **❗ 自动分片重新均衡（Auto Shard Rebalancing）**
> 
> 恢复后集群再次保持：
> 
> - 状态：**Green**
>     
> - 主分片正常
>     
> - 副本分片完整
>     
> - 负载均衡
>     

![image-20251204221226150](file:///C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20251204221226150.png?lastModify=1764935320)

Elasticsearch 数据的更改是由主分片决定的，主分片执行完增删改后同步到副分片中；

## 常用指令

### Linux

cat /etc/os-release  
top  
uptime  
free -h  
df -h  
du -sh  
uname -a  
hostname -I

df -Th  
mount / umount  
fdisk -l  

ll  
ldd /usr/local/kafka/bin/kafka  
cp -r /src /dest  
mv a b  
ls | grep "^test" | xargs -r rm -rf  
mkdir -p a/b/c  
touch file  
ln -s /opt/a b

ip add  
ip route  
ip link show  
ip link set dev eth0 up  
curl -I URL  
curl -v URL  
ss -tunlp  
traceroute  
dig 域名

ps aux grep xxx  
kill -9 PID  
nohup command &  
journalctl -u elasticsearch -f

unzip a.zip  
zip -r a.zip a/  
tar -zxvf a.tar.gz  
tar -zcvf a.tar.gz a/

chmod 755 file  
chown user:group file

tail -f xxx.log  
tail -n 20 xxx.log

apt purge <包名> && apt autoremove  
userdel -r minio

### 三剑客

> - 使用 `grep` 的 `-A` (After) 参数来显示匹配行**之后**的指定行数。
>     

grep -R keyword /path  
grep -A 5 '^administrator' 文件名  
find /path -name "*keyword*"

awk -F ":" '{print $1}' /etc/passwd  
awk -F '[ :]' '{print $1, $3}' file.txt  
awk '$3 > 1000 {print $1,$3}' /etc/passwd

sed -i 's/old/new/g' file  
sed -i '$d' file

### 待收录

ps aux --sort=-%cpu | head  
du -sh * | sort -h | tail

### VIM

dwi  
:set nu  
:set cursorcolumn  
:set colorcolumn=  
:10G  
vim +10 test

### 奇淫巧技

qpdf --password=Magedu@M65! --decrypt 文件输入路径 文件输出路径